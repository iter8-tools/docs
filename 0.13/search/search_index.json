{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Iter8 \u00b6","title":"Home"},{"location":"#iter8","text":"","title":"Iter8"},{"location":"contributing/","text":"Overview \u00b6 Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on key areas including: Problems found during setup of Iter8 Gaps in our getting started tutorial and other documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know! Ways to contribute \u00b6 We welcome many types of contributions including: CLI and Iter8 experiment charts Docs CI, builds, and tests Reviewing pull requests Ask for help \u00b6 The best ways to reach us with a question is to ask... On the original GitHub issue In the #development channel in the Iter8 Slack workspace During our community meetings Find an issue \u00b6 Iter8 issues are tracked here . Pull request lifecycle \u00b6 Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools . Sign Your Commits \u00b6 Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the Git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running: git commit --amend -s Development environment setup \u00b6 The Iter8 project consists of the following repos. iter8-tools/iter8 : source for the Iter8 CLI iter8-tools/hub : source for Iter8 experiment and supplementary charts iter8-tools/docs : source for Iter8 docs iter8-tools/homebrew-iter8 : Homebrew formula for the Iter8 CLI iter8-tools/iter8 \u00b6 This is the source repo for Iter8 CLI. Clone iter8 \u00b6 git clone https://github.com/iter8-tools/iter8.git Build Iter8 \u00b6 make build Install Iter8 locally \u00b6 make clean install iter8 version Run unit tests and see coverage information \u00b6 make tests make coverage make htmlcov Lint Iter8 \u00b6 make lint Build and push Iter8 image \u00b6 Define a name for your Docker image IMG =[ Docker image name ] Build and push Iter8 image to Docker make dist docker build -f Dockerfile.dev -t $IMG . docker push $IMG iter8-tools/docs \u00b6 This is the source repo for Iter8 documentation. Clone docs \u00b6 git clone https://github.com/iter8-tools/docs.git Locally serve docs \u00b6 From the root of this repo: python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s You can now see your local docs at http://localhost:8000 . You will also see live updates to http://localhost:8000 as you update the contents of the docs folder.","title":"Contributing"},{"location":"contributing/#overview","text":"Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on key areas including: Problems found during setup of Iter8 Gaps in our getting started tutorial and other documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!","title":"Overview"},{"location":"contributing/#ways-to-contribute","text":"We welcome many types of contributions including: CLI and Iter8 experiment charts Docs CI, builds, and tests Reviewing pull requests","title":"Ways to contribute"},{"location":"contributing/#ask-for-help","text":"The best ways to reach us with a question is to ask... On the original GitHub issue In the #development channel in the Iter8 Slack workspace During our community meetings","title":"Ask for help"},{"location":"contributing/#find-an-issue","text":"Iter8 issues are tracked here .","title":"Find an issue"},{"location":"contributing/#pull-request-lifecycle","text":"Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools .","title":"Pull request lifecycle"},{"location":"contributing/#sign-your-commits","text":"Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the Git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running: git commit --amend -s","title":"Sign Your Commits"},{"location":"contributing/#development-environment-setup","text":"The Iter8 project consists of the following repos. iter8-tools/iter8 : source for the Iter8 CLI iter8-tools/hub : source for Iter8 experiment and supplementary charts iter8-tools/docs : source for Iter8 docs iter8-tools/homebrew-iter8 : Homebrew formula for the Iter8 CLI","title":"Development environment setup"},{"location":"contributing/#iter8-toolsiter8","text":"This is the source repo for Iter8 CLI.","title":"iter8-tools/iter8"},{"location":"contributing/#clone-iter8","text":"git clone https://github.com/iter8-tools/iter8.git","title":"Clone iter8"},{"location":"contributing/#build-iter8","text":"make build","title":"Build Iter8"},{"location":"contributing/#install-iter8-locally","text":"make clean install iter8 version","title":"Install Iter8 locally"},{"location":"contributing/#run-unit-tests-and-see-coverage-information","text":"make tests make coverage make htmlcov","title":"Run unit tests and see coverage information"},{"location":"contributing/#lint-iter8","text":"make lint","title":"Lint Iter8"},{"location":"contributing/#build-and-push-iter8-image","text":"Define a name for your Docker image IMG =[ Docker image name ] Build and push Iter8 image to Docker make dist docker build -f Dockerfile.dev -t $IMG . docker push $IMG","title":"Build and push Iter8 image"},{"location":"contributing/#iter8-toolsdocs","text":"This is the source repo for Iter8 documentation.","title":"iter8-tools/docs"},{"location":"contributing/#clone-docs","text":"git clone https://github.com/iter8-tools/docs.git","title":"Clone docs"},{"location":"contributing/#locally-serve-docs","text":"From the root of this repo: python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s You can now see your local docs at http://localhost:8000 . You will also see live updates to http://localhost:8000 as you update the contents of the docs folder.","title":"Locally serve docs"},{"location":"roadmap/","text":"Roadmap \u00b6 Comparing multiple versions based on business rewards DevSecOps experiments MLOps concept drift detection experiments Iter8 Tekton task Spike/ramp testing","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Comparing multiple versions based on business rewards DevSecOps experiments MLOps concept drift detection experiments Iter8 Tekton task Spike/ramp testing","title":"Roadmap"},{"location":"community/community/","text":"Community \u00b6 Meetings \u00b6 Everyone is welcome to join our community meetings. They are usually on the 3 rd Wednesday of each month from 11:00 AM \u2013 12:00 PM EST/EDT. All community meetings will be announced on Iter8 Slack in advance. Here is the meeting link . View the calendar or subscribe ( Google Calendar , iCalendar file ). Feel free to bring discussion topics to the meeting. If you would like to present a demo, please drop us a note in the Iter8 Slack workspace . Our meetings are also recorded and publicly available on our YouTube channel . Slack \u00b6 Iter8 Slack workspace is here . Join the Iter8 Slack for usage and development related discussions. GitHub Issues \u00b6 GitHub issues for all Iter8 repositories are managed here .","title":"Community"},{"location":"community/community/#community","text":"","title":"Community"},{"location":"community/community/#meetings","text":"Everyone is welcome to join our community meetings. They are usually on the 3 rd Wednesday of each month from 11:00 AM \u2013 12:00 PM EST/EDT. All community meetings will be announced on Iter8 Slack in advance. Here is the meeting link . View the calendar or subscribe ( Google Calendar , iCalendar file ). Feel free to bring discussion topics to the meeting. If you would like to present a demo, please drop us a note in the Iter8 Slack workspace . Our meetings are also recorded and publicly available on our YouTube channel .","title":"Meetings"},{"location":"community/community/#slack","text":"Iter8 Slack workspace is here . Join the Iter8 Slack for usage and development related discussions.","title":"Slack"},{"location":"community/community/#github-issues","text":"GitHub issues for all Iter8 repositories are managed here .","title":"GitHub Issues"},{"location":"community/news/","text":"News and Announcements \u00b6 March 2023: New Stack blog article by Michael Kalantar. Iter8: Simple A/B/n Testing of Kubernetes Apps, ML Models February 2023: DZone article by Alan Cha. Automated Performance Testing With ArgoCD and Iter8 December 2022: DZone article by Michael Kalantar. Simplifying A/B/n Testing of Backend Services October 2022: Iter8 at KubeCon. Conference details Presentation by Srinivasan Parthasarathy. Video coming soon Lightning talk by Alan Cha. Video link here August 2022: ITNEXT article by Alan Cha. Performance testing with Iter8, now with custom metrics! August 2022: Knative blog article by Srinivasan Parthasarathy. Simple Performance Testing with SLOs June 2022: Iter8 at Open Source Summit. Video coming soon. Conference details May 2022: IBM Developer blog article by Srinivasan Parthasarathy. Dead simple benchmarking and SLO validation for Kubernetes services May 2022: New Stack blog article by Srinivasan Parthasarathy. Iter8 Unifies Performance Validation for gRPC and HTTP March 2022: New Stack blog article by Michael Kalantar. Simple Load Testing with GitHub Actions Feb 2022: New Stack blog article on Simple HTTP Load Testing with SLOs Nov 2021: Iter8 at ACM Symposium on Cloud Computing. Full paper here Iter8 v0.7 and older Oct 2021: New Stack blog article by Hai Huang: Progressive Delivery on OpenShift Oct 2021: Iter8 at PREVAIL conference. Video coming soon. Conference details Oct 2021: New Stack blog article by Srinivasan Parthasarathy: Validate Service-Level Objectives of REST APIs Using Iter8 Jul 2021: Blog article by Clive Cox: ML\u200c \u200cProgressive\u200c \u200cRollouts\u200c \u200cwith\u200c \u200cSeldon\u200c \u200cand\u200c \u200cIter8\u200c Jul 2021: Iter8 at Knative meetup May 2021: Iter8 at KubeCon + CloudNativeCon Europe Mar 2021: Iter8 at Knative meetup Mar 2021: Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing Oct 2020: Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 Oct 2020: Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application Oct 2020: Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes Oct 2020: Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood Aug 2020: Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control","title":"News"},{"location":"community/news/#news-and-announcements","text":"March 2023: New Stack blog article by Michael Kalantar. Iter8: Simple A/B/n Testing of Kubernetes Apps, ML Models February 2023: DZone article by Alan Cha. Automated Performance Testing With ArgoCD and Iter8 December 2022: DZone article by Michael Kalantar. Simplifying A/B/n Testing of Backend Services October 2022: Iter8 at KubeCon. Conference details Presentation by Srinivasan Parthasarathy. Video coming soon Lightning talk by Alan Cha. Video link here August 2022: ITNEXT article by Alan Cha. Performance testing with Iter8, now with custom metrics! August 2022: Knative blog article by Srinivasan Parthasarathy. Simple Performance Testing with SLOs June 2022: Iter8 at Open Source Summit. Video coming soon. Conference details May 2022: IBM Developer blog article by Srinivasan Parthasarathy. Dead simple benchmarking and SLO validation for Kubernetes services May 2022: New Stack blog article by Srinivasan Parthasarathy. Iter8 Unifies Performance Validation for gRPC and HTTP March 2022: New Stack blog article by Michael Kalantar. Simple Load Testing with GitHub Actions Feb 2022: New Stack blog article on Simple HTTP Load Testing with SLOs Nov 2021: Iter8 at ACM Symposium on Cloud Computing. Full paper here Iter8 v0.7 and older Oct 2021: New Stack blog article by Hai Huang: Progressive Delivery on OpenShift Oct 2021: Iter8 at PREVAIL conference. Video coming soon. Conference details Oct 2021: New Stack blog article by Srinivasan Parthasarathy: Validate Service-Level Objectives of REST APIs Using Iter8 Jul 2021: Blog article by Clive Cox: ML\u200c \u200cProgressive\u200c \u200cRollouts\u200c \u200cwith\u200c \u200cSeldon\u200c \u200cand\u200c \u200cIter8\u200c Jul 2021: Iter8 at Knative meetup May 2021: Iter8 at KubeCon + CloudNativeCon Europe Mar 2021: Iter8 at Knative meetup Mar 2021: Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing Oct 2020: Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 Oct 2020: Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application Oct 2020: Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes Oct 2020: Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood Aug 2020: Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control","title":"News and Announcements"},{"location":"getting-started/concepts/","text":"Iter8 \u00b6 Iter8 is the Kubernetes release optimizer built for DevOps, MLOps, SRE and data science teams. Iter8 makes it easy to ensure that Kubernetes apps and ML models perform well and maximize business value. Iter8 supports the following use-cases. Performance testing and SLO validation of HTTP services. Performance testing and SLO validation of gRPC services. SLO validation using custom metrics from any database(s) or REST API(s). A/B/n experiments. Iter8 experiment \u00b6 Iter8 introduces the notion of an experiment . An experiment is simply a list of tasks that are executed in a specific sequence. Iter8 provides a variety of configurable tasks. Authoring an experiment is as simple as specifying the names of the tasks and specifying their parameter values. The following are some examples of tasks provided by Iter8. Tasks for generating load and collecting built-in metrics for HTTP and gRPC services. A task for verifying service-level objectives (SLOs) for apps or app versions. A task for fetching custom metrics from any database(s) or REST API(s). A task for checking if an object exists in the Kubernetes cluster and is ready . In addition to predefined tasks, Iter8 packs a number of powerful features that facilitate experimentation. They include the following. HTML/text reports that promote end-user understanding of experiment results through visual insights. Assertions that verify whether the target app satisfies the specified SLOs or not during/after an experiment. Multi-loop experiments that can be executed periodically instead of just once (single-loop). Iter8 GitHub Action that enables you to invoke the Iter8 CLI within a GitHub Actions workflow. Imperative and declarative experiments \u00b6 You can use the Iter8 CLI to launch and manage experiments through the command line. This is the imperative style of experimentation. You can also use the Iter8 Autox controller to launch and manage experiments declaratively. AutoX , short for \u201cautomated experiments\u201d, allows Iter8 to detect changes to your Kubernetes resources objects and automatically start new experiments, allowing you to test your applications as soon as you release a new version. Under the covers \u00b6 In order to execute an experiment inside Kubernetes, Iter8 uses a Kubernetes job (single-loop) or a Kubernetes cronjob (multi-loop) workload, along with a Kubernetes secret. Iter8 instantiates all experiments using a Helm chart, that is also predefined and provided by Iter8. Implementation \u00b6 Iter8 is written in go and builds on a few awesome open source projects including: Helm Fortio ghz plotly.js","title":"Concepts"},{"location":"getting-started/concepts/#iter8","text":"Iter8 is the Kubernetes release optimizer built for DevOps, MLOps, SRE and data science teams. Iter8 makes it easy to ensure that Kubernetes apps and ML models perform well and maximize business value. Iter8 supports the following use-cases. Performance testing and SLO validation of HTTP services. Performance testing and SLO validation of gRPC services. SLO validation using custom metrics from any database(s) or REST API(s). A/B/n experiments.","title":"Iter8"},{"location":"getting-started/concepts/#iter8-experiment","text":"Iter8 introduces the notion of an experiment . An experiment is simply a list of tasks that are executed in a specific sequence. Iter8 provides a variety of configurable tasks. Authoring an experiment is as simple as specifying the names of the tasks and specifying their parameter values. The following are some examples of tasks provided by Iter8. Tasks for generating load and collecting built-in metrics for HTTP and gRPC services. A task for verifying service-level objectives (SLOs) for apps or app versions. A task for fetching custom metrics from any database(s) or REST API(s). A task for checking if an object exists in the Kubernetes cluster and is ready . In addition to predefined tasks, Iter8 packs a number of powerful features that facilitate experimentation. They include the following. HTML/text reports that promote end-user understanding of experiment results through visual insights. Assertions that verify whether the target app satisfies the specified SLOs or not during/after an experiment. Multi-loop experiments that can be executed periodically instead of just once (single-loop). Iter8 GitHub Action that enables you to invoke the Iter8 CLI within a GitHub Actions workflow.","title":"Iter8 experiment"},{"location":"getting-started/concepts/#imperative-and-declarative-experiments","text":"You can use the Iter8 CLI to launch and manage experiments through the command line. This is the imperative style of experimentation. You can also use the Iter8 Autox controller to launch and manage experiments declaratively. AutoX , short for \u201cautomated experiments\u201d, allows Iter8 to detect changes to your Kubernetes resources objects and automatically start new experiments, allowing you to test your applications as soon as you release a new version.","title":"Imperative and declarative experiments"},{"location":"getting-started/concepts/#under-the-covers","text":"In order to execute an experiment inside Kubernetes, Iter8 uses a Kubernetes job (single-loop) or a Kubernetes cronjob (multi-loop) workload, along with a Kubernetes secret. Iter8 instantiates all experiments using a Helm chart, that is also predefined and provided by Iter8.","title":"Under the covers"},{"location":"getting-started/concepts/#implementation","text":"Iter8 is written in go and builds on a few awesome open source projects including: Helm Fortio ghz plotly.js","title":"Implementation"},{"location":"getting-started/expreport/","text":"Text HTML iter8 k report The text report looks like this Experiment summary: ******************* Experiment completed: true No task failures: true Total number of tasks: 4 Number of completed tasks: 4 Whether or not service level objectives ( SLOs ) are satisfied: ************************************************************* SLO Conditions | Satisfied -------------- | --------- http/error-count < = 0 | true http/latency-mean ( msec ) < = 50 | true Latest observed values for metrics: *********************************** Metric | value ------- | ----- http/error-count | 0 .00 http/error-rate | 0 .00 http/latency-max ( msec ) | 19 .74 http/latency-mean ( msec ) | 5 .27 http/latency-min ( msec ) | 1 .16 http/latency-p50 ( msec ) | 4 .67 http/latency-p75 ( msec ) | 7 .00 http/latency-p90 ( msec ) | 9 .50 http/latency-p95 ( msec ) | 11 .33 http/latency-p99 ( msec ) | 18 .00 http/latency-p99.9 ( msec ) | 19 .56 http/latency-stddev ( msec ) | 3 .28 http/request-count | 100 .00 iter8 k report -o html > report.html # view in a browser The HTML report looks like this","title":"Expreport"},{"location":"getting-started/help/","text":"Get Help \u00b6 Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our community meetings! Follow Iter8 on Twitter .","title":"Get help"},{"location":"getting-started/help/#get-help","text":"Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our community meetings! Follow Iter8 on Twitter .","title":"Get Help"},{"location":"getting-started/install/","text":"Install Iter8 CLI \u00b6 Brew Binaries GitHub Actions Install the latest stable release of the Iter8 CLI using brew as follows. brew tap iter8-tools/iter8 brew install iter8@0.13 Install the latest stable release of the Iter8 CLI using a compressed binary tarball. darwin-amd64 (MacOS) linux-amd64 linux-386 windows-amd64 wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-darwin-amd64.tar.gz tar -xvf iter8-darwin-amd64.tar.gz Move darwin-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-amd64.tar.gz tar -xvf iter8-linux-amd64.tar.gz Move linux-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-386.tar.gz tar -xvf iter8-linux-386.tar.gz Move linux-386/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-windows-amd64.tar.gz tar -xvf iter8-windows-amd64.tar.gz Move windows-amd64/iter8.exe to any directory in your PATH . Install the latest stable release of the Iter8 CLI in your GitHub Actions workflow as follows. - uses : iter8-tools/iter8@v0.13","title":"Install Iter8"},{"location":"getting-started/install/#install-iter8-cli","text":"Brew Binaries GitHub Actions Install the latest stable release of the Iter8 CLI using brew as follows. brew tap iter8-tools/iter8 brew install iter8@0.13 Install the latest stable release of the Iter8 CLI using a compressed binary tarball. darwin-amd64 (MacOS) linux-amd64 linux-386 windows-amd64 wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-darwin-amd64.tar.gz tar -xvf iter8-darwin-amd64.tar.gz Move darwin-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-amd64.tar.gz tar -xvf iter8-linux-amd64.tar.gz Move linux-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-386.tar.gz tar -xvf iter8-linux-386.tar.gz Move linux-386/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-windows-amd64.tar.gz tar -xvf iter8-windows-amd64.tar.gz Move windows-amd64/iter8.exe to any directory in your PATH . Install the latest stable release of the Iter8 CLI in your GitHub Actions workflow as follows. - uses : iter8-tools/iter8@v0.13","title":"Install Iter8 CLI"},{"location":"getting-started/installbrewbins/","text":"Brew Binaries Install the latest stable release of the Iter8 CLI using brew as follows. brew tap iter8-tools/iter8 brew install iter8@0.13 Install the latest stable release of the Iter8 CLI using a compressed binary tarball. darwin-amd64 (MacOS) linux-amd64 linux-386 windows-amd64 wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-darwin-amd64.tar.gz tar -xvf iter8-darwin-amd64.tar.gz Move darwin-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-amd64.tar.gz tar -xvf iter8-linux-amd64.tar.gz Move linux-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-386.tar.gz tar -xvf iter8-linux-386.tar.gz Move linux-386/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-windows-amd64.tar.gz tar -xvf iter8-windows-amd64.tar.gz Move windows-amd64/iter8.exe to any directory in your PATH .","title":"Installbrewbins"},{"location":"getting-started/installghaction/","text":"GitHub Actions Install the latest stable release of the Iter8 CLI in your GitHub Actions workflow as follows. - uses : iter8-tools/iter8@v0.13","title":"Installghaction"},{"location":"getting-started/logs/","text":"Sample experiment logs INFO [ 2022 -06-27 11 :50:39 ] inited Helm config INFO [ 2022 -06-27 11 :50:39 ] experiment logs from Kubernetes cluster indented-trace = below ... time = 2022 -06-27 15 :48:59 level = info msg = task 1 : ready: started time = 2022 -06-27 15 :48:59 level = info msg = task 1 : ready: completed time = 2022 -06-27 15 :48:59 level = info msg = task 2 : ready: started time = 2022 -06-27 15 :48:59 level = info msg = task 2 : ready: completed time = 2022 -06-27 15 :48:59 level = info msg = task 3 : http: started time = 2022 -06-27 15 :49:11 level = info msg = task 3 : http: completed time = 2022 -06-27 15 :49:11 level = info msg = task 4 : assess: started time = 2022 -06-27 15 :49:11 level = info msg = task 4 : assess: completed","title":"Logs"},{"location":"getting-started/your-first-experiment/","text":"Your First Experiment \u00b6 Run your first Iter8 experiment by load testing a Kubernetes HTTP service and validating its service-level objectives (SLOs) . This is a single-loop Kubernetes experiment . Before you begin Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Install Iter8 CLI \u00b6 Brew Binaries Install the latest stable release of the Iter8 CLI using brew as follows. brew tap iter8-tools/iter8 brew install iter8@0.13 Install the latest stable release of the Iter8 CLI using a compressed binary tarball. darwin-amd64 (MacOS) linux-amd64 linux-386 windows-amd64 wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-darwin-amd64.tar.gz tar -xvf iter8-darwin-amd64.tar.gz Move darwin-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-amd64.tar.gz tar -xvf iter8-linux-amd64.tar.gz Move linux-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-386.tar.gz tar -xvf iter8-linux-386.tar.gz Move linux-386/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-windows-amd64.tar.gz tar -xvf iter8-windows-amd64.tar.gz Move windows-amd64/iter8.exe to any directory in your PATH . Launch experiment \u00b6 Launch the Iter8 experiment inside the Kubernetes cluster. GET example POST example iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/post \\ --set http.payloadStr = hello \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , http , and assess . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to the cluster-local HTTP service using the specified url , and collects Iter8's built-in HTTP load test metrics . This tasks supports both GET and POST requests, and for POST requests, a payload can be provided by using either payloadStr or payloadURL . The assess task verifies if the app satisfies the specified SLOs: i) the mean latency of the service does not exceed 50 msec, and ii) there are no errors (4xx or 5xx response codes) in the responses. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . Assert experiment outcomes \u00b6 Assert that the experiment completed without failures, and all SLOs are satisfied. The timeout flag below specifies a period of 120 sec for assert conditions to be satisfied. iter8 k assert -c completed -c nofailure -c slos --timeout 120s If the assert conditions are satisfied, the above command exits with code 0; else, it exits with code 1. Assertions are especially useful inside CI/CD/GitOps pipelines. Depending on the exit code of the assert command, your pipeline can branch into different actions. View experiment report \u00b6 Text HTML iter8 k report The text report looks like this Experiment summary: ******************* Experiment completed: true No task failures: true Total number of tasks: 4 Number of completed tasks: 4 Whether or not service level objectives ( SLOs ) are satisfied: ************************************************************* SLO Conditions | Satisfied -------------- | --------- http/error-count < = 0 | true http/latency-mean ( msec ) < = 50 | true Latest observed values for metrics: *********************************** Metric | value ------- | ----- http/error-count | 0 .00 http/error-rate | 0 .00 http/latency-max ( msec ) | 19 .74 http/latency-mean ( msec ) | 5 .27 http/latency-min ( msec ) | 1 .16 http/latency-p50 ( msec ) | 4 .67 http/latency-p75 ( msec ) | 7 .00 http/latency-p90 ( msec ) | 9 .50 http/latency-p95 ( msec ) | 11 .33 http/latency-p99 ( msec ) | 18 .00 http/latency-p99.9 ( msec ) | 19 .56 http/latency-stddev ( msec ) | 3 .28 http/request-count | 100 .00 iter8 k report -o html > report.html # view in a browser The HTML report looks like this View experiment logs \u00b6 Logs are useful when debugging an experiment. iter8 k log Sample experiment logs INFO [ 2022 -06-27 11 :50:39 ] inited Helm config INFO [ 2022 -06-27 11 :50:39 ] experiment logs from Kubernetes cluster indented-trace = below ... time = 2022 -06-27 15 :48:59 level = info msg = task 1 : ready: started time = 2022 -06-27 15 :48:59 level = info msg = task 1 : ready: completed time = 2022 -06-27 15 :48:59 level = info msg = task 2 : ready: started time = 2022 -06-27 15 :48:59 level = info msg = task 2 : ready: completed time = 2022 -06-27 15 :48:59 level = info msg = task 3 : http: started time = 2022 -06-27 15 :49:11 level = info msg = task 3 : http: completed time = 2022 -06-27 15 :49:11 level = info msg = task 4 : assess: started time = 2022 -06-27 15 :49:11 level = info msg = task 4 : assess: completed Cleanup \u00b6 Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/httpbin kubectl delete deploy/httpbin Congratulations! You completed your first Iter8 experiment. Some variations and extensions of this experiment The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload. The assess task can be configured with SLOs for any of Iter8's built-in HTTP load test metrics .","title":"Your first experiment"},{"location":"getting-started/your-first-experiment/#your-first-experiment","text":"Run your first Iter8 experiment by load testing a Kubernetes HTTP service and validating its service-level objectives (SLOs) . This is a single-loop Kubernetes experiment . Before you begin Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80","title":"Your First Experiment"},{"location":"getting-started/your-first-experiment/#install-iter8-cli","text":"Brew Binaries Install the latest stable release of the Iter8 CLI using brew as follows. brew tap iter8-tools/iter8 brew install iter8@0.13 Install the latest stable release of the Iter8 CLI using a compressed binary tarball. darwin-amd64 (MacOS) linux-amd64 linux-386 windows-amd64 wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-darwin-amd64.tar.gz tar -xvf iter8-darwin-amd64.tar.gz Move darwin-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-amd64.tar.gz tar -xvf iter8-linux-amd64.tar.gz Move linux-amd64/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-linux-386.tar.gz tar -xvf iter8-linux-386.tar.gz Move linux-386/iter8 to any directory in your PATH . wget https://github.com/iter8-tools/iter8/releases/latest/download/iter8-windows-amd64.tar.gz tar -xvf iter8-windows-amd64.tar.gz Move windows-amd64/iter8.exe to any directory in your PATH .","title":"Install Iter8 CLI"},{"location":"getting-started/your-first-experiment/#launch-experiment","text":"Launch the Iter8 experiment inside the Kubernetes cluster. GET example POST example iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/post \\ --set http.payloadStr = hello \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , http , and assess . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to the cluster-local HTTP service using the specified url , and collects Iter8's built-in HTTP load test metrics . This tasks supports both GET and POST requests, and for POST requests, a payload can be provided by using either payloadStr or payloadURL . The assess task verifies if the app satisfies the specified SLOs: i) the mean latency of the service does not exceed 50 msec, and ii) there are no errors (4xx or 5xx response codes) in the responses. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job .","title":"Launch experiment"},{"location":"getting-started/your-first-experiment/#assert-experiment-outcomes","text":"Assert that the experiment completed without failures, and all SLOs are satisfied. The timeout flag below specifies a period of 120 sec for assert conditions to be satisfied. iter8 k assert -c completed -c nofailure -c slos --timeout 120s If the assert conditions are satisfied, the above command exits with code 0; else, it exits with code 1. Assertions are especially useful inside CI/CD/GitOps pipelines. Depending on the exit code of the assert command, your pipeline can branch into different actions.","title":"Assert experiment outcomes"},{"location":"getting-started/your-first-experiment/#view-experiment-report","text":"Text HTML iter8 k report The text report looks like this Experiment summary: ******************* Experiment completed: true No task failures: true Total number of tasks: 4 Number of completed tasks: 4 Whether or not service level objectives ( SLOs ) are satisfied: ************************************************************* SLO Conditions | Satisfied -------------- | --------- http/error-count < = 0 | true http/latency-mean ( msec ) < = 50 | true Latest observed values for metrics: *********************************** Metric | value ------- | ----- http/error-count | 0 .00 http/error-rate | 0 .00 http/latency-max ( msec ) | 19 .74 http/latency-mean ( msec ) | 5 .27 http/latency-min ( msec ) | 1 .16 http/latency-p50 ( msec ) | 4 .67 http/latency-p75 ( msec ) | 7 .00 http/latency-p90 ( msec ) | 9 .50 http/latency-p95 ( msec ) | 11 .33 http/latency-p99 ( msec ) | 18 .00 http/latency-p99.9 ( msec ) | 19 .56 http/latency-stddev ( msec ) | 3 .28 http/request-count | 100 .00 iter8 k report -o html > report.html # view in a browser The HTML report looks like this","title":"View experiment report"},{"location":"getting-started/your-first-experiment/#view-experiment-logs","text":"Logs are useful when debugging an experiment. iter8 k log Sample experiment logs INFO [ 2022 -06-27 11 :50:39 ] inited Helm config INFO [ 2022 -06-27 11 :50:39 ] experiment logs from Kubernetes cluster indented-trace = below ... time = 2022 -06-27 15 :48:59 level = info msg = task 1 : ready: started time = 2022 -06-27 15 :48:59 level = info msg = task 1 : ready: completed time = 2022 -06-27 15 :48:59 level = info msg = task 2 : ready: started time = 2022 -06-27 15 :48:59 level = info msg = task 2 : ready: completed time = 2022 -06-27 15 :48:59 level = info msg = task 3 : http: started time = 2022 -06-27 15 :49:11 level = info msg = task 3 : http: completed time = 2022 -06-27 15 :49:11 level = info msg = task 4 : assess: started time = 2022 -06-27 15 :49:11 level = info msg = task 4 : assess: completed","title":"View experiment logs"},{"location":"getting-started/your-first-experiment/#cleanup","text":"Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/httpbin kubectl delete deploy/httpbin Congratulations! You completed your first Iter8 experiment. Some variations and extensions of this experiment The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload. The assess task can be configured with SLOs for any of Iter8's built-in HTTP load test metrics .","title":"Cleanup"},{"location":"metrics/custom-metrics/","text":"Custom Metrics \u00b6 Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments. Metrics with/without auth \u00b6 Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 url : {{ default .istioPromURL \"http : //prometheus.istio-system : 9090/api/v1/query\" }} provider : istio-prom method : GET metrics : - name : request-count type : counter description : | Number of requests params : - name : query value : | sum(last_over_time(istio_requests_total{ {{- template \"labels\" . }} }[{{ .elapsedTimeSeconds }}s])) or on() vector(0) jqExpression : .data.result[0].value[1] | tonumber Brief explanation of the request-count metric The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The url field provides the URL of the Prometheus service. The method field provides the HTTP method, in this case GET . The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. Placeholder substitution \u00b6 Note: This step is automated by Iter8 . Prometheus The placeholder elapsedTimeSeconds is substituted based on the start of the experiment or startingTime , if provided in the CLI. If startingTime is provided, then the time should follow RFC 3339 (for example: 2020-02-01T09:44:40Z or 2020-02-01T09:44:40.954641934Z ). JSON response \u00b6 Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } Processing the JSON response \u00b6 Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression . Error handling \u00b6 Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9","title":"Custom Metrics"},{"location":"metrics/custom-metrics/#custom-metrics","text":"Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments.","title":"Custom Metrics"},{"location":"metrics/custom-metrics/#metrics-withwithout-auth","text":"Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 url : {{ default .istioPromURL \"http : //prometheus.istio-system : 9090/api/v1/query\" }} provider : istio-prom method : GET metrics : - name : request-count type : counter description : | Number of requests params : - name : query value : | sum(last_over_time(istio_requests_total{ {{- template \"labels\" . }} }[{{ .elapsedTimeSeconds }}s])) or on() vector(0) jqExpression : .data.result[0].value[1] | tonumber Brief explanation of the request-count metric The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The url field provides the URL of the Prometheus service. The method field provides the HTTP method, in this case GET . The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus.","title":"Metrics with/without auth"},{"location":"metrics/custom-metrics/#placeholder-substitution","text":"Note: This step is automated by Iter8 . Prometheus The placeholder elapsedTimeSeconds is substituted based on the start of the experiment or startingTime , if provided in the CLI. If startingTime is provided, then the time should follow RFC 3339 (for example: 2020-02-01T09:44:40Z or 2020-02-01T09:44:40.954641934Z ).","title":"Placeholder substitution"},{"location":"metrics/custom-metrics/#json-response","text":"Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } }","title":"JSON response"},{"location":"metrics/custom-metrics/#processing-the-json-response","text":"Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression .","title":"Processing the JSON response"},{"location":"metrics/custom-metrics/#error-handling","text":"Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9","title":"Error handling"},{"location":"tutorials/load-test-grpc-multiple/","text":"Load Test Multiple gRPC methods \u00b6 Load Test gRPC with SLOs describes how to load test a single method from a gRPC service inside Kubernetes. This tutorial expands on the previous tutorial and describes how to load test multiple endpoints from an HTTP service. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i '' 's/localhost//' server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051 Launch experiment \u00b6 iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-getFeature/latency/mean = 50 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/latency/mean = 100 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , grpc , and assess . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to two methods of the cluster-local gRPC service, and collects Iter8's built-in gRPC load test metrics . The two methods are routeguide.RouteGuide.GetFeature and routeguide.RouteGuide.ListFeatures . Note that each method also has its own dataURL for the request payload. The assess task verifies if each method satisfies their respective error rate and mean latency SLOs. Both methods must have an error rate of 0 but the getFeature and listFeatures methods are allowed a maximum mean latency of 50 and 100 msecs, respectively. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . Some variations and extensions of this experiment The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections. The assess task can be configured with SLOs for any of Iter8's built-in grpc load test metrics . Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in your first experiment . Cleanup \u00b6 Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/routeguide kubectl delete deploy/routeguide","title":"Load test multiple gRPC methods"},{"location":"tutorials/load-test-grpc-multiple/#load-test-multiple-grpc-methods","text":"Load Test gRPC with SLOs describes how to load test a single method from a gRPC service inside Kubernetes. This tutorial expands on the previous tutorial and describes how to load test multiple endpoints from an HTTP service. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i '' 's/localhost//' server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051","title":"Load Test Multiple gRPC methods"},{"location":"tutorials/load-test-grpc-multiple/#launch-experiment","text":"iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-getFeature/latency/mean = 50 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/latency/mean = 100 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , grpc , and assess . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to two methods of the cluster-local gRPC service, and collects Iter8's built-in gRPC load test metrics . The two methods are routeguide.RouteGuide.GetFeature and routeguide.RouteGuide.ListFeatures . Note that each method also has its own dataURL for the request payload. The assess task verifies if each method satisfies their respective error rate and mean latency SLOs. Both methods must have an error rate of 0 but the getFeature and listFeatures methods are allowed a maximum mean latency of 50 and 100 msecs, respectively. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . Some variations and extensions of this experiment The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections. The assess task can be configured with SLOs for any of Iter8's built-in grpc load test metrics . Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in your first experiment .","title":"Launch experiment"},{"location":"tutorials/load-test-grpc-multiple/#cleanup","text":"Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/routeguide kubectl delete deploy/routeguide","title":"Cleanup"},{"location":"tutorials/load-test-grpc/","text":"Load Test gRPC with SLOs \u00b6 Load test a Kubernetes gRPC service and validate its service-level objectives (SLOs) . This is a single-loop Kubernetes experiment . See Load Test multiple gRPC methods to see a tutorial that describes how to load test multiple methods from an gRPC service. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i '' 's/localhost//' server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051 Launch experiment \u00b6 Unary example Server streaming example Client streaming example Bidirectional example iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job iter8 k launch \\ --set tasks ={ ready,grpc,assess } \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RouteChat \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/bidirectional.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , grpc , and assess . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to the specified method of the cluster-local gRPC service with host address routeguide.default:50051 and collects Iter8's built-in gRPC load test metrics . This task supports all four gRPC service methods: unary, server streaming, client streaming, and bidirectional streaming, and will provide payload in the appropriate manner using dataURL . The assess task verifies if the app satisfies the specified SLOs: i) there are no errors, ii) the mean latency of the service does not exceed 50 msec, and iii) the 97.5 th percentile latency does not exceed 200 msec. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . Some variations and extensions of this experiment The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections. The assess task can be configured with SLOs for any of Iter8's built-in grpc load test metrics . Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in your first experiment . Cleanup \u00b6 Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/routeguide kubectl delete deploy/routeguide","title":"Load test gRPC with SLOs"},{"location":"tutorials/load-test-grpc/#load-test-grpc-with-slos","text":"Load test a Kubernetes gRPC service and validate its service-level objectives (SLOs) . This is a single-loop Kubernetes experiment . See Load Test multiple gRPC methods to see a tutorial that describes how to load test multiple methods from an gRPC service. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i '' 's/localhost//' server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051","title":"Load Test gRPC with SLOs"},{"location":"tutorials/load-test-grpc/#launch-experiment","text":"Unary example Server streaming example Client streaming example Bidirectional example iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job iter8 k launch \\ --set tasks ={ ready,grpc,assess } \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RouteChat \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/bidirectional.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 800 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , grpc , and assess . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to the specified method of the cluster-local gRPC service with host address routeguide.default:50051 and collects Iter8's built-in gRPC load test metrics . This task supports all four gRPC service methods: unary, server streaming, client streaming, and bidirectional streaming, and will provide payload in the appropriate manner using dataURL . The assess task verifies if the app satisfies the specified SLOs: i) there are no errors, ii) the mean latency of the service does not exceed 50 msec, and iii) the 97.5 th percentile latency does not exceed 200 msec. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . Some variations and extensions of this experiment The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections. The assess task can be configured with SLOs for any of Iter8's built-in grpc load test metrics . Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in your first experiment .","title":"Launch experiment"},{"location":"tutorials/load-test-grpc/#cleanup","text":"Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/routeguide kubectl delete deploy/routeguide","title":"Cleanup"},{"location":"tutorials/load-test-http-multiple/","text":"Load Test Multiple HTTP endpoints \u00b6 Your first experiment describes how to load test a single endpoint from an HTTP service inside Kubernetes. This tutorial expands on the previous tutorial and describes how to load test multiple endpoints from an HTTP service. Before you begin Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch experiment \u00b6 Launch the Iter8 experiment inside the Kubernetes cluster. iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-get/latency-mean = 50 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/latency-mean = 100 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set assess.SLOs.upper.http-post/latency-mean = 150 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , http , and assess . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to three endpoints from the cluster-local HTTP service, and collects Iter8's built-in HTTP load test metrics . The three endpoints are http://httpbin.default/get , http://httpbin.default/anything , and http://httpbin.default/post . The last endpoint also has a payload string hello . The assess task verifies if each endpoint satisfies their respective error count and mean latency SLOs. All three must have an error count of 0 but the get , getAnything , and post endpoints are allowed a maximum mean latency of 50, 100, and 150 msecs, respectively. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in your first experiment . Cleanup \u00b6 Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/httpbin kubectl delete deploy/httpbin","title":"Load test multiple HTTP endpoints"},{"location":"tutorials/load-test-http-multiple/#load-test-multiple-http-endpoints","text":"Your first experiment describes how to load test a single endpoint from an HTTP service inside Kubernetes. This tutorial expands on the previous tutorial and describes how to load test multiple endpoints from an HTTP service. Before you begin Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80","title":"Load Test Multiple HTTP endpoints"},{"location":"tutorials/load-test-http-multiple/#launch-experiment","text":"Launch the Iter8 experiment inside the Kubernetes cluster. iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-get/latency-mean = 50 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/latency-mean = 100 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set assess.SLOs.upper.http-post/latency-mean = 150 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , http , and assess . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to three endpoints from the cluster-local HTTP service, and collects Iter8's built-in HTTP load test metrics . The three endpoints are http://httpbin.default/get , http://httpbin.default/anything , and http://httpbin.default/post . The last endpoint also has a payload string hello . The assess task verifies if each endpoint satisfies their respective error count and mean latency SLOs. All three must have an error count of 0 but the get , getAnything , and post endpoints are allowed a maximum mean latency of 50, 100, and 150 msecs, respectively. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in your first experiment .","title":"Launch experiment"},{"location":"tutorials/load-test-http-multiple/#cleanup","text":"Remove the Iter8 experiment and the sample app from the Kubernetes cluster and the local Iter8 charts folder. iter8 k delete kubectl delete svc/httpbin kubectl delete deploy/httpbin","title":"Cleanup"},{"location":"tutorials/abn/abn/","text":"A/B Experiments \u00b6 This tutorial describes an A/B testing experiment for a backend component. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Launch Iter8 A/B/n service \u00b6 Deploy the Iter8 A/B/n service. When deploying the service, specify which Kubernetes resource types to watch for each application. To watch for versions of the backend application in the default namespace, configure the service to watch for Kubernetes service and deployment resources: helm install --repo https://iter8-tools.github.io/hub iter8-abn abn \\ --set \"apps.default.backend.resources={service,deployment}\" Assumptions To simplify specification, Iter8 assumes certain conventions: The baseline track identifier is the application name Track identifiers associated with candidate versions are of the form <application_name>-candidate-<index> All resource objects for all versions are deployed in the same namespace There is only 1 resource object of a given type in each version The name of each object in the version associated with the baseline track is the application name The name of each object in the version associated with a candidate track is of the form <application_name>-candidate-<index> where index is 1, 2, etc. Deploy the sample application \u00b6 Deploy both the frontend and backend components of the application as described in each tab: frontend backend Install the frontend component using an implementation in the language of your choice: node Go kubectl create deployment frontend --image = iter8/abn-sample-frontend-node:0.13 kubectl expose deployment frontend --name = frontend --port = 8090 kubectl create deployment frontend --image = iter8/abn-sample-frontend-go:0.13 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call Lookup() before each call to the backend component. The frontend componet uses the returned track identifier to route the request to a version of the backend component. Deploy version v1 of the backend component, associating it with the track identifier backend . kubectl create deployment backend --image = iter8/abn-sample-backend:0.13-v1 kubectl label deployment backend app.kubernetes.io/version = v1 kubectl expose deployment backend --name = backend --port = 8091 Generate load \u00b6 Generate load. In separate shells, port-forward requests to the frontend component and generate load for multiple users. A script is provided to do this. To use it: kubectl port-forward service/frontend 8090 :8090 curl -s https://raw.githubusercontent.com/iter8-tools/docs/main/samples/abn-sample/generate_load.sh | sh -s -- Deploy a candidate version \u00b6 Deploy version v2 of the backend component, associating it with the track identifier backend-candidate-1 . kubectl create deployment backend-candidate-1 --image = iter8/abn-sample-backend:0.13-v2 kubectl label deployment backend-candidate-1 app.kubernetes.io/version = v2 kubectl expose deployment backend-candidate-1 --name = backend-candidate-1 --port = 8091 Until the candidate version is ready; that is, until all expected resources are deployed and available, calls to Lookup() will return only the backend track identifier. Once the candidate version is ready, Lookup() will return both track identifiers so that requests will be distributed between versions. Launch experiment \u00b6 iter8 k launch \\ --set abnmetrics.application = default/backend \\ --set \"tasks={abnmetrics}\" \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment periodically (in this case, once a minute) reads the abn metrics associated with the backend application component in the default namespace. These metrics are written by the frontend component using the WriteMetric() interface as a part of processing user requests. Inspect experiment report \u00b6 Inspect the metrics: iter8 k report Sample output from report Experiment summary: ******************* Experiment completed: true No task failures: true Total number of tasks: 1 Number of completed tasks: 1 Number of completed loops: 3 Latest observed values for metrics: *********************************** Metric | backend (v1) | backend-candidate-1 (v2) ------- | ----- | ----- abn/sample_metric/count | 35.00 | 28.00 abn/sample_metric/max | 99.00 | 100.00 abn/sample_metric/mean | 56.31 | 52.79 abn/sample_metric/min | 0.00 | 1.00 abn/sample_metric/stddev | 28.52 | 31.91 The output allows you to compare the versions against each other and select a winner. Since the experiment runs periodically, the values in the report will change over time. Once a winner is identified, the experiment can be terminated, the winner can be promoted, and the candidate version(s) can be deleted. To delete the experiment: iter8 k delete Promote candidate version \u00b6 Delete the candidate version: kubectl delete deployment backend-candidate-1 kubectl delete service backend-candidate-1 Update the version associated with the baseline track identifier backend : kubectl set image deployment/backend abn-sample-backend = iter8/abn-sample-backend:0.13-v2 kubectl label --overwrite deployment/backend app.kubernetes.io/version = v2 Cleanup \u00b6 Delete sample application \u00b6 kubectl delete \\ deploy/frontend deploy/backend deploy/backend-candidate-1 \\ service/frontend service/backend service/backend-candidate-1 Uninstall the A/B/n service \u00b6 helm delete iter8-abn","title":"A/B experiments"},{"location":"tutorials/abn/abn/#ab-experiments","text":"This tutorial describes an A/B testing experiment for a backend component. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments.","title":"A/B Experiments"},{"location":"tutorials/abn/abn/#launch-iter8-abn-service","text":"Deploy the Iter8 A/B/n service. When deploying the service, specify which Kubernetes resource types to watch for each application. To watch for versions of the backend application in the default namespace, configure the service to watch for Kubernetes service and deployment resources: helm install --repo https://iter8-tools.github.io/hub iter8-abn abn \\ --set \"apps.default.backend.resources={service,deployment}\" Assumptions To simplify specification, Iter8 assumes certain conventions: The baseline track identifier is the application name Track identifiers associated with candidate versions are of the form <application_name>-candidate-<index> All resource objects for all versions are deployed in the same namespace There is only 1 resource object of a given type in each version The name of each object in the version associated with the baseline track is the application name The name of each object in the version associated with a candidate track is of the form <application_name>-candidate-<index> where index is 1, 2, etc.","title":"Launch Iter8 A/B/n service"},{"location":"tutorials/abn/abn/#deploy-the-sample-application","text":"Deploy both the frontend and backend components of the application as described in each tab: frontend backend Install the frontend component using an implementation in the language of your choice: node Go kubectl create deployment frontend --image = iter8/abn-sample-frontend-node:0.13 kubectl expose deployment frontend --name = frontend --port = 8090 kubectl create deployment frontend --image = iter8/abn-sample-frontend-go:0.13 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call Lookup() before each call to the backend component. The frontend componet uses the returned track identifier to route the request to a version of the backend component. Deploy version v1 of the backend component, associating it with the track identifier backend . kubectl create deployment backend --image = iter8/abn-sample-backend:0.13-v1 kubectl label deployment backend app.kubernetes.io/version = v1 kubectl expose deployment backend --name = backend --port = 8091","title":"Deploy the sample application"},{"location":"tutorials/abn/abn/#generate-load","text":"Generate load. In separate shells, port-forward requests to the frontend component and generate load for multiple users. A script is provided to do this. To use it: kubectl port-forward service/frontend 8090 :8090 curl -s https://raw.githubusercontent.com/iter8-tools/docs/main/samples/abn-sample/generate_load.sh | sh -s --","title":"Generate load"},{"location":"tutorials/abn/abn/#deploy-a-candidate-version","text":"Deploy version v2 of the backend component, associating it with the track identifier backend-candidate-1 . kubectl create deployment backend-candidate-1 --image = iter8/abn-sample-backend:0.13-v2 kubectl label deployment backend-candidate-1 app.kubernetes.io/version = v2 kubectl expose deployment backend-candidate-1 --name = backend-candidate-1 --port = 8091 Until the candidate version is ready; that is, until all expected resources are deployed and available, calls to Lookup() will return only the backend track identifier. Once the candidate version is ready, Lookup() will return both track identifiers so that requests will be distributed between versions.","title":"Deploy a candidate version"},{"location":"tutorials/abn/abn/#launch-experiment","text":"iter8 k launch \\ --set abnmetrics.application = default/backend \\ --set \"tasks={abnmetrics}\" \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment periodically (in this case, once a minute) reads the abn metrics associated with the backend application component in the default namespace. These metrics are written by the frontend component using the WriteMetric() interface as a part of processing user requests.","title":"Launch experiment"},{"location":"tutorials/abn/abn/#inspect-experiment-report","text":"Inspect the metrics: iter8 k report Sample output from report Experiment summary: ******************* Experiment completed: true No task failures: true Total number of tasks: 1 Number of completed tasks: 1 Number of completed loops: 3 Latest observed values for metrics: *********************************** Metric | backend (v1) | backend-candidate-1 (v2) ------- | ----- | ----- abn/sample_metric/count | 35.00 | 28.00 abn/sample_metric/max | 99.00 | 100.00 abn/sample_metric/mean | 56.31 | 52.79 abn/sample_metric/min | 0.00 | 1.00 abn/sample_metric/stddev | 28.52 | 31.91 The output allows you to compare the versions against each other and select a winner. Since the experiment runs periodically, the values in the report will change over time. Once a winner is identified, the experiment can be terminated, the winner can be promoted, and the candidate version(s) can be deleted. To delete the experiment: iter8 k delete","title":"Inspect experiment report"},{"location":"tutorials/abn/abn/#promote-candidate-version","text":"Delete the candidate version: kubectl delete deployment backend-candidate-1 kubectl delete service backend-candidate-1 Update the version associated with the baseline track identifier backend : kubectl set image deployment/backend abn-sample-backend = iter8/abn-sample-backend:0.13-v2 kubectl label --overwrite deployment/backend app.kubernetes.io/version = v2","title":"Promote candidate version"},{"location":"tutorials/abn/abn/#cleanup","text":"","title":"Cleanup"},{"location":"tutorials/abn/abn/#delete-sample-application","text":"kubectl delete \\ deploy/frontend deploy/backend deploy/backend-candidate-1 \\ service/frontend service/backend service/backend-candidate-1","title":"Delete sample application"},{"location":"tutorials/abn/abn/#uninstall-the-abn-service","text":"helm delete iter8-abn","title":"Uninstall the A/B/n service"},{"location":"tutorials/autox/autox/","text":"Automated Experiments: AutoX \u00b6 AutoX, short for \"automated experiments\", allows Iter8 to detect changes to your Kubernetes resources objects and automatically start new experiments, allowing you to test your services as soon as you release a new version. Releasing a new version of an application typically involves the creation of new Kubernetes resource objects and/or updates to existing ones. AutoX can be configured to watch for such changes and automatically launch new experiments. You can configure AutoX with multiple experiment groups and, for each group, specify the Kubernetes resource object that AutoX will watch and one or more experiments to be performed in response to new versions of this object. Let us now see this in action using a Kubernetes HTTP service and configuring AutoX so that whenever a new version of the service is released, AutoX will start a new HTTP performance test that will validate if the service meets latency and error-related requirements. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Setup Kubernetes cluster with ArgoCD \u00b6 AutoX uses Argo CD , a popular continuous delivery tool, in order to launch the experiments. A basic install of Argo CD can be done as follows: kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml See here for more information about installation. Deploy application \u00b6 Now, we will create an httpbin deployment and service. kubectl create deployment httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deployment httpbin --port = 80 Apply version label \u00b6 Next, we will assign httpbin deployment the app.kubernetes.io/version label (version label). AutoX will launch experiments only when this label is present on your trigger object. It will relaunch experiments whenever this version label is modified. kubectl label deployment httpbin app.kubernetes.io/version = 1 .0.0 Setup Kubernetes cluster with Iter8 AutoX \u00b6 Next, we will configure and install the AutoX controller. helm install autox autox --repo https://iter8-tools.github.io/hub/ --version 0 .1.6 \\ --set 'groups.httpbin.trigger.name=httpbin' \\ --set 'groups.httpbin.trigger.namespace=default' \\ --set 'groups.httpbin.trigger.group=apps' \\ --set 'groups.httpbin.trigger.version=v1' \\ --set 'groups.httpbin.trigger.resource=deployments' \\ --set 'groups.httpbin.specs.iter8.name=iter8' \\ --set 'groups.httpbin.specs.iter8.values.tasks={ready,http,assess}' \\ --set 'groups.httpbin.specs.iter8.values.ready.deploy=httpbin' \\ --set 'groups.httpbin.specs.iter8.values.ready.service=httpbin' \\ --set 'groups.httpbin.specs.iter8.values.ready.timeout=60s' \\ --set 'groups.httpbin.specs.iter8.values.http.url=http://httpbin.default/get' \\ --set 'groups.httpbin.specs.iter8.values.assess.SLOs.upper.http/error-count=0' \\ --set 'groups.httpbin.specs.iter8.values.assess.SLOs.upper.http/latency-mean=50' \\ --set 'groups.httpbin.specs.iter8.version=0.13.0' \\ --set 'groups.httpbin.specs.iter8.values.runner=job' The configuration of the AutoX controller is composed of a trigger object definition and a set of experiment specifications . In this case, the trigger object is the httpbin deployment and there is only one experiment, an HTTP performance test with SLO validation associated with this trigger. To go into more detail, the configuration is a set of groups , and each group is composed of a trigger object definition and a set of experiment specifications . This enables AutoX to manage one or more trigger objects, each associated with one or more experiments. In this tutorial, there is only one group named httpbin ( groups.httpbin... ), and within that group, there is the trigger object definition ( groups.httpbin.trigger... ) and a single experiment spec named iter8 ( groups.httpbin.specs.iter8... ). The trigger object definition is a combination of the name, namespace, and the group-version-resource (GVR) metadata of the trigger object, in this case httpbin , default , and GVR apps , deployments , and v1 , respectively. The experiment is an HTTP SLO validation test on the httpbin service that is described in greater detail here . This Iter8 experiment is composed of three tasks, ready , http , and assess . The ready task will ensure that the httpbin deployment and service are running. The http task will make requests to the specified URL and will collect latency and error-related metrics. Lastly, the assess task will ensure that the mean latency is less than 50 milliseconds and the error count is 0. In addition, the runner is set to job as this will be a single-loop experiment . Observe experiment \u00b6 After starting AutoX, the HTTP SLO validation test should quickly follow. You can now use the Iter8 CLI in order to check the status and see the results of the test. The following command allows you to check the status of the test. Note that you need to specify an experiment group via the -g option. The experiment group for experiments started by AutoX is in the form autox-<group name>-<experiment spec name> so in this case, it would be autox-httpbin-iter8 . iter8 k assert -c completed -c nofailure -c slos -g autox-httpbin-iter8 Sample output from assert INFO[2023-01-11 14:43:45] inited Helm config INFO[2023-01-11 14:43:45] experiment has no failure INFO[2023-01-11 14:43:45] SLOs are satisfied INFO[2023-01-11 14:43:45] all conditions were satisfied We can see in the sample output that the test has completed, there were no failures, and all SLOs and conditions were satisfied. And the following command allows you to check the results of the experiment. iter8 k report -g autox-httpbin-iter8 Sample output from report Experiment summary: ******************* Experiment completed: true No task failures: true Total number of tasks: 4 Number of completed tasks: 4 Whether or not service level objectives (SLOs) are satisfied: ************************************************************* SLO Conditions | Satisfied -------------- | --------- http/error-count <= 0 | true http/latency-mean (msec) <= 50 | true Latest observed values for metrics: *********************************** Metric | value ------- | ----- http/error-count | 0.00 http/error-rate | 0.00 http/latency-max (msec) | 25.11 http/latency-mean (msec) | 5.59 http/latency-min (msec) | 1.29 http/latency-p50 (msec) | 4.39 http/latency-p75 (msec) | 6.71 http/latency-p90 (msec) | 10.40 http/latency-p95 (msec) | 13.00 http/latency-p99 (msec) | 25.00 http/latency-p99.9 (msec) | 25.10 http/latency-stddev (msec) | 4.37 http/request-count | 100.00 In the sample output, we can see an experiment summary, a list of SLOs and whether they were satisfied or not, as well as any additional metrics that were collected as part of the experiment. You can also produce an HTML report that you can view in the browser. iter8 k report -g autox-httpbin-iter8 -o html > report.html Sample output from HTTP report Continuous and automated experimentation \u00b6 Now that AutoX is watching the httpbin deployment, release a new version will relaunch the HTTP SLO validation test. The version update must be accompanied by a change to the deployment's app.kubernetes.io/version label (version label); otherwise, AutoX will not do anything. For simplicity, we will simply change the version label to the deployment in order to relaunch the HTTP SLO validation test. In the real world, a new version would typically involve a change to the deployment spec (e.g., the container image) and this change should be accompanied by a change to the version label. kubectl label deployment httpbin app.kubernetes.io/version = 2 .0.0 --overwrite Observe new experiment \u00b6 Check if a new experiment has been launched. Refer to Observe experiment for the necessary commands. If we were to continue to update the deployment (and change its version label), then AutoX would relaunch the experiment for each such change. Next steps \u00b6 Firstly, the HTTP SLO validation test is flexible, and you can augment it in a number of ways, such as adding headers, providing a payload, or modulating the query rate. To learn more, see the documentation for the httpbin task. AutoX is designed to use any Kubernetes resource object (including those with a custom resource type) as a trigger object in AutoX. For example, the trigger object can be a Knative service, a KServe inference service, or a Seldon deployment. AutoX is designed to automate a variety of experiments. For example, instead of using the http task, you can use grpc task in order to run an gRPC SLO validation test. Here is the documentation for the grpc task as well as a tutorial for gRPC SLO Validation. Furthermore, you can add additional tasks that ship out-of-the-box with Iter8, in order to enrich the experiments. For example, you can add a slack task so that your experiment results will be posted on Slack. That way, you can automatically have the latest performance statistics after every update. Here is the documentation for the slack task as well as a tutorial for using the Slack task. You can also automate experiments that are not from Iter8. For example, a Litmus Chaos chaos experiment is available on Iter8 hub, which can also be configured with AutoX. Lastly, recall that you can provide multiple groups and experiment specs so AutoX can launch and manage a whole suite of experiments for multiple Kubernetes applications and namespaces. Clean up \u00b6 # delete AutoX controller and any experiments started by AutoX helm delete autox # delete trigger and service kubectl delete deployment/httpbin kubectl delete service/httpbin # delete ArgoCD kubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml","title":"Automated experiments"},{"location":"tutorials/autox/autox/#automated-experiments-autox","text":"AutoX, short for \"automated experiments\", allows Iter8 to detect changes to your Kubernetes resources objects and automatically start new experiments, allowing you to test your services as soon as you release a new version. Releasing a new version of an application typically involves the creation of new Kubernetes resource objects and/or updates to existing ones. AutoX can be configured to watch for such changes and automatically launch new experiments. You can configure AutoX with multiple experiment groups and, for each group, specify the Kubernetes resource object that AutoX will watch and one or more experiments to be performed in response to new versions of this object. Let us now see this in action using a Kubernetes HTTP service and configuring AutoX so that whenever a new version of the service is released, AutoX will start a new HTTP performance test that will validate if the service meets latency and error-related requirements. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments.","title":"Automated Experiments: AutoX"},{"location":"tutorials/autox/autox/#setup-kubernetes-cluster-with-argocd","text":"AutoX uses Argo CD , a popular continuous delivery tool, in order to launch the experiments. A basic install of Argo CD can be done as follows: kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml See here for more information about installation.","title":"Setup Kubernetes cluster with ArgoCD"},{"location":"tutorials/autox/autox/#deploy-application","text":"Now, we will create an httpbin deployment and service. kubectl create deployment httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deployment httpbin --port = 80","title":"Deploy application"},{"location":"tutorials/autox/autox/#apply-version-label","text":"Next, we will assign httpbin deployment the app.kubernetes.io/version label (version label). AutoX will launch experiments only when this label is present on your trigger object. It will relaunch experiments whenever this version label is modified. kubectl label deployment httpbin app.kubernetes.io/version = 1 .0.0","title":"Apply version label"},{"location":"tutorials/autox/autox/#setup-kubernetes-cluster-with-iter8-autox","text":"Next, we will configure and install the AutoX controller. helm install autox autox --repo https://iter8-tools.github.io/hub/ --version 0 .1.6 \\ --set 'groups.httpbin.trigger.name=httpbin' \\ --set 'groups.httpbin.trigger.namespace=default' \\ --set 'groups.httpbin.trigger.group=apps' \\ --set 'groups.httpbin.trigger.version=v1' \\ --set 'groups.httpbin.trigger.resource=deployments' \\ --set 'groups.httpbin.specs.iter8.name=iter8' \\ --set 'groups.httpbin.specs.iter8.values.tasks={ready,http,assess}' \\ --set 'groups.httpbin.specs.iter8.values.ready.deploy=httpbin' \\ --set 'groups.httpbin.specs.iter8.values.ready.service=httpbin' \\ --set 'groups.httpbin.specs.iter8.values.ready.timeout=60s' \\ --set 'groups.httpbin.specs.iter8.values.http.url=http://httpbin.default/get' \\ --set 'groups.httpbin.specs.iter8.values.assess.SLOs.upper.http/error-count=0' \\ --set 'groups.httpbin.specs.iter8.values.assess.SLOs.upper.http/latency-mean=50' \\ --set 'groups.httpbin.specs.iter8.version=0.13.0' \\ --set 'groups.httpbin.specs.iter8.values.runner=job' The configuration of the AutoX controller is composed of a trigger object definition and a set of experiment specifications . In this case, the trigger object is the httpbin deployment and there is only one experiment, an HTTP performance test with SLO validation associated with this trigger. To go into more detail, the configuration is a set of groups , and each group is composed of a trigger object definition and a set of experiment specifications . This enables AutoX to manage one or more trigger objects, each associated with one or more experiments. In this tutorial, there is only one group named httpbin ( groups.httpbin... ), and within that group, there is the trigger object definition ( groups.httpbin.trigger... ) and a single experiment spec named iter8 ( groups.httpbin.specs.iter8... ). The trigger object definition is a combination of the name, namespace, and the group-version-resource (GVR) metadata of the trigger object, in this case httpbin , default , and GVR apps , deployments , and v1 , respectively. The experiment is an HTTP SLO validation test on the httpbin service that is described in greater detail here . This Iter8 experiment is composed of three tasks, ready , http , and assess . The ready task will ensure that the httpbin deployment and service are running. The http task will make requests to the specified URL and will collect latency and error-related metrics. Lastly, the assess task will ensure that the mean latency is less than 50 milliseconds and the error count is 0. In addition, the runner is set to job as this will be a single-loop experiment .","title":"Setup Kubernetes cluster with Iter8 AutoX"},{"location":"tutorials/autox/autox/#observe-experiment","text":"After starting AutoX, the HTTP SLO validation test should quickly follow. You can now use the Iter8 CLI in order to check the status and see the results of the test. The following command allows you to check the status of the test. Note that you need to specify an experiment group via the -g option. The experiment group for experiments started by AutoX is in the form autox-<group name>-<experiment spec name> so in this case, it would be autox-httpbin-iter8 . iter8 k assert -c completed -c nofailure -c slos -g autox-httpbin-iter8 Sample output from assert INFO[2023-01-11 14:43:45] inited Helm config INFO[2023-01-11 14:43:45] experiment has no failure INFO[2023-01-11 14:43:45] SLOs are satisfied INFO[2023-01-11 14:43:45] all conditions were satisfied We can see in the sample output that the test has completed, there were no failures, and all SLOs and conditions were satisfied. And the following command allows you to check the results of the experiment. iter8 k report -g autox-httpbin-iter8 Sample output from report Experiment summary: ******************* Experiment completed: true No task failures: true Total number of tasks: 4 Number of completed tasks: 4 Whether or not service level objectives (SLOs) are satisfied: ************************************************************* SLO Conditions | Satisfied -------------- | --------- http/error-count <= 0 | true http/latency-mean (msec) <= 50 | true Latest observed values for metrics: *********************************** Metric | value ------- | ----- http/error-count | 0.00 http/error-rate | 0.00 http/latency-max (msec) | 25.11 http/latency-mean (msec) | 5.59 http/latency-min (msec) | 1.29 http/latency-p50 (msec) | 4.39 http/latency-p75 (msec) | 6.71 http/latency-p90 (msec) | 10.40 http/latency-p95 (msec) | 13.00 http/latency-p99 (msec) | 25.00 http/latency-p99.9 (msec) | 25.10 http/latency-stddev (msec) | 4.37 http/request-count | 100.00 In the sample output, we can see an experiment summary, a list of SLOs and whether they were satisfied or not, as well as any additional metrics that were collected as part of the experiment. You can also produce an HTML report that you can view in the browser. iter8 k report -g autox-httpbin-iter8 -o html > report.html Sample output from HTTP report","title":"Observe experiment"},{"location":"tutorials/autox/autox/#continuous-and-automated-experimentation","text":"Now that AutoX is watching the httpbin deployment, release a new version will relaunch the HTTP SLO validation test. The version update must be accompanied by a change to the deployment's app.kubernetes.io/version label (version label); otherwise, AutoX will not do anything. For simplicity, we will simply change the version label to the deployment in order to relaunch the HTTP SLO validation test. In the real world, a new version would typically involve a change to the deployment spec (e.g., the container image) and this change should be accompanied by a change to the version label. kubectl label deployment httpbin app.kubernetes.io/version = 2 .0.0 --overwrite","title":"Continuous and automated experimentation"},{"location":"tutorials/autox/autox/#observe-new-experiment","text":"Check if a new experiment has been launched. Refer to Observe experiment for the necessary commands. If we were to continue to update the deployment (and change its version label), then AutoX would relaunch the experiment for each such change.","title":"Observe new experiment"},{"location":"tutorials/autox/autox/#next-steps","text":"Firstly, the HTTP SLO validation test is flexible, and you can augment it in a number of ways, such as adding headers, providing a payload, or modulating the query rate. To learn more, see the documentation for the httpbin task. AutoX is designed to use any Kubernetes resource object (including those with a custom resource type) as a trigger object in AutoX. For example, the trigger object can be a Knative service, a KServe inference service, or a Seldon deployment. AutoX is designed to automate a variety of experiments. For example, instead of using the http task, you can use grpc task in order to run an gRPC SLO validation test. Here is the documentation for the grpc task as well as a tutorial for gRPC SLO Validation. Furthermore, you can add additional tasks that ship out-of-the-box with Iter8, in order to enrich the experiments. For example, you can add a slack task so that your experiment results will be posted on Slack. That way, you can automatically have the latest performance statistics after every update. Here is the documentation for the slack task as well as a tutorial for using the Slack task. You can also automate experiments that are not from Iter8. For example, a Litmus Chaos chaos experiment is available on Iter8 hub, which can also be configured with AutoX. Lastly, recall that you can provide multiple groups and experiment specs so AutoX can launch and manage a whole suite of experiments for multiple Kubernetes applications and namespaces.","title":"Next steps"},{"location":"tutorials/autox/autox/#clean-up","text":"# delete AutoX controller and any experiments started by AutoX helm delete autox # delete trigger and service kubectl delete deployment/httpbin kubectl delete service/httpbin # delete ArgoCD kubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml","title":"Clean up"},{"location":"tutorials/chaos/slo-validation-chaos/","text":"Chaos Injection with SLOs \u00b6 Perform a joint Iter8 and LitmusChaos experiment. This joint experiment enables you to verify if an app continues to be resilient (satisfies SLOs) in the midst of chaos (pod kill). In the tutorial, the app consists of a Kubernetes service and deployment. The chaos experiment kills the app's pods intermittently. At the same time, the Iter8 experiment performs a load test of the app and validates its service-level objectives (SLOs) . Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Install Litmus in Kubernetes using these steps . Create the httpbin deployment file. cat <<EOF> deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: httpbin labels: app: httpbin spec: replicas: 1 selector: matchLabels: app: httpbin template: metadata: labels: app: httpbin spec: containers: - name: httpbin image: kennethreitz/httpbin ports: - containerPort: 80 initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', 'sleep 1'] EOF Create the httpbin deployment. kubectl apply -f deploy.yaml Create the httpbin service. kubectl expose deploy httpbin --port = 80 Launch experiments \u00b6 Launch the LitmusChaos and Iter8 experiments as described below. LitmusChaos Iter8 helm install httpbin litmuschaos \\ --repo https://iter8-tools.github.io/hub/ \\ --set applabel = 'app=httpbin' \\ --set totalChaosDuration = 3600 \\ --set chaosInterval = 5 About this LitmusChaos experiment This is a LitmusChaos pod-delete experiment packaged for reusability in the form of a Helm chart. This experiment causes (forced/graceful) pod failure of specific/random replicas of application resources, in this case, pods with a label called app with value httpbin . The deletion of pod(s) will be attempted by the chaos experiment once every chaosInterval (5) seconds, and this experiment will terminate after totalChaosDuration (3600) seconds. iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.chaosengine = litmuschaos-httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/get \\ --set http.duration = 30s \\ --set http.qps = 20 \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/latency-p99 = 100 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job About this Iter8 experiment This Iter8 experiment is similar to your first Iter8 experiment with some notable changes. The ready task in this experiment also checks if the chaosengine resource exists before it starts, and in addition to the mean latency and error count SLOs, it verifies that the 99 th percentile latency is under 100 msec. Observe experiments \u00b6 Observe the LitmusChaos and Iter8 experiments as follows. The chaos and Iter8 experiments LitmusChaos Iter8 Verify that the phase of the chaos experiment is Running . kubectl get chaosresults/litmuschaos-httpbin-pod-delete -n default \\ -o jsonpath = '{.status.experimentStatus.phase}' On completion of the LitmusChaos experiment After the LitmusChaos experiment completes (in ~3600 sec), the phase of the experiment will change to Completed . At that point, you can verify that the chaos experiment returns a Pass verdict. The Pass verdict states that the application is still running after chaos has ended. When the LitmusChaos experiment is still running, its verdict will be set to Awaited . kubectl get chaosresults/litmuschaos-httpbin-pod-delete -n default \\ -o jsonpath = '{.status.experimentStatus.verdict}' Due to chaos injection, and the fact that the number of replicas is set to 1, SLOs are not expected to be satisfied within the Iter8 experiment. Verify this is the case. # the SLOs assertion is expected to fail iter8 k assert -c completed -c nofailure -c slos --timeout 30s For a more detailed report of the Iter8 experiment, run the report command. iter8 k report Cleanup experiments \u00b6 Clean up the LitmusChaos and Iter8 experiments as described below. LitmusChaos Iter8 helm uninstall httpbin iter8 k delete Scale app and retry \u00b6 Scale up the app so that replica count is increased to 3. kubectl scale --replicas = 3 -n default deploy/httpbin The scaled app is now more resilient. Performing the same experiments as above will now result in SLOs being satisfied. Relaunch the experiments and observe the experiments . You should now find that the SLOs are satisfied. Cleanup \u00b6 Cleanup the Kubernetes cluster. App Experiments LitmusChaos Cleanup the app. kubectl delete svc/httpbin kubectl delete deploy/httpbin Cleanup the experiments as described here . Uninstall LitmusChaos from your cluster as described here . Some variations and extensions of this experiment Reuse the above experiment with your app by replacing the httpbin app with your app, and modifying the experiment values appropriately. Iter8 supports load testing and SLO validation for gRPC services . Try a joint chaos injection and SLO validation experiment for gRPC. Litmus makes it possible to inject over 51 types of chaos . Modify the LitmusChaos Helm chart with new templates in order to use any of these other types of chaos experiments.","title":"Chaos injection with SLOs"},{"location":"tutorials/chaos/slo-validation-chaos/#chaos-injection-with-slos","text":"Perform a joint Iter8 and LitmusChaos experiment. This joint experiment enables you to verify if an app continues to be resilient (satisfies SLOs) in the midst of chaos (pod kill). In the tutorial, the app consists of a Kubernetes service and deployment. The chaos experiment kills the app's pods intermittently. At the same time, the Iter8 experiment performs a load test of the app and validates its service-level objectives (SLOs) . Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Install Litmus in Kubernetes using these steps . Create the httpbin deployment file. cat <<EOF> deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: httpbin labels: app: httpbin spec: replicas: 1 selector: matchLabels: app: httpbin template: metadata: labels: app: httpbin spec: containers: - name: httpbin image: kennethreitz/httpbin ports: - containerPort: 80 initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', 'sleep 1'] EOF Create the httpbin deployment. kubectl apply -f deploy.yaml Create the httpbin service. kubectl expose deploy httpbin --port = 80","title":"Chaos Injection with SLOs"},{"location":"tutorials/chaos/slo-validation-chaos/#launch-experiments","text":"Launch the LitmusChaos and Iter8 experiments as described below. LitmusChaos Iter8 helm install httpbin litmuschaos \\ --repo https://iter8-tools.github.io/hub/ \\ --set applabel = 'app=httpbin' \\ --set totalChaosDuration = 3600 \\ --set chaosInterval = 5 About this LitmusChaos experiment This is a LitmusChaos pod-delete experiment packaged for reusability in the form of a Helm chart. This experiment causes (forced/graceful) pod failure of specific/random replicas of application resources, in this case, pods with a label called app with value httpbin . The deletion of pod(s) will be attempted by the chaos experiment once every chaosInterval (5) seconds, and this experiment will terminate after totalChaosDuration (3600) seconds. iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.chaosengine = litmuschaos-httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/get \\ --set http.duration = 30s \\ --set http.qps = 20 \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/latency-p99 = 100 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job About this Iter8 experiment This Iter8 experiment is similar to your first Iter8 experiment with some notable changes. The ready task in this experiment also checks if the chaosengine resource exists before it starts, and in addition to the mean latency and error count SLOs, it verifies that the 99 th percentile latency is under 100 msec.","title":"Launch experiments"},{"location":"tutorials/chaos/slo-validation-chaos/#observe-experiments","text":"Observe the LitmusChaos and Iter8 experiments as follows. The chaos and Iter8 experiments LitmusChaos Iter8 Verify that the phase of the chaos experiment is Running . kubectl get chaosresults/litmuschaos-httpbin-pod-delete -n default \\ -o jsonpath = '{.status.experimentStatus.phase}' On completion of the LitmusChaos experiment After the LitmusChaos experiment completes (in ~3600 sec), the phase of the experiment will change to Completed . At that point, you can verify that the chaos experiment returns a Pass verdict. The Pass verdict states that the application is still running after chaos has ended. When the LitmusChaos experiment is still running, its verdict will be set to Awaited . kubectl get chaosresults/litmuschaos-httpbin-pod-delete -n default \\ -o jsonpath = '{.status.experimentStatus.verdict}' Due to chaos injection, and the fact that the number of replicas is set to 1, SLOs are not expected to be satisfied within the Iter8 experiment. Verify this is the case. # the SLOs assertion is expected to fail iter8 k assert -c completed -c nofailure -c slos --timeout 30s For a more detailed report of the Iter8 experiment, run the report command. iter8 k report","title":"Observe experiments"},{"location":"tutorials/chaos/slo-validation-chaos/#cleanup-experiments","text":"Clean up the LitmusChaos and Iter8 experiments as described below. LitmusChaos Iter8 helm uninstall httpbin iter8 k delete","title":"Cleanup experiments"},{"location":"tutorials/chaos/slo-validation-chaos/#scale-app-and-retry","text":"Scale up the app so that replica count is increased to 3. kubectl scale --replicas = 3 -n default deploy/httpbin The scaled app is now more resilient. Performing the same experiments as above will now result in SLOs being satisfied. Relaunch the experiments and observe the experiments . You should now find that the SLOs are satisfied.","title":"Scale app and retry"},{"location":"tutorials/chaos/slo-validation-chaos/#cleanup","text":"Cleanup the Kubernetes cluster. App Experiments LitmusChaos Cleanup the app. kubectl delete svc/httpbin kubectl delete deploy/httpbin Cleanup the experiments as described here . Uninstall LitmusChaos from your cluster as described here . Some variations and extensions of this experiment Reuse the above experiment with your app by replacing the httpbin app with your app, and modifying the experiment values appropriately. Iter8 supports load testing and SLO validation for gRPC services . Try a joint chaos injection and SLO validation experiment for gRPC. Litmus makes it possible to inject over 51 types of chaos . Modify the LitmusChaos Helm chart with new templates in order to use any of these other types of chaos experiments.","title":"Cleanup"},{"location":"tutorials/custom-metrics/assert/","text":"Assert that the experiment encountered no failures, and all SLOs are satisfied. iter8 k assert -c nofailure -c slos Sample output from assert INFO [ 2021 -11-10 09 :33:12 ] experiment has no failure INFO [ 2021 -11-10 09 :33:12 ] SLOs are satisfied INFO [ 2021 -11-10 09 :33:12 ] all conditions were satisfied","title":"Assert"},{"location":"tutorials/custom-metrics/one-version/","text":"SLO validation using custom metrics (single version) \u00b6 Validate SLOs for an app by fetching the app's metrics from a database (like Prometheus). This is a multi-loop Kubernetes experiment . Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Install Istio . Install Istio's Prometheus add-on . Enable automatic Istio sidecar injection for the default namespace. This ensures that the pods created in steps 5 and 6 will have the Istio sidecar. kubectl label namespace default istio-injection = enabled --overwrite Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Generate load. kubectl run fortio --image = fortio/fortio --command -- fortio load -t 6000s http://httpbin.default/get Launch experiment \u00b6 iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.namespace = default \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment consists of two tasks , namely, custommetrics , and assess . The custommetrics task in this experiment works by downloading a provider template named istio-prom from a URL, substituting the template variables with values , using the resulting provider spec to query Prometheus for metrics, and processing the response from Prometheus to extract the metric values. Metrics defined by this template include error-rate and latency-mean ; the Prometheus labels used by this template are stored in labels ; all the metrics and variables associated with this template are documented as part of the template . The assess task verifies if the app satisfies the specified SLOs: i) there are no errors, and ii) the mean latency of the app does not exceed 100 msec. This is a multi-loop Kubernetes experiment . Hence, its runner value is set to cronjob . The cronjobSchedule expression specifies that each experiment loop (i.e., the sequence of tasks in the experiment) is scheduled for execution periodically once every minute. This enables Iter8 to refresh the metric values and perform SLO validation using the latest metric values during each loop. Assert experiment outcomes \u00b6 Assert that the experiment encountered no failures, and all SLOs are satisfied. iter8 k assert -c nofailure -c slos Sample output from assert INFO [ 2021 -11-10 09 :33:12 ] experiment has no failure INFO [ 2021 -11-10 09 :33:12 ] SLOs are satisfied INFO [ 2021 -11-10 09 :33:12 ] all conditions were satisfied View experiment report and logs, and cleanup as described in your first experiment . Some variations and extensions of this experiment Perform SLO validation for multiple versions of an app using custom metrics . Define and use your own provider templates. This enables you to use any app-specific metrics from any database as part of Iter8 experiments. Read the documentation for the custommetrics task to learn more. Alter the cronjobSchedule expression so that experiment loops are repeated at a frequency of your choice. Use use https://crontab.guru to learn more about cronjobSchedule expressions.","title":"One version"},{"location":"tutorials/custom-metrics/one-version/#slo-validation-using-custom-metrics-single-version","text":"Validate SLOs for an app by fetching the app's metrics from a database (like Prometheus). This is a multi-loop Kubernetes experiment . Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Install Istio . Install Istio's Prometheus add-on . Enable automatic Istio sidecar injection for the default namespace. This ensures that the pods created in steps 5 and 6 will have the Istio sidecar. kubectl label namespace default istio-injection = enabled --overwrite Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Generate load. kubectl run fortio --image = fortio/fortio --command -- fortio load -t 6000s http://httpbin.default/get","title":"SLO validation using custom metrics (single version)"},{"location":"tutorials/custom-metrics/one-version/#launch-experiment","text":"iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.namespace = default \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment consists of two tasks , namely, custommetrics , and assess . The custommetrics task in this experiment works by downloading a provider template named istio-prom from a URL, substituting the template variables with values , using the resulting provider spec to query Prometheus for metrics, and processing the response from Prometheus to extract the metric values. Metrics defined by this template include error-rate and latency-mean ; the Prometheus labels used by this template are stored in labels ; all the metrics and variables associated with this template are documented as part of the template . The assess task verifies if the app satisfies the specified SLOs: i) there are no errors, and ii) the mean latency of the app does not exceed 100 msec. This is a multi-loop Kubernetes experiment . Hence, its runner value is set to cronjob . The cronjobSchedule expression specifies that each experiment loop (i.e., the sequence of tasks in the experiment) is scheduled for execution periodically once every minute. This enables Iter8 to refresh the metric values and perform SLO validation using the latest metric values during each loop.","title":"Launch experiment"},{"location":"tutorials/custom-metrics/one-version/#assert-experiment-outcomes","text":"Assert that the experiment encountered no failures, and all SLOs are satisfied. iter8 k assert -c nofailure -c slos Sample output from assert INFO [ 2021 -11-10 09 :33:12 ] experiment has no failure INFO [ 2021 -11-10 09 :33:12 ] SLOs are satisfied INFO [ 2021 -11-10 09 :33:12 ] all conditions were satisfied View experiment report and logs, and cleanup as described in your first experiment . Some variations and extensions of this experiment Perform SLO validation for multiple versions of an app using custom metrics . Define and use your own provider templates. This enables you to use any app-specific metrics from any database as part of Iter8 experiments. Read the documentation for the custommetrics task to learn more. Alter the cronjobSchedule expression so that experiment loops are repeated at a frequency of your choice. Use use https://crontab.guru to learn more about cronjobSchedule expressions.","title":"Assert experiment outcomes"},{"location":"tutorials/custom-metrics/two-or-more-versions/","text":"SLO validation using custom metrics (multiple versions) \u00b6 Validate SLOs for multiple versions of an app by fetching metrics for each app version from a database (like Prometheus). This is a multi-loop Kubernetes experiment . Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Try an SLO validation experiment using custom metrics for a single version of an app . Complete the Istio traffic mirroring tutorial , specifically, the Before you begin section, the Creating a default routing policy section, and the Mirroring traffic to v2 section. Omit the the Cleaning up step (you can clean up once you are done with this tutorial). Install Istio's Prometheus add-on . Generate load. kubectl run fortio --image = fortio/fortio --command -- fortio load -t 6000s http://httpbin.default:8000/get Launch experiment \u00b6 iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.namespace = default \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set custommetrics.values.labels.reporter = destination \\ --set 'custommetrics.versionValues[0].labels.destination_version=v1' \\ --set 'custommetrics.versionValues[1].labels.destination_version=v2' \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment extends the SLO validation experiment using custom metrics for a single app version . There are two versions of the app in this experiment. Variable values that are specific to the first version are specified under custommetrics.versionValues[0] , while those that are specific to the second version are specified under custommetrics.versionValues[1] . For the first version, Iter8 merges custommetrics.values with custommetrics.versionValues[0] (the latter takes precedence), and uses the result for template variable substitution. Similarly, for the second version, Iter8 merges custommetrics.values with custommetrics.versionValues[1] (the latter takes precedence), and uses the result for template variable substitution. Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in this experiment . Some variations and extensions of this experiment Define and use your own provider templates. This enables you to use any app-specific metrics from any database as part of Iter8 experiments. Read the documentation for the custommetrics task to learn more. Alter the cronjobSchedule expression so that experiment loops are repeated at a frequency of your choice. Use use https://crontab.guru to learn more about cronjobSchedule expressions.","title":"Two or more versions"},{"location":"tutorials/custom-metrics/two-or-more-versions/#slo-validation-using-custom-metrics-multiple-versions","text":"Validate SLOs for multiple versions of an app by fetching metrics for each app version from a database (like Prometheus). This is a multi-loop Kubernetes experiment . Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Try an SLO validation experiment using custom metrics for a single version of an app . Complete the Istio traffic mirroring tutorial , specifically, the Before you begin section, the Creating a default routing policy section, and the Mirroring traffic to v2 section. Omit the the Cleaning up step (you can clean up once you are done with this tutorial). Install Istio's Prometheus add-on . Generate load. kubectl run fortio --image = fortio/fortio --command -- fortio load -t 6000s http://httpbin.default:8000/get","title":"SLO validation using custom metrics (multiple versions)"},{"location":"tutorials/custom-metrics/two-or-more-versions/#launch-experiment","text":"iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.namespace = default \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set custommetrics.values.labels.reporter = destination \\ --set 'custommetrics.versionValues[0].labels.destination_version=v1' \\ --set 'custommetrics.versionValues[1].labels.destination_version=v2' \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment extends the SLO validation experiment using custom metrics for a single app version . There are two versions of the app in this experiment. Variable values that are specific to the first version are specified under custommetrics.versionValues[0] , while those that are specific to the second version are specified under custommetrics.versionValues[1] . For the first version, Iter8 merges custommetrics.values with custommetrics.versionValues[0] (the latter takes precedence), and uses the result for template variable substitution. Similarly, for the second version, Iter8 merges custommetrics.values with custommetrics.versionValues[1] (the latter takes precedence), and uses the result for template variable substitution. Assert experiment outcomes, view experiment report, view experiment logs, and cleanup as described in this experiment . Some variations and extensions of this experiment Define and use your own provider templates. This enables you to use any app-specific metrics from any database as part of Iter8 experiments. Read the documentation for the custommetrics task to learn more. Alter the cronjobSchedule expression so that experiment loops are repeated at a frequency of your choice. Use use https://crontab.guru to learn more about cronjobSchedule expressions.","title":"Launch experiment"},{"location":"tutorials/integrations/ghactions/","text":"GitHub Actions \u00b6 There are two ways that you can use Iter8 with GitHub Actions. You can run Iter8 CLI within a GitHub Actions workflow and you can also use Iter8 to trigger a GitHub Actions workflow from an experiment. Use Iter8 in a GitHub Actions workflow \u00b6 Install the latest version of the Iter8 CLI using iter8-tools/iter8@v0.13 . Once installed, the Iter8 CLI can be used as documented in various tutorials. For example: 1 2 3 4 5 6 7 8 # Install Iter8 CLI - uses : iter8-tools/iter8@v0.13 # Launch an experiment inside Kubernetes # This assumes that your Kubernetes cluster is accessible from the GitHub Actions pipeline - run : | iter8 k launch --set \"tasks={http}\" \\ --set http.url=http://httpbin.org/get \\ --set runner=job Trigger a GitHub Actions workflow from an Iter8 experiment \u00b6 Iter8 provides a github task that sends a repository_dispatch which can trigger the workflows in the default branch of a GitHub repository. Example \u00b6 In this example, you will run the Your First Experiment but at the end of the experiment, Iter8 will trigger a workflow on GitHub. In this simple example, the workflow will simply print out the experiment report that it will receive with the repository_dispatch . In a more sophisticated scenario, the workflow could, for example, read from the experiment report and based on whether or not task failured or metrics did not meet SLOs, determine what to do next. In a GitOps scenario, it could make changes to the Git repository, promote winners, or restart pipelines among other things. To summarize what will happen, you will create a new GitHub repository, add a workflow that will respond to the github task, set up and run an experiment, and check if the workflow was triggered. The github task requires the name of a repository, the name of the owner, as well as an authentication token in order to send the repository_dispatch . To see a full list of the github task parameters, see here . Create a new repository on GitHub. Add the following workflow. name : iter8 `github` task test on : repository_dispatch : types : iter8 jobs : my-job : runs-on : ubuntu-latest steps : - run : 'echo \"payload: ${{ toJson(github.event.client_payload) }}\"' Note that this workflow has one job that will print out the client_payload . The default github task payload is configured with client_payload set to experiment report. This means that this job will simply print out the entire experiment report. Also note that the on.repository_dispatch.types is set to iter8 . The default github task payload is configured with event_type set to iter8 . This indicates that once the repository_dispatch has been sent, only workflows on the default branch with on.repository_dispatch.types set to iter8 will be triggered. Create a GitHub personal access token for the token parameter. Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the experiment with the github task with the appropriate values. iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> \\ --set runner = job Verify that the workflow has been triggered after the experiment has completed. Some variations and extensions of the github task The default github task payload sends the entirety of the experiment report. In your workflow, you can read from the report and use that data for control flow or use snippets of that data in different actions. For example, you can check to see if there have been any task failures during the experiment and perform different actions. You do not need to use the default github task payload . You can provide your own payload by overriding the default of the payloadTemplateURL . For example, instead of sending the entirety of the experiment report, you can create a payload template that only sends a subset. Try a multi-loop experiment with an if parameter to control when the github task is run. A multi-loop experiment will allow you to run the tasks on a recurring basis, allowing you to monitor your app over a course of time. For example: iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" This will run http , assess , and github tasks every minute. If you would like to run the github task only during the 10 th loop, use the if parameter. iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set github.owner=<GitHub owner> \\ --set github.repo=<GitHub repository> \\ --set github.token=<GitHub token> \\ --set github.if=\"Result.NumLoops == 10\" --set runner=cronjob \\ --set cronjobSchedule=\"*/1 * * * *\"","title":"GitHub Actions"},{"location":"tutorials/integrations/ghactions/#github-actions","text":"There are two ways that you can use Iter8 with GitHub Actions. You can run Iter8 CLI within a GitHub Actions workflow and you can also use Iter8 to trigger a GitHub Actions workflow from an experiment.","title":"GitHub Actions"},{"location":"tutorials/integrations/ghactions/#use-iter8-in-a-github-actions-workflow","text":"Install the latest version of the Iter8 CLI using iter8-tools/iter8@v0.13 . Once installed, the Iter8 CLI can be used as documented in various tutorials. For example: 1 2 3 4 5 6 7 8 # Install Iter8 CLI - uses : iter8-tools/iter8@v0.13 # Launch an experiment inside Kubernetes # This assumes that your Kubernetes cluster is accessible from the GitHub Actions pipeline - run : | iter8 k launch --set \"tasks={http}\" \\ --set http.url=http://httpbin.org/get \\ --set runner=job","title":"Use Iter8 in a GitHub Actions workflow"},{"location":"tutorials/integrations/ghactions/#trigger-a-github-actions-workflow-from-an-iter8-experiment","text":"Iter8 provides a github task that sends a repository_dispatch which can trigger the workflows in the default branch of a GitHub repository.","title":"Trigger a GitHub Actions workflow from an Iter8 experiment"},{"location":"tutorials/integrations/ghactions/#example","text":"In this example, you will run the Your First Experiment but at the end of the experiment, Iter8 will trigger a workflow on GitHub. In this simple example, the workflow will simply print out the experiment report that it will receive with the repository_dispatch . In a more sophisticated scenario, the workflow could, for example, read from the experiment report and based on whether or not task failured or metrics did not meet SLOs, determine what to do next. In a GitOps scenario, it could make changes to the Git repository, promote winners, or restart pipelines among other things. To summarize what will happen, you will create a new GitHub repository, add a workflow that will respond to the github task, set up and run an experiment, and check if the workflow was triggered. The github task requires the name of a repository, the name of the owner, as well as an authentication token in order to send the repository_dispatch . To see a full list of the github task parameters, see here . Create a new repository on GitHub. Add the following workflow. name : iter8 `github` task test on : repository_dispatch : types : iter8 jobs : my-job : runs-on : ubuntu-latest steps : - run : 'echo \"payload: ${{ toJson(github.event.client_payload) }}\"' Note that this workflow has one job that will print out the client_payload . The default github task payload is configured with client_payload set to experiment report. This means that this job will simply print out the entire experiment report. Also note that the on.repository_dispatch.types is set to iter8 . The default github task payload is configured with event_type set to iter8 . This indicates that once the repository_dispatch has been sent, only workflows on the default branch with on.repository_dispatch.types set to iter8 will be triggered. Create a GitHub personal access token for the token parameter. Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the experiment with the github task with the appropriate values. iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> \\ --set runner = job Verify that the workflow has been triggered after the experiment has completed. Some variations and extensions of the github task The default github task payload sends the entirety of the experiment report. In your workflow, you can read from the report and use that data for control flow or use snippets of that data in different actions. For example, you can check to see if there have been any task failures during the experiment and perform different actions. You do not need to use the default github task payload . You can provide your own payload by overriding the default of the payloadTemplateURL . For example, instead of sending the entirety of the experiment report, you can create a payload template that only sends a subset. Try a multi-loop experiment with an if parameter to control when the github task is run. A multi-loop experiment will allow you to run the tasks on a recurring basis, allowing you to monitor your app over a course of time. For example: iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" This will run http , assess , and github tasks every minute. If you would like to run the github task only during the 10 th loop, use the if parameter. iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set github.owner=<GitHub owner> \\ --set github.repo=<GitHub repository> \\ --set github.token=<GitHub token> \\ --set github.if=\"Result.NumLoops == 10\" --set runner=cronjob \\ --set cronjobSchedule=\"*/1 * * * *\"","title":"Example"},{"location":"tutorials/integrations/istio/","text":"SLO Validation using Istio's Prometheus Add-on \u00b6 SLO validation using Istio's Prometheus add-on SLO validation using Istio's Prometheus add-on with traffic mirroring Istio Examples based on Iter8 v0.7 \u00b6 Hybrid (A/B + SLOs) testing SLO validation SLO validation (single version) Progressive traffic shifting Fixed % split","title":"Istio"},{"location":"tutorials/integrations/istio/#slo-validation-using-istios-prometheus-add-on","text":"SLO validation using Istio's Prometheus add-on SLO validation using Istio's Prometheus add-on with traffic mirroring","title":"SLO Validation using Istio's Prometheus Add-on"},{"location":"tutorials/integrations/istio/#istio-examples-based-on-iter8-v07","text":"Hybrid (A/B + SLOs) testing SLO validation SLO validation (single version) Progressive traffic shifting Fixed % split","title":"Istio Examples based on Iter8 v0.7"},{"location":"tutorials/integrations/knative/","text":"Knative blog article on performance testing of HTTP and gRPC services with SLO validation using Iter8.","title":"Knative"},{"location":"tutorials/integrations/linkerd/","text":"Linkerd Examples based on Iter8 v0.7 \u00b6 A/B testing","title":"Linkerd"},{"location":"tutorials/integrations/linkerd/#linkerd-examples-based-on-iter8-v07","text":"A/B testing","title":"Linkerd Examples based on Iter8 v0.7"},{"location":"tutorials/integrations/litmus/","text":"LitmusChaos \u00b6 Chaos injection with SLOs","title":"Litmus"},{"location":"tutorials/integrations/litmus/#litmuschaos","text":"Chaos injection with SLOs","title":"LitmusChaos"},{"location":"tutorials/integrations/overview/","text":"Integrations \u00b6 The tutorials under the integrations section are maintained by members of the Iter8 community. They may become outdated. If you find that something is not working, please lend a helping hand and fix it in a PR. More integrations and examples are always welcome.","title":"Integrations"},{"location":"tutorials/integrations/overview/#integrations","text":"The tutorials under the integrations section are maintained by members of the Iter8 community. They may become outdated. If you find that something is not working, please lend a helping hand and fix it in a PR. More integrations and examples are always welcome.","title":"Integrations"},{"location":"tutorials/integrations/seldon/","text":"Seldon Examples based on Iter8 v0.7 \u00b6 Hybrid (A/B + SLOs) testing Progressive traffic shifting","title":"Seldon"},{"location":"tutorials/integrations/seldon/#seldon-examples-based-on-iter8-v07","text":"Hybrid (A/B + SLOs) testing Progressive traffic shifting","title":"Seldon Examples based on Iter8 v0.7"},{"location":"tutorials/integrations/slack/","text":"Use Iter8 to send a message to a Slack channel \u00b6 Iter8 provides a slack task that sends a message to a Slack channel using a webhook . Example \u00b6 In this example, you will run the Your First Experiment but at the end of the experiment, Iter8 will send a message on Slack. The message will simply contain the experiment report in text form. However, you can easily construct a more sophisticated message by providing your own payload template. This task could provide important updates on an experiment over Slack, for example a summary at the end of an experiment. To summarize what will happen, you will create a new channel on Slack and configure a webhook, set up and run an experiment, and check if a message was sent to the channel. The slack task requires the URL of the Slack webhook. To see a full list of the github task parameters, see here . Create a new channel in your Slack organization. Create a Slack app, enable incoming webhooks, and create a new incoming webhook. See here . Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the experiment with the slack task with the appropriate values. iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST \\ --set runner = job Verify that the message has been sent after the experiment has completed. Some variations and extensions of the slack task The default slack task payload sends the entirety of the experiment report. However, you do not need to use the default payload. You can provide your own payload by overriding the default of the `payloadTemplateURL`. For example, you can create a payload that selectively print out parts of the experiment report instead of the whole thing. You can also use Slack's [Block Kit](https://api.slack.com/block-kit/building) in order to make your message more sophisticated. You can use markdown, create different sections, or add interactivity, such as buttons. Try a multi-loop experiment with an if parameter to control when the slack task is run. A multi-loop experiment will allow you to run the tasks on a recurring basis, allowing you to monitor your app over a course of time. For example: iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" This will run http , assess , and slack tasks every minute. If you would like to run the slack task only during the 10 th loop, use the if parameter. iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set slack.url=<Slack webhook> \\ --set slack.method=POST \\ --set slack.if=\"Result.NumLoops == 10\" --set runner=cronjob \\ --set cronjobSchedule=\"*/1 * * * *\"","title":"Slack"},{"location":"tutorials/integrations/slack/#use-iter8-to-send-a-message-to-a-slack-channel","text":"Iter8 provides a slack task that sends a message to a Slack channel using a webhook .","title":"Use Iter8 to send a message to a Slack channel"},{"location":"tutorials/integrations/slack/#example","text":"In this example, you will run the Your First Experiment but at the end of the experiment, Iter8 will send a message on Slack. The message will simply contain the experiment report in text form. However, you can easily construct a more sophisticated message by providing your own payload template. This task could provide important updates on an experiment over Slack, for example a summary at the end of an experiment. To summarize what will happen, you will create a new channel on Slack and configure a webhook, set up and run an experiment, and check if a message was sent to the channel. The slack task requires the URL of the Slack webhook. To see a full list of the github task parameters, see here . Create a new channel in your Slack organization. Create a Slack app, enable incoming webhooks, and create a new incoming webhook. See here . Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the experiment with the slack task with the appropriate values. iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST \\ --set runner = job Verify that the message has been sent after the experiment has completed. Some variations and extensions of the slack task The default slack task payload sends the entirety of the experiment report. However, you do not need to use the default payload. You can provide your own payload by overriding the default of the `payloadTemplateURL`. For example, you can create a payload that selectively print out parts of the experiment report instead of the whole thing. You can also use Slack's [Block Kit](https://api.slack.com/block-kit/building) in order to make your message more sophisticated. You can use markdown, create different sections, or add interactivity, such as buttons. Try a multi-loop experiment with an if parameter to control when the slack task is run. A multi-loop experiment will allow you to run the tasks on a recurring basis, allowing you to monitor your app over a course of time. For example: iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" This will run http , assess , and slack tasks every minute. If you would like to run the slack task only during the 10 th loop, use the if parameter. iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set slack.url=<Slack webhook> \\ --set slack.method=POST \\ --set slack.if=\"Result.NumLoops == 10\" --set runner=cronjob \\ --set cronjobSchedule=\"*/1 * * * *\"","title":"Example"},{"location":"tutorials/integrations/kserve/canary-testing/","text":"Canary Testing \u00b6 This tutorial shows how easy it is validate SLOs for multiple versions of a model in KServe when fetching metrics from a metrics database like Prometheus. We show this using the sklearn-iris model used to describe canary rollouts in KServe. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash Install Prometheus monitoring for KServe using these instructions . Experiment Setup \u00b6 Deploy two models to compare and generate load against them. We follow the instructions for the KServe canary rollout example to deploy the models. Create InferenceService for Initial Model \u00b6 kubectl apply -f - <<EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-iris\" spec: predictor: model: modelFormat: name: sklearn storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\" EOF Update InferenceService with a Canary Model \u00b6 kubectl apply -f - <<EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-iris\" spec: predictor: canaryTrafficPercent: 10 model: modelFormat: name: sklearn storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model-2\" EOF Generate Load \u00b6 Port forward requests to the cluster: INGRESS_GATEWAY = $( kubectl get svc --namespace istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward --namespace istio-system svc/ $INGRESS_GATEWAY 8080 :80 Send prediction requests to the inference service. The following script generates about one request a second. In a production cluster, this step is not required since your inference service will receive requests from real users. SERVICE_HOSTNAME = \"sklearn-iris.default.example.com\" # kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3 cat <<EOF > \"./iris-input.json\" { \"instances\": [ [6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6] ] } EOF while true ; do curl -H \"Host: ${ SERVICE_HOSTNAME } \" \\ http://localhost:8080/v1/models/sklearn-iris:predict \\ -d @./iris-input.json sleep 1 done Launch Iter8 Experiment \u00b6 Launch an Iter8 experiment inside the Kubernetes cluster: iter8 k launch \\ --set \"tasks={ready,custommetrics,assess}\" \\ --set ready.isvc = sklearn-iris \\ --set ready.timeout = 180s \\ --set custommetrics.templates.kserve-prometheus = \"https://gist.githubusercontent.com/kalantar/adc6c9b0efe483c00b8f0c20605ac36c/raw/c4562e87b7ac0652b0e46f8f494d024307bff7a1/kserve-prometheus.tpl\" \\ --set custommetrics.values.labels.service_name = sklearn-iris-predictor-default \\ --set 'custommetrics.versionValues[0].labels.revision_name=sklearn-iris-predictor-default-00002' \\ --set 'custommetrics.versionValues[1].labels.revision_name=sklearn-iris-predictor-default-00001' \\ --set \"custommetrics.values.latencyPercentiles={50,75,90,95}\" \\ --set assess.SLOs.upper.kserve-prometheus/error-count = 0 \\ --set assess.SLOs.upper.kserve-prometheus/latency-mean = 25 \\ --set assess.SLOs.upper.kserve-prometheus/latency-p '90' = 40 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment consists of three tasks , namely, ready , custommetrics and assess . The ready task checks if the sklearn-iris InferenceService exists and is Ready . The custommetrics task reads metrics from a Prometheus service as defined by the template . The template is parameterised using labels for service and revision name. You can identify the revision names from the InferenceService : kubectl get isvc sklearn-iris -o json \\ | jq -r '.status.components.predictor.traffic | .[] | .revisionName' The service name is the prefix (remove the trailing -ddddd ). The assess task verifies if the model satisfies the specified SLOs: there are no errors the mean latency of the prediction does not exceed 25 msec, and the 90 th percentile latency for prediction does not exceed 40 msec. This is a multi-loop Kubernetes experiment ; its runner is cronjob . The cronjobSchedule expression specifies the frequency of the experiment execution -- periodically refreshing the metric values and performing SLO validation using the updated values. You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment . Clean up \u00b6 To clean up, delete the Iter8 experiment: iter8 k delete Remove the InferenceService and the request data: kubectl delete inferenceservice sklearn-iris rm ./iris-input.json You can remove Prometheus using these instructions .","title":"Canary testing"},{"location":"tutorials/integrations/kserve/canary-testing/#canary-testing","text":"This tutorial shows how easy it is validate SLOs for multiple versions of a model in KServe when fetching metrics from a metrics database like Prometheus. We show this using the sklearn-iris model used to describe canary rollouts in KServe. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash Install Prometheus monitoring for KServe using these instructions .","title":"Canary Testing"},{"location":"tutorials/integrations/kserve/canary-testing/#experiment-setup","text":"Deploy two models to compare and generate load against them. We follow the instructions for the KServe canary rollout example to deploy the models.","title":"Experiment Setup"},{"location":"tutorials/integrations/kserve/canary-testing/#create-inferenceservice-for-initial-model","text":"kubectl apply -f - <<EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-iris\" spec: predictor: model: modelFormat: name: sklearn storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\" EOF","title":"Create InferenceService for Initial Model"},{"location":"tutorials/integrations/kserve/canary-testing/#update-inferenceservice-with-a-canary-model","text":"kubectl apply -f - <<EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-iris\" spec: predictor: canaryTrafficPercent: 10 model: modelFormat: name: sklearn storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model-2\" EOF","title":"Update InferenceService with a Canary Model"},{"location":"tutorials/integrations/kserve/canary-testing/#generate-load","text":"Port forward requests to the cluster: INGRESS_GATEWAY = $( kubectl get svc --namespace istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward --namespace istio-system svc/ $INGRESS_GATEWAY 8080 :80 Send prediction requests to the inference service. The following script generates about one request a second. In a production cluster, this step is not required since your inference service will receive requests from real users. SERVICE_HOSTNAME = \"sklearn-iris.default.example.com\" # kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3 cat <<EOF > \"./iris-input.json\" { \"instances\": [ [6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6] ] } EOF while true ; do curl -H \"Host: ${ SERVICE_HOSTNAME } \" \\ http://localhost:8080/v1/models/sklearn-iris:predict \\ -d @./iris-input.json sleep 1 done","title":"Generate Load"},{"location":"tutorials/integrations/kserve/canary-testing/#launch-iter8-experiment","text":"Launch an Iter8 experiment inside the Kubernetes cluster: iter8 k launch \\ --set \"tasks={ready,custommetrics,assess}\" \\ --set ready.isvc = sklearn-iris \\ --set ready.timeout = 180s \\ --set custommetrics.templates.kserve-prometheus = \"https://gist.githubusercontent.com/kalantar/adc6c9b0efe483c00b8f0c20605ac36c/raw/c4562e87b7ac0652b0e46f8f494d024307bff7a1/kserve-prometheus.tpl\" \\ --set custommetrics.values.labels.service_name = sklearn-iris-predictor-default \\ --set 'custommetrics.versionValues[0].labels.revision_name=sklearn-iris-predictor-default-00002' \\ --set 'custommetrics.versionValues[1].labels.revision_name=sklearn-iris-predictor-default-00001' \\ --set \"custommetrics.values.latencyPercentiles={50,75,90,95}\" \\ --set assess.SLOs.upper.kserve-prometheus/error-count = 0 \\ --set assess.SLOs.upper.kserve-prometheus/latency-mean = 25 \\ --set assess.SLOs.upper.kserve-prometheus/latency-p '90' = 40 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment consists of three tasks , namely, ready , custommetrics and assess . The ready task checks if the sklearn-iris InferenceService exists and is Ready . The custommetrics task reads metrics from a Prometheus service as defined by the template . The template is parameterised using labels for service and revision name. You can identify the revision names from the InferenceService : kubectl get isvc sklearn-iris -o json \\ | jq -r '.status.components.predictor.traffic | .[] | .revisionName' The service name is the prefix (remove the trailing -ddddd ). The assess task verifies if the model satisfies the specified SLOs: there are no errors the mean latency of the prediction does not exceed 25 msec, and the 90 th percentile latency for prediction does not exceed 40 msec. This is a multi-loop Kubernetes experiment ; its runner is cronjob . The cronjobSchedule expression specifies the frequency of the experiment execution -- periodically refreshing the metric values and performing SLO validation using the updated values. You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment .","title":"Launch Iter8 Experiment"},{"location":"tutorials/integrations/kserve/canary-testing/#clean-up","text":"To clean up, delete the Iter8 experiment: iter8 k delete Remove the InferenceService and the request data: kubectl delete inferenceservice sklearn-iris rm ./iris-input.json You can remove Prometheus using these instructions .","title":"Clean up"},{"location":"tutorials/integrations/kserve/continuous-validation/","text":"Continuous Validation \u00b6 This tutorial shows how easy it is validate SLOs for a single model in KServe when fetching metrics from a metrics database like Prometheus. We show this using the sklearn-iris model used for a first InferenceService in the KServe documentation. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash Install Prometheus monitoring for KServe using these instructions . Experiment Setup \u00b6 Deploy the model and generate load against it. We follow the instructions for the KServe first InferenceService to deploy the model. Create InferenceService for Initial Model \u00b6 kubectl apply -f - <<EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-iris\" spec: predictor: model: modelFormat: name: sklearn storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\" EOF Generate Load \u00b6 Port forward requests to the cluster: INGRESS_GATEWAY = $( kubectl get svc --namespace istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward --namespace istio-system svc/ $INGRESS_GATEWAY 8080 :80 Send prediction requests to the inference service. The following script generates about one request a second. In a production cluster, this step is not required since your inference service will receive requests from real users. SERVICE_HOSTNAME = \"sklearn-iris.default.example.com\" # kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3 cat <<EOF > \"./iris-input.json\" { \"instances\": [ [6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6] ] } EOF while true ; do curl -H \"Host: ${ SERVICE_HOSTNAME } \" \\ http://localhost:8080/v1/models/sklearn-iris:predict \\ -d @./iris-input.json sleep 1 done Launch Iter8 Experiment \u00b6 Launch the Iter8 experiment inside the Kubernetes cluster: iter8 k launch \\ --set \"tasks={ready,custommetrics,assess}\" \\ --set ready.isvc = sklearn-iris \\ --set ready.timeout = 180s \\ --set custommetrics.templates.kserve-prometheus = \"https://gist.githubusercontent.com/kalantar/adc6c9b0efe483c00b8f0c20605ac36c/raw/c4562e87b7ac0652b0e46f8f494d024307bff7a1/kserve-prometheus.tpl\" \\ --set custommetrics.values.labels.service_name = sklearn-iris-predictor-default \\ --set \"custommetrics.values.latencyPercentiles={50,75,90,95}\" \\ --set assess.SLOs.upper.kserve-prometheus/error-count = 0 \\ --set assess.SLOs.upper.kserve-prometheus/latency-mean = 25 \\ --set assess.SLOs.upper.kserve-prometheus/latency-p '90' = 40 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment consists of three tasks , namely, ready , custommetrics and assess . The ready task checks if the sklearn-iris InferenceService exists and is Ready . The custommetrics task checks reads metrics from a Prometheus service as defined by the template. The assess task verifies if the model satisfies the specified SLOs: there are no errors the mean latency of the prediction does not exceed 25 msec, and the 90 th percentile latency for prediction does not exceed 40 msec. This is a multi-loop Kubernetes experiment ; its runner is cronjob . The cronjobSchedule expression specifies the frequency of the experiment execution -- periodically refreshing the metric values and performing SLO validation using the updated values. You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment . Clean up \u00b6 To clean up, delete the Iter8 experiment: iter8 k delete Remove the InferenceService and the request data: kubectl delete inferenceservice sklearn-iris rm ./iris-input.json You can remove Prometheus using these instructions .","title":"Continuous validation"},{"location":"tutorials/integrations/kserve/continuous-validation/#continuous-validation","text":"This tutorial shows how easy it is validate SLOs for a single model in KServe when fetching metrics from a metrics database like Prometheus. We show this using the sklearn-iris model used for a first InferenceService in the KServe documentation. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash Install Prometheus monitoring for KServe using these instructions .","title":"Continuous Validation"},{"location":"tutorials/integrations/kserve/continuous-validation/#experiment-setup","text":"Deploy the model and generate load against it. We follow the instructions for the KServe first InferenceService to deploy the model.","title":"Experiment Setup"},{"location":"tutorials/integrations/kserve/continuous-validation/#create-inferenceservice-for-initial-model","text":"kubectl apply -f - <<EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-iris\" spec: predictor: model: modelFormat: name: sklearn storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\" EOF","title":"Create InferenceService for Initial Model"},{"location":"tutorials/integrations/kserve/continuous-validation/#generate-load","text":"Port forward requests to the cluster: INGRESS_GATEWAY = $( kubectl get svc --namespace istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward --namespace istio-system svc/ $INGRESS_GATEWAY 8080 :80 Send prediction requests to the inference service. The following script generates about one request a second. In a production cluster, this step is not required since your inference service will receive requests from real users. SERVICE_HOSTNAME = \"sklearn-iris.default.example.com\" # kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3 cat <<EOF > \"./iris-input.json\" { \"instances\": [ [6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6] ] } EOF while true ; do curl -H \"Host: ${ SERVICE_HOSTNAME } \" \\ http://localhost:8080/v1/models/sklearn-iris:predict \\ -d @./iris-input.json sleep 1 done","title":"Generate Load"},{"location":"tutorials/integrations/kserve/continuous-validation/#launch-iter8-experiment","text":"Launch the Iter8 experiment inside the Kubernetes cluster: iter8 k launch \\ --set \"tasks={ready,custommetrics,assess}\" \\ --set ready.isvc = sklearn-iris \\ --set ready.timeout = 180s \\ --set custommetrics.templates.kserve-prometheus = \"https://gist.githubusercontent.com/kalantar/adc6c9b0efe483c00b8f0c20605ac36c/raw/c4562e87b7ac0652b0e46f8f494d024307bff7a1/kserve-prometheus.tpl\" \\ --set custommetrics.values.labels.service_name = sklearn-iris-predictor-default \\ --set \"custommetrics.values.latencyPercentiles={50,75,90,95}\" \\ --set assess.SLOs.upper.kserve-prometheus/error-count = 0 \\ --set assess.SLOs.upper.kserve-prometheus/latency-mean = 25 \\ --set assess.SLOs.upper.kserve-prometheus/latency-p '90' = 40 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" About this experiment This experiment consists of three tasks , namely, ready , custommetrics and assess . The ready task checks if the sklearn-iris InferenceService exists and is Ready . The custommetrics task checks reads metrics from a Prometheus service as defined by the template. The assess task verifies if the model satisfies the specified SLOs: there are no errors the mean latency of the prediction does not exceed 25 msec, and the 90 th percentile latency for prediction does not exceed 40 msec. This is a multi-loop Kubernetes experiment ; its runner is cronjob . The cronjobSchedule expression specifies the frequency of the experiment execution -- periodically refreshing the metric values and performing SLO validation using the updated values. You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment .","title":"Launch Iter8 Experiment"},{"location":"tutorials/integrations/kserve/continuous-validation/#clean-up","text":"To clean up, delete the Iter8 experiment: iter8 k delete Remove the InferenceService and the request data: kubectl delete inferenceservice sklearn-iris rm ./iris-input.json You can remove Prometheus using these instructions .","title":"Clean up"},{"location":"tutorials/integrations/kserve/grpc/","text":"Load Test a KServe Model (via gRPC) \u00b6 This tutorial shows how easy it is to run a load test for KServe when using gRPC to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash Deploy an InferenceService \u00b6 Create an InferenceService which exposes a gRPC port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl create -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF Launch Experiment \u00b6 Launch the Iter8 experiment inside the Kubernetes cluster: GRPC_HOST = $( kubectl get isvc sklearn-irisv2 -o jsonpath = '{.status.components.predictor.address.url}' | sed 's#.*//##' ) GRPC_PORT = 80 iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set grpc.protoURL = https://raw.githubusercontent.com/kserve/kserve/master/docs/predict-api/v2/grpc_predict_v2.proto \\ --set grpc.host = ${ GRPC_HOST } : ${ GRPC_PORT } \\ --set grpc.call = inference.GRPCInferenceService.ModelInfer \\ --set grpc.dataURL = https://gist.githubusercontent.com/kalantar/6e9eaa03cad8f4e86b20eeb712efef45/raw/56496ed5fa9078b8c9cdad590d275ab93beaaee4/sklearn-irisv2-input-grpc.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 5000 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 7500 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , grpc , and assess . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The grpc task sends call requests to the inference.GRPCInferenceService.ModelInfer method of the cluster-local gRPC service with host address ${GRPC_HOST}:${GRPC_PORT} , and collects Iter8's built-in gRPC load test metrics. The assess task verifies if the app satisfies the specified SLOs: i) there are no errors, ii) the mean latency of the service does not exceed 50 msec, and iii) the 97.5 th percentile latency does not exceed 200 msec. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment . Some variations and extensions of this experiment The grpc task can be configured with load related parameters such as the number of requests, requests per second, or number of concurrent connections. The assess task can be configured with SLOs for any of Iter8's built-in gRPC load test metrics . Clean up \u00b6 iter8 k delete kubectl delete inferenceservice sklearn-irisv2","title":"gRPC"},{"location":"tutorials/integrations/kserve/grpc/#load-test-a-kserve-model-via-grpc","text":"This tutorial shows how easy it is to run a load test for KServe when using gRPC to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash","title":"Load Test a KServe Model (via gRPC)"},{"location":"tutorials/integrations/kserve/grpc/#deploy-an-inferenceservice","text":"Create an InferenceService which exposes a gRPC port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl create -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF","title":"Deploy an InferenceService"},{"location":"tutorials/integrations/kserve/grpc/#launch-experiment","text":"Launch the Iter8 experiment inside the Kubernetes cluster: GRPC_HOST = $( kubectl get isvc sklearn-irisv2 -o jsonpath = '{.status.components.predictor.address.url}' | sed 's#.*//##' ) GRPC_PORT = 80 iter8 k launch \\ --set \"tasks={ready,grpc,assess}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set grpc.protoURL = https://raw.githubusercontent.com/kserve/kserve/master/docs/predict-api/v2/grpc_predict_v2.proto \\ --set grpc.host = ${ GRPC_HOST } : ${ GRPC_PORT } \\ --set grpc.call = inference.GRPCInferenceService.ModelInfer \\ --set grpc.dataURL = https://gist.githubusercontent.com/kalantar/6e9eaa03cad8f4e86b20eeb712efef45/raw/56496ed5fa9078b8c9cdad590d275ab93beaaee4/sklearn-irisv2-input-grpc.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 5000 \\ --set assess.SLOs.upper.grpc/latency/p '97\\.5' = 7500 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , grpc , and assess . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The grpc task sends call requests to the inference.GRPCInferenceService.ModelInfer method of the cluster-local gRPC service with host address ${GRPC_HOST}:${GRPC_PORT} , and collects Iter8's built-in gRPC load test metrics. The assess task verifies if the app satisfies the specified SLOs: i) there are no errors, ii) the mean latency of the service does not exceed 50 msec, and iii) the 97.5 th percentile latency does not exceed 200 msec. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment . Some variations and extensions of this experiment The grpc task can be configured with load related parameters such as the number of requests, requests per second, or number of concurrent connections. The assess task can be configured with SLOs for any of Iter8's built-in gRPC load test metrics .","title":"Launch Experiment"},{"location":"tutorials/integrations/kserve/grpc/#clean-up","text":"iter8 k delete kubectl delete inferenceservice sklearn-irisv2","title":"Clean up"},{"location":"tutorials/integrations/kserve/http/","text":"Load Test a KServe Model (via HTTP) \u00b6 This tutorial shows how easy it is to run a load test for KServe when using HTTP to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash Deploy an InferenceService \u00b6 Create an InferenceService which exposes an HTTP port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF Launch Experiment \u00b6 Launch an Iter8 experiment inside the Kubernetes cluster: iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set http.url = http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer \\ --set http.payloadURL = https://gist.githubusercontent.com/kalantar/d2dd03e8ebff2c57c3cfa992b44a54ad/raw/97a0480d0dfb1deef56af73a0dd31c80dc9b71f4/sklearn-irisv2-input.json \\ --set http.contentType = \"application/json\" \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , http , and assess . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The http task sends requests to the cluster-local HTTP service whose URL exposed by the InferenceService, http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer , and collects Iter8's built-in HTTP load test metrics . The assess task verifies if the app satisfies the specified SLOs: i) the mean latency of the service does not exceed 50 msec, and ii) there are no errors (4xx or 5xx response codes) in the responses. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment . Some variations and extensions of this experiment The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The assess task can be configured with SLOs for any of Iter8's built-in HTTP load test metrics . Clean up \u00b6 iter8 k delete kubectl delete inferenceservice sklearn-irisv2","title":"HTTP"},{"location":"tutorials/integrations/kserve/http/#load-test-a-kserve-model-via-http","text":"This tutorial shows how easy it is to run a load test for KServe when using HTTP to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try your first experiment . Understand the main concepts behind Iter8 experiments. Ensure that you have the kubectl CLI. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.10/hack/quick_install.sh\" | bash","title":"Load Test a KServe Model (via HTTP)"},{"location":"tutorials/integrations/kserve/http/#deploy-an-inferenceservice","text":"Create an InferenceService which exposes an HTTP port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF","title":"Deploy an InferenceService"},{"location":"tutorials/integrations/kserve/http/#launch-experiment","text":"Launch an Iter8 experiment inside the Kubernetes cluster: iter8 k launch \\ --set \"tasks={ready,http,assess}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set http.url = http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer \\ --set http.payloadURL = https://gist.githubusercontent.com/kalantar/d2dd03e8ebff2c57c3cfa992b44a54ad/raw/97a0480d0dfb1deef56af73a0dd31c80dc9b71f4/sklearn-irisv2-input.json \\ --set http.contentType = \"application/json\" \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job About this experiment This experiment consists of three tasks , namely, ready , http , and assess . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The http task sends requests to the cluster-local HTTP service whose URL exposed by the InferenceService, http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer , and collects Iter8's built-in HTTP load test metrics . The assess task verifies if the app satisfies the specified SLOs: i) the mean latency of the service does not exceed 50 msec, and ii) there are no errors (4xx or 5xx response codes) in the responses. This is a single-loop Kubernetes experiment where all the previously mentioned tasks will run once and the experiment will finish. Hence, its runner value is set to job . You can assert experiment outcomes, view an experiment report, and view experiment logs as described in your first experiment . Some variations and extensions of this experiment The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The assess task can be configured with SLOs for any of Iter8's built-in HTTP load test metrics .","title":"Launch Experiment"},{"location":"tutorials/integrations/kserve/http/#clean-up","text":"iter8 k delete kubectl delete inferenceservice sklearn-irisv2","title":"Clean up"},{"location":"tutorials/integrations/kserve/kserve/","text":"KServe Examples based on Iter8 v0.7 \u00b6 A/B Testing and Progressive Traffic Shift Hybrid (A/B + SLOs) testing Progressive traffic shifting Fixed-%-split Session affinity","title":"Other (Iter8 v0.7)"},{"location":"tutorials/integrations/kserve/kserve/#kserve-examples-based-on-iter8-v07","text":"A/B Testing and Progressive Traffic Shift Hybrid (A/B + SLOs) testing Progressive traffic shifting Fixed-%-split Session affinity","title":"KServe Examples based on Iter8 v0.7"},{"location":"tutorials/integrations/kserve-mm/blue-green/","text":"Blue-Green Rollout of a ML Model \u00b6 This tutorial shows how Iter8 can be used to implement a blue-green rollout of ML models hosted in a KServe modelmesh serving environment. In a blue-green rollout, a percentage of inference requests are directed to a candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a blue-green rollout by automatically configuring the network to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate model. Iter8 automatically handles all underlying network configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. Install Istio . You can install the demo profile . Install the Iter8 controller \u00b6 Helm Kustomize Install the Iter8 controller using helm as follows. helm install --repo https://iter8-tools.github.io/hub iter8-traffic traffic Install the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1' Deploy a primary model \u00b6 Deploy the primary version of a model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService Naming the model with the suffix -0 (and the candidate with the suffix -1 ) simplifies the rollout initialization. However, any name can be specified. The label iter8.tools/watch: \"true\" lets Iter8 know that it should pay attention to changes to this InferenceService . Inspect the deployed InferenceService : kubectl get inferenceservice wisdom-0 When the READY field becomes True , the model is fully deployed. Initialize the Blue-Green routing policy \u00b6 Initialize model rollout with a blue-green traffic pattern as follows: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: blue-green modelName: wisdom EOF The initialize-rollout template (with trafficStrategy: blue-green ) configures the Istio service mesh to route all requests to the primary version of the model ( wisdom-0 ). Further, it defines the routing policy that will be used by Iter8 when it observes changes in the models. By default, this routing policy splits inference requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart. Verify network configuration \u00b6 To verify the network configuration, you can inspect the network configuration: kubectl get virtualservice -o yaml wisdom To send inference requests to the model: In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer Note that the model version responding to each inference request can be determined from the modelName field of the response. Deploy a candidate model \u00b6 Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate InferenceService The model name ( wisdom ) and version ( v2 ) are recorded using the labels app.kubernets.io/name and app.kubernets.io.version . In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real world example, this would be different. Verify network configuration changes \u00b6 The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that inference requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml Send additional inference requests as described above. Modify weights (optional) \u00b6 You can modify the weight distribution of inference requests using the Iter8 traffic-template chart: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: modify-weights targetEnv: kserve-modelmesh trafficStrategy: blue-green modelName: wisdom modelVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights overrides the default traffic split for all future candidate deployments. As above, you can verify the network configuration changes. Promote the candidate model \u00b6 Promoting the candidate involves redefining the primary InferenceService using the new model and deleting the candidate InferenceService . Redefine the primary InferenceService \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernets.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete the candidate InferenceService \u00b6 kubectl delete inferenceservice wisdom-1 Verify network configuration changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model. Clean up \u00b6 Delete the candidate model: kubectl delete --force isvc/wisdom-1 Delete routing artifacts: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl delete --force -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: blue-green modelName: wisdom EOF Delete the primary model: kubectl delete --force isvc/wisdom-0 Uninstall the Iter8 controller: Helm Kustomize Delete the Iter8 controller using helm as follows. helm delete iter8-traffic Delete the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Blue-Green"},{"location":"tutorials/integrations/kserve-mm/blue-green/#blue-green-rollout-of-a-ml-model","text":"This tutorial shows how Iter8 can be used to implement a blue-green rollout of ML models hosted in a KServe modelmesh serving environment. In a blue-green rollout, a percentage of inference requests are directed to a candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a blue-green rollout by automatically configuring the network to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate model. Iter8 automatically handles all underlying network configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. Install Istio . You can install the demo profile .","title":"Blue-Green Rollout of a ML Model"},{"location":"tutorials/integrations/kserve-mm/blue-green/#install-the-iter8-controller","text":"Helm Kustomize Install the Iter8 controller using helm as follows. helm install --repo https://iter8-tools.github.io/hub iter8-traffic traffic Install the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve-mm/blue-green/#deploy-a-primary-model","text":"Deploy the primary version of a model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService Naming the model with the suffix -0 (and the candidate with the suffix -1 ) simplifies the rollout initialization. However, any name can be specified. The label iter8.tools/watch: \"true\" lets Iter8 know that it should pay attention to changes to this InferenceService . Inspect the deployed InferenceService : kubectl get inferenceservice wisdom-0 When the READY field becomes True , the model is fully deployed.","title":"Deploy a primary model"},{"location":"tutorials/integrations/kserve-mm/blue-green/#initialize-the-blue-green-routing-policy","text":"Initialize model rollout with a blue-green traffic pattern as follows: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: blue-green modelName: wisdom EOF The initialize-rollout template (with trafficStrategy: blue-green ) configures the Istio service mesh to route all requests to the primary version of the model ( wisdom-0 ). Further, it defines the routing policy that will be used by Iter8 when it observes changes in the models. By default, this routing policy splits inference requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart.","title":"Initialize the Blue-Green routing policy"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-network-configuration","text":"To verify the network configuration, you can inspect the network configuration: kubectl get virtualservice -o yaml wisdom To send inference requests to the model: In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer Note that the model version responding to each inference request can be determined from the modelName field of the response.","title":"Verify network configuration"},{"location":"tutorials/integrations/kserve-mm/blue-green/#deploy-a-candidate-model","text":"Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate InferenceService The model name ( wisdom ) and version ( v2 ) are recorded using the labels app.kubernets.io/name and app.kubernets.io.version . In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real world example, this would be different.","title":"Deploy a candidate model"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-network-configuration-changes","text":"The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that inference requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml Send additional inference requests as described above.","title":"Verify network configuration changes"},{"location":"tutorials/integrations/kserve-mm/blue-green/#modify-weights-optional","text":"You can modify the weight distribution of inference requests using the Iter8 traffic-template chart: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: modify-weights targetEnv: kserve-modelmesh trafficStrategy: blue-green modelName: wisdom modelVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights overrides the default traffic split for all future candidate deployments. As above, you can verify the network configuration changes.","title":"Modify weights (optional)"},{"location":"tutorials/integrations/kserve-mm/blue-green/#promote-the-candidate-model","text":"Promoting the candidate involves redefining the primary InferenceService using the new model and deleting the candidate InferenceService .","title":"Promote the candidate model"},{"location":"tutorials/integrations/kserve-mm/blue-green/#redefine-the-primary-inferenceservice","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernets.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine the primary InferenceService"},{"location":"tutorials/integrations/kserve-mm/blue-green/#delete-the-candidate-inferenceservice","text":"kubectl delete inferenceservice wisdom-1","title":"Delete the candidate InferenceService"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-network-configuration-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model.","title":"Verify network configuration changes"},{"location":"tutorials/integrations/kserve-mm/blue-green/#clean-up","text":"Delete the candidate model: kubectl delete --force isvc/wisdom-1 Delete routing artifacts: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl delete --force -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: blue-green modelName: wisdom EOF Delete the primary model: kubectl delete --force isvc/wisdom-0 Uninstall the Iter8 controller: Helm Kustomize Delete the Iter8 controller using helm as follows. helm delete iter8-traffic Delete the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Clean up"},{"location":"tutorials/integrations/kserve-mm/canary/","text":"Canary Rollout of a ML Model \u00b6 This tutorial shows how Iter8 can be used to implement a canary rollout of ML models hosted in a KServe modelmesh serving environment. In a canary rollout, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a canary rollout by automatically configuring the network to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Iter8 automatically handles the underlying network configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. Install Istio . You can install the demo profile . Install the Iter8 controller \u00b6 Helm Kustomize Install the Iter8 controller using helm as follows. helm install --repo https://iter8-tools.github.io/hub iter8-traffic traffic Install the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1' Deploy a primary model \u00b6 Deploy the primary version of a model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService Naming the model with the suffix -0 (and the candidate with the suffix -1 ) simplifies the rollout initialization. However, any name can be specified. The label iter8.tools/watch: \"true\" lets Iter8 know that it should pay attention to changes to this InferenceService . Inspect the deployed InferenceService : kubectl get inferenceservice wisdom-0 When the READY field becomes True , the model is fully deployed. Initialize the Canary routing policy \u00b6 Initialize model rollout with a canary traffic pattern as follows: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: canary modelName: wisdom EOF The initialize-rollout template (with trafficStrategy: canary ) configures the Istio service mesh to route all requests to the primary version of the model ( wisdom-0 ). Further, it defines a routing policy that will be used by Iter8 when it observes changes in the models. By default, this routing policy sends inference requests with the header traffic set to the value test to the candidate version of the model and all remaining inference requests to the primary version of the model. For detailed configuration options, see the Helm chart. Verify network configuration \u00b6 To verify the network configuration, you can inspect the network configuration: kubectl get virtualservice -o yaml wisdom To send inference requests to the model: In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer or, send request with header traffic: test : cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -H 'traffic: test' \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer Note that the model version responding to each inference request can be determined from the modelName field of the response. Deploy a candidate model \u00b6 Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate InferenceService The model name ( wisdom ) and version ( v2 ) are recorded using the labels app.kubernets.io/name and app.kubernets.io.version . In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different. Verify network configuration changes \u00b6 The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that inference requests are now distributed between the primary model and the secondary model according to a header matching rule: kubectl get virtualservice wisdom -o yaml Send additional inference requests as described above. Promote the candidate model \u00b6 Promoting the candidate involves redefining the primary InferenceService using the new model and deleting the candidate InferenceService . Redefine the primary InferenceService \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernets.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete the candidate InferenceService \u00b6 kubectl delete inferenceservice wisdom-1 Verify network configuration changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model. Clean up \u00b6 Delete the candidate model: kubectl delete --force isvc/wisdom-1 Delete routing artifacts: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl delete --force -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: canary modelName: wisdom EOF Delete the primary model: kubectl delete --force isvc/wisdom-0 Uninstall the Iter8 controller: Helm Kustomize Delete the Iter8 controller using helm as follows. helm delete iter8-traffic Delete the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Canary"},{"location":"tutorials/integrations/kserve-mm/canary/#canary-rollout-of-a-ml-model","text":"This tutorial shows how Iter8 can be used to implement a canary rollout of ML models hosted in a KServe modelmesh serving environment. In a canary rollout, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a canary rollout by automatically configuring the network to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Iter8 automatically handles the underlying network configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. Install Istio . You can install the demo profile .","title":"Canary Rollout of a ML Model"},{"location":"tutorials/integrations/kserve-mm/canary/#install-the-iter8-controller","text":"Helm Kustomize Install the Iter8 controller using helm as follows. helm install --repo https://iter8-tools.github.io/hub iter8-traffic traffic Install the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve-mm/canary/#deploy-a-primary-model","text":"Deploy the primary version of a model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService Naming the model with the suffix -0 (and the candidate with the suffix -1 ) simplifies the rollout initialization. However, any name can be specified. The label iter8.tools/watch: \"true\" lets Iter8 know that it should pay attention to changes to this InferenceService . Inspect the deployed InferenceService : kubectl get inferenceservice wisdom-0 When the READY field becomes True , the model is fully deployed.","title":"Deploy a primary model"},{"location":"tutorials/integrations/kserve-mm/canary/#initialize-the-canary-routing-policy","text":"Initialize model rollout with a canary traffic pattern as follows: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: canary modelName: wisdom EOF The initialize-rollout template (with trafficStrategy: canary ) configures the Istio service mesh to route all requests to the primary version of the model ( wisdom-0 ). Further, it defines a routing policy that will be used by Iter8 when it observes changes in the models. By default, this routing policy sends inference requests with the header traffic set to the value test to the candidate version of the model and all remaining inference requests to the primary version of the model. For detailed configuration options, see the Helm chart.","title":"Initialize the Canary routing policy"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-network-configuration","text":"To verify the network configuration, you can inspect the network configuration: kubectl get virtualservice -o yaml wisdom To send inference requests to the model: In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer or, send request with header traffic: test : cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -H 'traffic: test' \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer Note that the model version responding to each inference request can be determined from the modelName field of the response.","title":"Verify network configuration"},{"location":"tutorials/integrations/kserve-mm/canary/#deploy-a-candidate-model","text":"Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate InferenceService The model name ( wisdom ) and version ( v2 ) are recorded using the labels app.kubernets.io/name and app.kubernets.io.version . In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different.","title":"Deploy a candidate model"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-network-configuration-changes","text":"The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that inference requests are now distributed between the primary model and the secondary model according to a header matching rule: kubectl get virtualservice wisdom -o yaml Send additional inference requests as described above.","title":"Verify network configuration changes"},{"location":"tutorials/integrations/kserve-mm/canary/#promote-the-candidate-model","text":"Promoting the candidate involves redefining the primary InferenceService using the new model and deleting the candidate InferenceService .","title":"Promote the candidate model"},{"location":"tutorials/integrations/kserve-mm/canary/#redefine-the-primary-inferenceservice","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernets.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine the primary InferenceService"},{"location":"tutorials/integrations/kserve-mm/canary/#delete-the-candidate-inferenceservice","text":"kubectl delete inferenceservice wisdom-1","title":"Delete the candidate InferenceService"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-network-configuration-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model.","title":"Verify network configuration changes"},{"location":"tutorials/integrations/kserve-mm/canary/#clean-up","text":"Delete the candidate model: kubectl delete --force isvc/wisdom-1 Delete routing artifacts: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl delete --force -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: canary modelName: wisdom EOF Delete the primary model: kubectl delete --force isvc/wisdom-0 Uninstall the Iter8 controller: Helm Kustomize Delete the Iter8 controller using helm as follows. helm delete iter8-traffic Delete the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Clean up"},{"location":"tutorials/integrations/kserve-mm/deleteiter8controller/","text":"Helm Kustomize Delete the Iter8 controller using helm as follows. helm delete iter8-traffic Delete the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Deleteiter8controller"},{"location":"tutorials/integrations/kserve-mm/installiter8controller/","text":"Helm Kustomize Install the Iter8 controller using helm as follows. helm install --repo https://iter8-tools.github.io/hub iter8-traffic traffic Install the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Installiter8controller"},{"location":"tutorials/integrations/kserve-mm/mirror/","text":"Mirrored Rollout of a ML Model \u00b6 This tutorial shows how Iter8 can be used to implement a mirrored rollout of ML models in a KServe modelmesh serving environment. In a mirrored rollout, all inference requests are sent to the primary version of the model. In addition, a portion of the requests are also sent to the candidate version of the model. The responses from the candidate version are ignored. Iter8 enables a mirrored rollout by automatically configuring the network to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate version of the model. Iter8 automatically handles the underlying network configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. Install Istio . You can install the demo profile . Install the Iter8 controller \u00b6 Helm Kustomize Install the Iter8 controller using helm as follows. helm install --repo https://iter8-tools.github.io/hub iter8-traffic traffic Install the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1' Deploy a primary model \u00b6 Deploy the primary version of a model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService Naming the model with the suffix -0 (and the candidate with the suffix -1 ) simplifies the rollout initialization. However, any name can be specified. The label iter8.tools/watch: \"true\" lets Iter8 know that it should pay attention to changes to this InferenceService . Inspect the deployed InferenceService : kubectl get inferenceservice wisdom-0 When the READY field becomes True , the model is fully deployed. Initialize the Mirroring routing policy \u00b6 Initialize the model rollout with a mirror traffic pattern as follows: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: mirror modelName: wisdom EOF The initialize-rollout template ( with trafficStrategy: mirror ) configures the Istio service mesh to route all requests to the primary model ( wisdom-0 ). Further, it defines a routing policy that will be used by Iter8 when it observes changes in the models. By default, this routing policy sends all inference requests to the primary version of the model. It also sends them all of them (100%) to the candidate version of the model. Responses from the candidate are ignored. Verify network configuration \u00b6 You can inspect the network configuration: kubectl get virtualservice -o yaml wisdom To send inference requests to the model: In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer Note that the model version responding to each inference request can be determined from the modelName field of the response. Deploy a candidate model \u00b6 Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate InferenceService The model name ( wisdom ) and version ( v2 ) are recorded using the labels app.kubernets.io/name and app.kubernets.io.version . In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real world example, this would be different. Verify network configuration changes \u00b6 The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that inference requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml Modify the percentage of mirrored traffic (optional) \u00b6 You can modify the percentage of inference requests that are mirrored (send to the candidate version) using the Iter8 traffic-template chart. For example, to change the mirrored percentage to 20%, use: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: modify-weights targetEnv: kserve-modelmesh trafficStrategy: mirror modelName: wisdom mirrorPercentage: 20 EOF Note that using the modify-weights overrides the default behavior for all future candidate deployments. As above, you can verify the network configuration changes. Promote the candidate model \u00b6 Promoting the candidate involves redefining the primary InferenceService using the new model and deleting the candidate InferenceService . Redefine the primary InferenceService \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernets.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete the candidate InferenceService \u00b6 kubectl delete inferenceservice wisdom-1 Verify network configuration changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model. Clean up \u00b6 Delete the candidate model: kubectl delete --force isvc/wisdom-1 Delete routing artifacts: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl delete --force -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: mirror modelName: wisdom EOF Delete the primary model: kubectl delete --force isvc/wisdom-0 Uninstall the Iter8 controller: Helm Kustomize Delete the Iter8 controller using helm as follows. helm delete iter8-traffic Delete the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Mirrored Rollout of a ML Model"},{"location":"tutorials/integrations/kserve-mm/mirror/#mirrored-rollout-of-a-ml-model","text":"This tutorial shows how Iter8 can be used to implement a mirrored rollout of ML models in a KServe modelmesh serving environment. In a mirrored rollout, all inference requests are sent to the primary version of the model. In addition, a portion of the requests are also sent to the candidate version of the model. The responses from the candidate version are ignored. Iter8 enables a mirrored rollout by automatically configuring the network to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate version of the model. Iter8 automatically handles the underlying network configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. Install Istio . You can install the demo profile .","title":"Mirrored Rollout of a ML Model"},{"location":"tutorials/integrations/kserve-mm/mirror/#install-the-iter8-controller","text":"Helm Kustomize Install the Iter8 controller using helm as follows. helm install --repo https://iter8-tools.github.io/hub iter8-traffic traffic Install the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl apply -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve-mm/mirror/#deploy-a-primary-model","text":"Deploy the primary version of a model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService Naming the model with the suffix -0 (and the candidate with the suffix -1 ) simplifies the rollout initialization. However, any name can be specified. The label iter8.tools/watch: \"true\" lets Iter8 know that it should pay attention to changes to this InferenceService . Inspect the deployed InferenceService : kubectl get inferenceservice wisdom-0 When the READY field becomes True , the model is fully deployed.","title":"Deploy a primary model"},{"location":"tutorials/integrations/kserve-mm/mirror/#initialize-the-mirroring-routing-policy","text":"Initialize the model rollout with a mirror traffic pattern as follows: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: mirror modelName: wisdom EOF The initialize-rollout template ( with trafficStrategy: mirror ) configures the Istio service mesh to route all requests to the primary model ( wisdom-0 ). Further, it defines a routing policy that will be used by Iter8 when it observes changes in the models. By default, this routing policy sends all inference requests to the primary version of the model. It also sends them all of them (100%) to the candidate version of the model. Responses from the candidate are ignored.","title":"Initialize the Mirroring routing policy"},{"location":"tutorials/integrations/kserve-mm/mirror/#verify-network-configuration","text":"You can inspect the network configuration: kubectl get virtualservice -o yaml wisdom To send inference requests to the model: In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.13.18/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer Note that the model version responding to each inference request can be determined from the modelName field of the response.","title":"Verify network configuration"},{"location":"tutorials/integrations/kserve-mm/mirror/#deploy-a-candidate-model","text":"Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate InferenceService The model name ( wisdom ) and version ( v2 ) are recorded using the labels app.kubernets.io/name and app.kubernets.io.version . In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real world example, this would be different.","title":"Deploy a candidate model"},{"location":"tutorials/integrations/kserve-mm/mirror/#verify-network-configuration-changes","text":"The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that inference requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml","title":"Verify network configuration changes"},{"location":"tutorials/integrations/kserve-mm/mirror/#modify-the-percentage-of-mirrored-traffic-optional","text":"You can modify the percentage of inference requests that are mirrored (send to the candidate version) using the Iter8 traffic-template chart. For example, to change the mirrored percentage to 20%, use: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl apply -f - templateName: modify-weights targetEnv: kserve-modelmesh trafficStrategy: mirror modelName: wisdom mirrorPercentage: 20 EOF Note that using the modify-weights overrides the default behavior for all future candidate deployments. As above, you can verify the network configuration changes.","title":"Modify the percentage of mirrored traffic (optional)"},{"location":"tutorials/integrations/kserve-mm/mirror/#promote-the-candidate-model","text":"Promoting the candidate involves redefining the primary InferenceService using the new model and deleting the candidate InferenceService .","title":"Promote the candidate model"},{"location":"tutorials/integrations/kserve-mm/mirror/#redefine-the-primary-inferenceservice","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 namespace: modelmesh-serving labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v2 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernets.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine the primary InferenceService"},{"location":"tutorials/integrations/kserve-mm/mirror/#delete-the-candidate-inferenceservice","text":"kubectl delete inferenceservice wisdom-1","title":"Delete the candidate InferenceService"},{"location":"tutorials/integrations/kserve-mm/mirror/#verify-network-configuration-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model.","title":"Verify network configuration changes"},{"location":"tutorials/integrations/kserve-mm/mirror/#clean-up","text":"Delete the candidate model: kubectl delete --force isvc/wisdom-1 Delete routing artifacts: cat <<EOF | helm template traffic --repo https://iter8-tools.github.io/hub traffic-templates -f - | kubectl delete --force -f - templateName: initialize-rollout targetEnv: kserve-modelmesh trafficStrategy: mirror modelName: wisdom EOF Delete the primary model: kubectl delete --force isvc/wisdom-0 Uninstall the Iter8 controller: Helm Kustomize Delete the Iter8 controller using helm as follows. helm delete iter8-traffic Delete the Iter8 controller using kustomize as follows. cluster scoped namespace scoped kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/clusterScoped?ref=traffic-templates-0.1.1' kubectl delete -k 'https://github.com/iter8-tools/hub.git/kustomize/traffic/namespaceScoped?ref=traffic-templates-0.1.1'","title":"Clean up"},{"location":"user-guide/commands/iter8/","text":"iter8 \u00b6 Kubernetes release optimizer Synopsis \u00b6 Iter8 is the Kubernetes release optimizer built for DevOps, MLOps, SRE and data science teams. Iter8 makes it easy to ensure that Kubernetes apps and ML models perform well and maximize business value. Options \u00b6 -h, --help help for iter8 -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") SEE ALSO \u00b6 iter8 k - Work with Kubernetes experiments iter8 version - Print Iter8 CLI version Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8"},{"location":"user-guide/commands/iter8/#iter8","text":"Kubernetes release optimizer","title":"iter8"},{"location":"user-guide/commands/iter8/#synopsis","text":"Iter8 is the Kubernetes release optimizer built for DevOps, MLOps, SRE and data science teams. Iter8 makes it easy to ensure that Kubernetes apps and ML models perform well and maximize business value.","title":"Synopsis"},{"location":"user-guide/commands/iter8/#options","text":"-h, --help help for iter8 -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\")","title":"Options"},{"location":"user-guide/commands/iter8/#see-also","text":"iter8 k - Work with Kubernetes experiments iter8 version - Print Iter8 CLI version","title":"SEE ALSO"},{"location":"user-guide/commands/iter8/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/commands/iter8_k/","text":"iter8 k \u00b6 Work with Kubernetes experiments Synopsis \u00b6 Work with Kubernetes experiments Options \u00b6 -h, --help help for k --kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -n, --namespace string namespace scope for this request Options inherited from parent commands \u00b6 -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") SEE ALSO \u00b6 iter8 - Kubernetes release optimizer iter8 k assert - Assert if Kubernetes experiment result satisfies conditions iter8 k delete - Delete an experiment (group) in Kubernetes iter8 k launch - Launch an experiment inside a Kubernetes cluster iter8 k log - Fetch logs for a Kubernetes experiment iter8 k report - Generate report for Kubernetes experiment Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8 k"},{"location":"user-guide/commands/iter8_k/#iter8-k","text":"Work with Kubernetes experiments","title":"iter8 k"},{"location":"user-guide/commands/iter8_k/#synopsis","text":"Work with Kubernetes experiments","title":"Synopsis"},{"location":"user-guide/commands/iter8_k/#options","text":"-h, --help help for k --kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -n, --namespace string namespace scope for this request","title":"Options"},{"location":"user-guide/commands/iter8_k/#options-inherited-from-parent-commands","text":"-l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\")","title":"Options inherited from parent commands"},{"location":"user-guide/commands/iter8_k/#see-also","text":"iter8 - Kubernetes release optimizer iter8 k assert - Assert if Kubernetes experiment result satisfies conditions iter8 k delete - Delete an experiment (group) in Kubernetes iter8 k launch - Launch an experiment inside a Kubernetes cluster iter8 k log - Fetch logs for a Kubernetes experiment iter8 k report - Generate report for Kubernetes experiment","title":"SEE ALSO"},{"location":"user-guide/commands/iter8_k/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/commands/iter8_k_assert/","text":"iter8 k assert \u00b6 Assert if Kubernetes experiment result satisfies conditions Synopsis \u00b6 Assert if the result of a Kubernetes experiment satisfies the specified conditions. If all conditions are satisfied, the command exits with code 0. Else, the command exits with code 1. Assertions are especially useful for automation inside CI/CD/GitOps pipelines. Supported conditions are 'completed', 'nofailure', 'slos', which indicate that the experiment has completed, none of the tasks have failed, and the SLOs are satisfied. iter8 k assert -c completed -c nofailure -c slos # same as iter8 k assert -c completed,nofailure,slos You can optionally specify a timeout, which is the maximum amount of time to wait for the conditions to be satisfied: iter8 k assert -c completed,nofailure,slos -t 5s iter8 k assert [flags] Options \u00b6 -c, --condition strings completed | nofailure | slos; can specify multiple or separate conditions with commas; -g, --group string name of the experiment group (default \"default\") -h, --help help for assert --timeout duration timeout duration (e.g., 5s) Options inherited from parent commands \u00b6 --kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request SEE ALSO \u00b6 iter8 k - Work with Kubernetes experiments Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8 k assert"},{"location":"user-guide/commands/iter8_k_assert/#iter8-k-assert","text":"Assert if Kubernetes experiment result satisfies conditions","title":"iter8 k assert"},{"location":"user-guide/commands/iter8_k_assert/#synopsis","text":"Assert if the result of a Kubernetes experiment satisfies the specified conditions. If all conditions are satisfied, the command exits with code 0. Else, the command exits with code 1. Assertions are especially useful for automation inside CI/CD/GitOps pipelines. Supported conditions are 'completed', 'nofailure', 'slos', which indicate that the experiment has completed, none of the tasks have failed, and the SLOs are satisfied. iter8 k assert -c completed -c nofailure -c slos # same as iter8 k assert -c completed,nofailure,slos You can optionally specify a timeout, which is the maximum amount of time to wait for the conditions to be satisfied: iter8 k assert -c completed,nofailure,slos -t 5s iter8 k assert [flags]","title":"Synopsis"},{"location":"user-guide/commands/iter8_k_assert/#options","text":"-c, --condition strings completed | nofailure | slos; can specify multiple or separate conditions with commas; -g, --group string name of the experiment group (default \"default\") -h, --help help for assert --timeout duration timeout duration (e.g., 5s)","title":"Options"},{"location":"user-guide/commands/iter8_k_assert/#options-inherited-from-parent-commands","text":"--kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request","title":"Options inherited from parent commands"},{"location":"user-guide/commands/iter8_k_assert/#see-also","text":"iter8 k - Work with Kubernetes experiments","title":"SEE ALSO"},{"location":"user-guide/commands/iter8_k_assert/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/commands/iter8_k_delete/","text":"iter8 k delete \u00b6 Delete an experiment (group) in Kubernetes Synopsis \u00b6 Delete an experiment (group) in Kubernetes. iter8 k delete iter8 k delete [flags] Options \u00b6 -g, --group string name of the experiment group (default \"default\") -h, --help help for delete Options inherited from parent commands \u00b6 --kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request SEE ALSO \u00b6 iter8 k - Work with Kubernetes experiments Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8 k delete"},{"location":"user-guide/commands/iter8_k_delete/#iter8-k-delete","text":"Delete an experiment (group) in Kubernetes","title":"iter8 k delete"},{"location":"user-guide/commands/iter8_k_delete/#synopsis","text":"Delete an experiment (group) in Kubernetes. iter8 k delete iter8 k delete [flags]","title":"Synopsis"},{"location":"user-guide/commands/iter8_k_delete/#options","text":"-g, --group string name of the experiment group (default \"default\") -h, --help help for delete","title":"Options"},{"location":"user-guide/commands/iter8_k_delete/#options-inherited-from-parent-commands","text":"--kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request","title":"Options inherited from parent commands"},{"location":"user-guide/commands/iter8_k_delete/#see-also","text":"iter8 k - Work with Kubernetes experiments","title":"SEE ALSO"},{"location":"user-guide/commands/iter8_k_delete/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/commands/iter8_k_launch/","text":"iter8 k launch \u00b6 Launch an experiment inside a Kubernetes cluster Synopsis \u00b6 Launch an experiment inside a Kubernetes cluster. iter8 k launch --set \"tasks={http}\" --set http.url=https://httpbin.org/get \\ --set runner=job Use the dry option to simulate a Kubernetes experiment. This creates the manifest.yaml file, but does not run the experiment, and does not deploy any experiment resource objects in the cluster. iter8 k launch \\ --set http.url=https://httpbin.org/get \\ --set runner=job \\ --dry The launch command creates the 'charts' subdirectory under the current working directory, downloads the Iter8 experiment chart, and places it under 'charts'. This behavior can be controlled using various launch flags. This command supports setting values using the same mechanisms as in Helm. Please see https://helm.sh/docs/chart_template_guide/values_files/ for more detailed descriptions. In particular, this command supports the --set, --set-file, --set-string, and -f (--values) options all of which have the same behavior as in Helm. iter8 k launch [flags] Options \u00b6 -c, --chartName string name of the experiment chart (default \"iter8\") --dry simulate an experiment launch; outputs manifest.yaml file -g, --group string name of the experiment group (default \"default\") -h, --help help for launch --localChart use local chart identified by --chartName --set stringArray set values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --set-file stringArray set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string stringArray set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) -f, --values strings specify values in a YAML file or a URL (can specify multiple) Options inherited from parent commands \u00b6 --kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request SEE ALSO \u00b6 iter8 k - Work with Kubernetes experiments Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8 k launch"},{"location":"user-guide/commands/iter8_k_launch/#iter8-k-launch","text":"Launch an experiment inside a Kubernetes cluster","title":"iter8 k launch"},{"location":"user-guide/commands/iter8_k_launch/#synopsis","text":"Launch an experiment inside a Kubernetes cluster. iter8 k launch --set \"tasks={http}\" --set http.url=https://httpbin.org/get \\ --set runner=job Use the dry option to simulate a Kubernetes experiment. This creates the manifest.yaml file, but does not run the experiment, and does not deploy any experiment resource objects in the cluster. iter8 k launch \\ --set http.url=https://httpbin.org/get \\ --set runner=job \\ --dry The launch command creates the 'charts' subdirectory under the current working directory, downloads the Iter8 experiment chart, and places it under 'charts'. This behavior can be controlled using various launch flags. This command supports setting values using the same mechanisms as in Helm. Please see https://helm.sh/docs/chart_template_guide/values_files/ for more detailed descriptions. In particular, this command supports the --set, --set-file, --set-string, and -f (--values) options all of which have the same behavior as in Helm. iter8 k launch [flags]","title":"Synopsis"},{"location":"user-guide/commands/iter8_k_launch/#options","text":"-c, --chartName string name of the experiment chart (default \"iter8\") --dry simulate an experiment launch; outputs manifest.yaml file -g, --group string name of the experiment group (default \"default\") -h, --help help for launch --localChart use local chart identified by --chartName --set stringArray set values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --set-file stringArray set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string stringArray set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) -f, --values strings specify values in a YAML file or a URL (can specify multiple)","title":"Options"},{"location":"user-guide/commands/iter8_k_launch/#options-inherited-from-parent-commands","text":"--kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request","title":"Options inherited from parent commands"},{"location":"user-guide/commands/iter8_k_launch/#see-also","text":"iter8 k - Work with Kubernetes experiments","title":"SEE ALSO"},{"location":"user-guide/commands/iter8_k_launch/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/commands/iter8_k_log/","text":"iter8 k log \u00b6 Fetch logs for a Kubernetes experiment Synopsis \u00b6 Fetch logs for a Kubernetes experiment. iter8 k log iter8 k log [flags] Options \u00b6 -g, --group string name of the experiment group (default \"default\") -h, --help help for log Options inherited from parent commands \u00b6 --kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request SEE ALSO \u00b6 iter8 k - Work with Kubernetes experiments Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8 k log"},{"location":"user-guide/commands/iter8_k_log/#iter8-k-log","text":"Fetch logs for a Kubernetes experiment","title":"iter8 k log"},{"location":"user-guide/commands/iter8_k_log/#synopsis","text":"Fetch logs for a Kubernetes experiment. iter8 k log iter8 k log [flags]","title":"Synopsis"},{"location":"user-guide/commands/iter8_k_log/#options","text":"-g, --group string name of the experiment group (default \"default\") -h, --help help for log","title":"Options"},{"location":"user-guide/commands/iter8_k_log/#options-inherited-from-parent-commands","text":"--kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request","title":"Options inherited from parent commands"},{"location":"user-guide/commands/iter8_k_log/#see-also","text":"iter8 k - Work with Kubernetes experiments","title":"SEE ALSO"},{"location":"user-guide/commands/iter8_k_log/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/commands/iter8_k_report/","text":"iter8 k report \u00b6 Generate report for Kubernetes experiment Synopsis \u00b6 Generate a text or HTML report of a Kubernetes experiment. iter8 k report # same as iter8 k report -o text or iter8 k report -o html > report.html # view with browser iter8 k report [flags] Options \u00b6 -g, --group string name of the experiment group (default \"default\") -h, --help help for report -o, --outputFormat string text | html (default \"text\") Options inherited from parent commands \u00b6 --kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request SEE ALSO \u00b6 iter8 k - Work with Kubernetes experiments Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8 k report"},{"location":"user-guide/commands/iter8_k_report/#iter8-k-report","text":"Generate report for Kubernetes experiment","title":"iter8 k report"},{"location":"user-guide/commands/iter8_k_report/#synopsis","text":"Generate a text or HTML report of a Kubernetes experiment. iter8 k report # same as iter8 k report -o text or iter8 k report -o html > report.html # view with browser iter8 k report [flags]","title":"Synopsis"},{"location":"user-guide/commands/iter8_k_report/#options","text":"-g, --group string name of the experiment group (default \"default\") -h, --help help for report -o, --outputFormat string text | html (default \"text\")","title":"Options"},{"location":"user-guide/commands/iter8_k_report/#options-inherited-from-parent-commands","text":"--kube-apiserver string the address and the port for the Kubernetes API server --kube-as-group stringArray group to impersonate for the operation, this flag can be repeated to specify multiple groups. --kube-as-user string username to impersonate for the operation --kube-ca-file string the certificate authority file for the Kubernetes API server connection --kube-context string name of the kubeconfig context to use --kube-token string bearer token used for authentication --kubeconfig string path to the kubeconfig file -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") -n, --namespace string namespace scope for this request","title":"Options inherited from parent commands"},{"location":"user-guide/commands/iter8_k_report/#see-also","text":"iter8 k - Work with Kubernetes experiments","title":"SEE ALSO"},{"location":"user-guide/commands/iter8_k_report/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/commands/iter8_version/","text":"iter8 version \u00b6 Print Iter8 CLI version Synopsis \u00b6 Print the version of Iter8 CLI. iter8 version The output may look as follows: $ cmd.BuildInfo{Version:\"v0.13.0\", GitCommit:\"f24e86f3d3eceb02eabbba54b40af2c940f55ad5\", GoVersion:\"go1.19.3\"} In the sample output shown above: Version is the semantic version of the Iter8 CLI. GitCommit is the SHA hash for the commit that this version was built from. GoVersion is the version of Go that was used to compile Iter8 CLI. iter8 version [flags] Options \u00b6 -h, --help help for version --short print abbreviated version info Options inherited from parent commands \u00b6 -l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\") SEE ALSO \u00b6 iter8 - Kubernetes release optimizer Auto generated by spf13/cobra on 25-Jan-2023 \u00b6","title":"iter8 version"},{"location":"user-guide/commands/iter8_version/#iter8-version","text":"Print Iter8 CLI version","title":"iter8 version"},{"location":"user-guide/commands/iter8_version/#synopsis","text":"Print the version of Iter8 CLI. iter8 version The output may look as follows: $ cmd.BuildInfo{Version:\"v0.13.0\", GitCommit:\"f24e86f3d3eceb02eabbba54b40af2c940f55ad5\", GoVersion:\"go1.19.3\"} In the sample output shown above: Version is the semantic version of the Iter8 CLI. GitCommit is the SHA hash for the commit that this version was built from. GoVersion is the version of Go that was used to compile Iter8 CLI. iter8 version [flags]","title":"Synopsis"},{"location":"user-guide/commands/iter8_version/#options","text":"-h, --help help for version --short print abbreviated version info","title":"Options"},{"location":"user-guide/commands/iter8_version/#options-inherited-from-parent-commands","text":"-l, --loglevel string trace, debug, info, warning, error, fatal, panic (default \"info\")","title":"Options inherited from parent commands"},{"location":"user-guide/commands/iter8_version/#see-also","text":"iter8 - Kubernetes release optimizer","title":"SEE ALSO"},{"location":"user-guide/commands/iter8_version/#auto-generated-by-spf13cobra-on-25-jan-2023","text":"","title":"Auto generated by spf13/cobra on 25-Jan-2023"},{"location":"user-guide/tasks/abnmetrics/","text":"abnmetrics \u00b6 Fetch metrics from the Iter8 A/B/n service. Usage example \u00b6 In this experiment, the abnmetrics task fetches metrics from the A/B/n service for the application default/backend . The task is run periodically (as defined by cronjobSchedule ). iter8 k launch \\ --set \"tasks={abnmetrics}\" \\ --set abnmetrics.application=default/backend \\ --set runner=cronjob \\ --set cronjobSchedule=\"*/1 * * * *\" Parameters \u00b6 Name Type Description endpoint string Endpoint of the A/B/n service. Defaults to iter8-abn:50051 application string Application name in form namespace/name","title":"abnmetrics"},{"location":"user-guide/tasks/abnmetrics/#abnmetrics","text":"Fetch metrics from the Iter8 A/B/n service.","title":"abnmetrics"},{"location":"user-guide/tasks/abnmetrics/#usage-example","text":"In this experiment, the abnmetrics task fetches metrics from the A/B/n service for the application default/backend . The task is run periodically (as defined by cronjobSchedule ). iter8 k launch \\ --set \"tasks={abnmetrics}\" \\ --set abnmetrics.application=default/backend \\ --set runner=cronjob \\ --set cronjobSchedule=\"*/1 * * * *\"","title":"Usage example"},{"location":"user-guide/tasks/abnmetrics/#parameters","text":"Name Type Description endpoint string Endpoint of the A/B/n service. Defaults to iter8-abn:50051 application string Application name in form namespace/name","title":"Parameters"},{"location":"user-guide/tasks/assess/","text":"assess \u00b6 Assess if service-level objectives (SLOs) are satisfied by app versions. Usage example \u00b6 In this experiment, the assess task validates if the http/latency-mean metric has a value that does not exceed 50, and the http/error-count metric has a value that does not exceed 0. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.url=https://httpbin.org/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set runner=job Parameters \u00b6 Name Type Description SLOs struct Service-level objectives that will be validated by this task. This struct contains two fields upper and lower . upper \u00b6 Name Type Description upper map[string]float Map keys are fully-qualified metric names and map values are upper limits of those metrics. lower \u00b6 Name Type Description lower map[string]float Map keys are fully-qualified metric names and map values are lower limits of those metrics.","title":"assess"},{"location":"user-guide/tasks/assess/#assess","text":"Assess if service-level objectives (SLOs) are satisfied by app versions.","title":"assess"},{"location":"user-guide/tasks/assess/#usage-example","text":"In this experiment, the assess task validates if the http/latency-mean metric has a value that does not exceed 50, and the http/error-count metric has a value that does not exceed 0. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.url=https://httpbin.org/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set runner=job","title":"Usage example"},{"location":"user-guide/tasks/assess/#parameters","text":"Name Type Description SLOs struct Service-level objectives that will be validated by this task. This struct contains two fields upper and lower .","title":"Parameters"},{"location":"user-guide/tasks/assess/#upper","text":"Name Type Description upper map[string]float Map keys are fully-qualified metric names and map values are upper limits of those metrics.","title":"upper"},{"location":"user-guide/tasks/assess/#lower","text":"Name Type Description lower map[string]float Map keys are fully-qualified metric names and map values are lower limits of those metrics.","title":"lower"},{"location":"user-guide/tasks/custommetrics/","text":"custommetrics \u00b6 Fetch metrics from databases (like Prometheus) and other REST APIs. Usage Example \u00b6 In this example, the custommetrics task fetches metrics from the Prometheus database that is created by Istio's Prometheus add-on . iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set custommetrics.values.labels.namespace = default \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" Parameters \u00b6 Name Type Description templates map[string]string A map where each key is the name of a provider , and the corresponding value is a URL containing the provider template . values map[string]interface{} A map that contains the values for variables in provider templates . When there are two or more app versions, this map contains values that are common to all versions. versionValues []map[string]interface{} An array that contains version-specific values for variables in provider templates . While fetching metrics for version i , the task merges values with versionValues[i] (latter takes precedence), and the merged map contains the values for variables in provider templates. How it works \u00b6 The logic of this task is illustrated by the following flowchart. graph TD A([Start]) --> B([Get provider template]); B --> C([Compute variable values]); C --> D([Create provider spec by combining template with values]); D --> E([Query database]); E --> F([Process response]); F --> G([Update metric value in experiment]); G --> H{Done with all metrics?}; H ---->|No| E; H ---->|Yes| I{Done with all versions?}; I ---->|No| C; I ---->|Yes| J([End]); We describe the concepts or provider spec and provider template next. Provider spec \u00b6 Iter8 needs the information following in order to fetch metrics from a database. The HTTP URL where the database can be queried. The HTTP headers and method (GET/POST) to be used while querying the database. For each metric to be fetched from the database: The specific HTTP query to be used, in particular, the HTTP query parameters and body (if any). The logic for parsing the query response and retrieving the metric value. The above information is encapsulated by ProviderSpec , a data structure which Iter8 associates with each provider, and Metric , a data structure which Iter8 associates with each metric provided by a provider. Golang type definitions for ProviderSpec and Metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type ProviderSpec struct { // URL is the database endpoint URL string `json:\"url\" yaml:\"url\"` // Method is the HTTP method that needs to be used Method string `json:\"method\" yaml:\"method\"` // Headers is the set of HTTP headers that need to be sent Headers map [ string ] string `json:\"headers\" yaml:\"headers\"` // Metrics is the set of metrics that can be obtained Metrics [] Metric `json:\"metrics\" yaml:\"metrics\"` } type Metric struct { // Name is the name of the metric Name string `json:\"name\" yaml:\"name\"` // Description is the description of the metric Description * string `json:\"description,omitempty\" yaml:\"description,omitempty\"` // Type is the type of the metric, either gauge or counter Type string `json:\"type\" yaml:\"type\"` // Units is the unit of the metric, which can be omitted for unitless metrics Units * string `json:\"units,omitempty\" yaml:\"units,omitempty\"` // Params is the set of HTTP parameters that need to be sent Params * [] HTTPParam `json:\"params,omitempty\" yaml:\"params,omitempty\"` // Body is the HTTP request body that needs to be sent Body * string `json:\"body,omitempty\" yaml:\"body,omitempty\"` // JqExpression is the jq expression that can extract the value from the HTTP // response JqExpression string `json:\"jqExpression\" yaml:\"jqExpression\"` } type HTTPParam struct { // Name is the name of the HTTP parameter Name string `json:\"name\" yaml:\"name\"` // Value is the value of the HTTP parameter Value string `json:\"value\" yaml:\"value\"` } The ProviderSpec and Metric data structures together supply Iter8 with all the information needed to query databases, process the response to extract metric values, store the metric values in experiments, and display them in experiment reports with auxiliary information (such as description and units). Metric types are defined here . Provider template \u00b6 Rather than supplying provider specs directly, Iter8 enables users to supply one or more Golang templates for provider specs. Iter8 combines the provider templates with values , in order to generate provider specs in YAML format, and uses them to query for the metrics. Example providers specs: * istio-prom for Istio's Prometheus plugin In order to create provider templates and use them in experiments, it is necessary to have a clear understanding of how variable values are computed, and how the response from the database is processed by Iter8. We describe these steps next. Computing variable values \u00b6 Variable values are configured explicitly by the user during experiment launch. The sole exception to this rule is the elapsedTimeSeconds variable which is computed by Iter8. Please see the tabs below to learn more about how to configure values and how Iter8 computes elapsedTimeSeconds . Configure values for one version Configure values for multiple versions How Iter8 computes elapsedTimeSeconds When the experiment involves a single version of the app, template variable values are supplied directly as part of the custommetrics.values map. See usage example for an illustration. When the experiment involves two or more versions of the app, values that are shared by all versions are supplied as part of the custommetrics.values map, and values that are specific to versions are supplied as part of the custommetrics.versionValues list. The length of this list is the number of versions, and custommetrics.versionValues[i] is the map that holds values specific to version i . Iter8 merges custommetrics.values with custommetrics.versionValues[i] (latter takes precedence), and uses the resulting map for version i when substituting template variables. Configuring values for two versions is illustrated in the following usage example. iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.namespace = default \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set custommetrics.values.labels.reporter = destination \\ --set 'custommetrics.versionValues[0].labels.destination_version=v1' \\ --set 'custommetrics.versionValues[1].labels.destination_version=v2' \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" A metric query often involves specifying the time window over which the metric need to be computed. In provider templates , a special template variable named elapsedTimeSeconds holds the length of this time window. Its use within a template is illustrated in the following snippets. query query template sum ( last_over_time ( istio_requests_total { destination_app = \"httpbin\" , namespace = \"default\" }[ 3600 s ])) The metric is computed over the recent one-hour time window (that ends at the current time). sum ( last_over_time ( istio_requests_total { destination_app = \"httpbin\" , namespace = \"default\" }[{{ . elapsedTimeSeconds }} s ])) The metric is computed over a recent time window (that ends at the current time). The length of this window is determined by the value of the template variable elapsedTimeSeconds . Iter8 computes the value of the elapsedTimeSeconds variable dynamically in this task. This is the desirable behavior in multi-loop experiments (see usage example ), where metrics need to be fetched periodically, and the time window over which metrics are computed stretches farther back with each loop. The following sequence diagram illustrates how elapsedTimeSeconds changes over loops. sequenceDiagram startingTime-)loop1: elapsedTimeSeconds=60; startingTime-)loop2: elapsedTimeSeconds=120; startingTime-)loop3: elapsedTimeSeconds=180; Iter8 computes elapsedTimeSeconds based on another variable named startingTime . The default value of startingTime is the time at which the experiment is launched. The user can override the default by explicitly configuring startingTime during experiment launch, in the RFC 3339 format (for example, 2020-02-01T09:44:40Z or 2020-02-01T09:44:40.954641934Z ). Iter8 sets elapsedTimeSeconds as the difference (in seconds) between the current time and startingTime . This logic is illustrated in the following flowchart. graph TD A([Start]) --> B{startingTime parameter supplied?}; B ---->|Yes| C([elapsedTimeSeconds = currentTime - startingTime]); B ---->|No| D([startingTime = time when experiment was launched]); D --> C; C --> E([End]); Note that the above design enables the user to supply different startingTime values for different app versions (for instance, based on the creation timestamps of the versions). Single startingTime value Two versions with different startingTime values --set custommetrics.values.startingTime = \"2020-02-01T09:44:40Z\" --set custommetrics.versionValues [ 0 ] .startingTime = \"2020-02-01T09:44:40Z\" \\ --set custommetrics.versionValues [ 1 ] .startingTime = \"2020-02-05T14:22:15Z\" Processing response \u00b6 The metrics provider is expected to respond to Iter8's HTTP request for a metric with a JSON object. The format of this JSON object is provider-specific. Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus response example Prometheus jqExpression example The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression . Defining and using providers \u00b6 Understand how the custommetrics task works; this is described in this section . Create your provider template and serve it from a URL. A sample provider template is in this section . Configure the custommetrics task with one or more provider templates. An example of custommetrics configuration is in this section . The metrics fetched by this task can be used to assess app versions in Iter8 experiments. An example that illustrates the use of both custommetrics and assess tasks together is in this section .","title":"custommetrics"},{"location":"user-guide/tasks/custommetrics/#custommetrics","text":"Fetch metrics from databases (like Prometheus) and other REST APIs.","title":"custommetrics"},{"location":"user-guide/tasks/custommetrics/#usage-example","text":"In this example, the custommetrics task fetches metrics from the Prometheus database that is created by Istio's Prometheus add-on . iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set custommetrics.values.labels.namespace = default \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\"","title":"Usage Example"},{"location":"user-guide/tasks/custommetrics/#parameters","text":"Name Type Description templates map[string]string A map where each key is the name of a provider , and the corresponding value is a URL containing the provider template . values map[string]interface{} A map that contains the values for variables in provider templates . When there are two or more app versions, this map contains values that are common to all versions. versionValues []map[string]interface{} An array that contains version-specific values for variables in provider templates . While fetching metrics for version i , the task merges values with versionValues[i] (latter takes precedence), and the merged map contains the values for variables in provider templates.","title":"Parameters"},{"location":"user-guide/tasks/custommetrics/#how-it-works","text":"The logic of this task is illustrated by the following flowchart. graph TD A([Start]) --> B([Get provider template]); B --> C([Compute variable values]); C --> D([Create provider spec by combining template with values]); D --> E([Query database]); E --> F([Process response]); F --> G([Update metric value in experiment]); G --> H{Done with all metrics?}; H ---->|No| E; H ---->|Yes| I{Done with all versions?}; I ---->|No| C; I ---->|Yes| J([End]); We describe the concepts or provider spec and provider template next.","title":"How it works"},{"location":"user-guide/tasks/custommetrics/#provider-spec","text":"Iter8 needs the information following in order to fetch metrics from a database. The HTTP URL where the database can be queried. The HTTP headers and method (GET/POST) to be used while querying the database. For each metric to be fetched from the database: The specific HTTP query to be used, in particular, the HTTP query parameters and body (if any). The logic for parsing the query response and retrieving the metric value. The above information is encapsulated by ProviderSpec , a data structure which Iter8 associates with each provider, and Metric , a data structure which Iter8 associates with each metric provided by a provider. Golang type definitions for ProviderSpec and Metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type ProviderSpec struct { // URL is the database endpoint URL string `json:\"url\" yaml:\"url\"` // Method is the HTTP method that needs to be used Method string `json:\"method\" yaml:\"method\"` // Headers is the set of HTTP headers that need to be sent Headers map [ string ] string `json:\"headers\" yaml:\"headers\"` // Metrics is the set of metrics that can be obtained Metrics [] Metric `json:\"metrics\" yaml:\"metrics\"` } type Metric struct { // Name is the name of the metric Name string `json:\"name\" yaml:\"name\"` // Description is the description of the metric Description * string `json:\"description,omitempty\" yaml:\"description,omitempty\"` // Type is the type of the metric, either gauge or counter Type string `json:\"type\" yaml:\"type\"` // Units is the unit of the metric, which can be omitted for unitless metrics Units * string `json:\"units,omitempty\" yaml:\"units,omitempty\"` // Params is the set of HTTP parameters that need to be sent Params * [] HTTPParam `json:\"params,omitempty\" yaml:\"params,omitempty\"` // Body is the HTTP request body that needs to be sent Body * string `json:\"body,omitempty\" yaml:\"body,omitempty\"` // JqExpression is the jq expression that can extract the value from the HTTP // response JqExpression string `json:\"jqExpression\" yaml:\"jqExpression\"` } type HTTPParam struct { // Name is the name of the HTTP parameter Name string `json:\"name\" yaml:\"name\"` // Value is the value of the HTTP parameter Value string `json:\"value\" yaml:\"value\"` } The ProviderSpec and Metric data structures together supply Iter8 with all the information needed to query databases, process the response to extract metric values, store the metric values in experiments, and display them in experiment reports with auxiliary information (such as description and units). Metric types are defined here .","title":"Provider spec"},{"location":"user-guide/tasks/custommetrics/#provider-template","text":"Rather than supplying provider specs directly, Iter8 enables users to supply one or more Golang templates for provider specs. Iter8 combines the provider templates with values , in order to generate provider specs in YAML format, and uses them to query for the metrics. Example providers specs: * istio-prom for Istio's Prometheus plugin In order to create provider templates and use them in experiments, it is necessary to have a clear understanding of how variable values are computed, and how the response from the database is processed by Iter8. We describe these steps next.","title":"Provider template"},{"location":"user-guide/tasks/custommetrics/#computing-variable-values","text":"Variable values are configured explicitly by the user during experiment launch. The sole exception to this rule is the elapsedTimeSeconds variable which is computed by Iter8. Please see the tabs below to learn more about how to configure values and how Iter8 computes elapsedTimeSeconds . Configure values for one version Configure values for multiple versions How Iter8 computes elapsedTimeSeconds When the experiment involves a single version of the app, template variable values are supplied directly as part of the custommetrics.values map. See usage example for an illustration. When the experiment involves two or more versions of the app, values that are shared by all versions are supplied as part of the custommetrics.values map, and values that are specific to versions are supplied as part of the custommetrics.versionValues list. The length of this list is the number of versions, and custommetrics.versionValues[i] is the map that holds values specific to version i . Iter8 merges custommetrics.values with custommetrics.versionValues[i] (latter takes precedence), and uses the resulting map for version i when substituting template variables. Configuring values for two versions is illustrated in the following usage example. iter8 k launch \\ --set \"tasks={custommetrics,assess}\" \\ --set custommetrics.templates.istio-prom = \"https://raw.githubusercontent.com/iter8-tools/hub/main/templates/custommetrics/istio-prom.tpl\" \\ --set custommetrics.values.labels.namespace = default \\ --set custommetrics.values.labels.destination_app = httpbin \\ --set custommetrics.values.labels.reporter = destination \\ --set 'custommetrics.versionValues[0].labels.destination_version=v1' \\ --set 'custommetrics.versionValues[1].labels.destination_version=v2' \\ --set assess.SLOs.upper.istio-prom/error-rate = 0 \\ --set assess.SLOs.upper.istio-prom/latency-mean = 100 \\ --set runner = cronjob \\ --set cronjobSchedule = \"*/1 * * * *\" A metric query often involves specifying the time window over which the metric need to be computed. In provider templates , a special template variable named elapsedTimeSeconds holds the length of this time window. Its use within a template is illustrated in the following snippets. query query template sum ( last_over_time ( istio_requests_total { destination_app = \"httpbin\" , namespace = \"default\" }[ 3600 s ])) The metric is computed over the recent one-hour time window (that ends at the current time). sum ( last_over_time ( istio_requests_total { destination_app = \"httpbin\" , namespace = \"default\" }[{{ . elapsedTimeSeconds }} s ])) The metric is computed over a recent time window (that ends at the current time). The length of this window is determined by the value of the template variable elapsedTimeSeconds . Iter8 computes the value of the elapsedTimeSeconds variable dynamically in this task. This is the desirable behavior in multi-loop experiments (see usage example ), where metrics need to be fetched periodically, and the time window over which metrics are computed stretches farther back with each loop. The following sequence diagram illustrates how elapsedTimeSeconds changes over loops. sequenceDiagram startingTime-)loop1: elapsedTimeSeconds=60; startingTime-)loop2: elapsedTimeSeconds=120; startingTime-)loop3: elapsedTimeSeconds=180; Iter8 computes elapsedTimeSeconds based on another variable named startingTime . The default value of startingTime is the time at which the experiment is launched. The user can override the default by explicitly configuring startingTime during experiment launch, in the RFC 3339 format (for example, 2020-02-01T09:44:40Z or 2020-02-01T09:44:40.954641934Z ). Iter8 sets elapsedTimeSeconds as the difference (in seconds) between the current time and startingTime . This logic is illustrated in the following flowchart. graph TD A([Start]) --> B{startingTime parameter supplied?}; B ---->|Yes| C([elapsedTimeSeconds = currentTime - startingTime]); B ---->|No| D([startingTime = time when experiment was launched]); D --> C; C --> E([End]); Note that the above design enables the user to supply different startingTime values for different app versions (for instance, based on the creation timestamps of the versions). Single startingTime value Two versions with different startingTime values --set custommetrics.values.startingTime = \"2020-02-01T09:44:40Z\" --set custommetrics.versionValues [ 0 ] .startingTime = \"2020-02-01T09:44:40Z\" \\ --set custommetrics.versionValues [ 1 ] .startingTime = \"2020-02-05T14:22:15Z\"","title":"Computing variable values"},{"location":"user-guide/tasks/custommetrics/#processing-response","text":"The metrics provider is expected to respond to Iter8's HTTP request for a metric with a JSON object. The format of this JSON object is provider-specific. Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus response example Prometheus jqExpression example The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression .","title":"Processing response"},{"location":"user-guide/tasks/custommetrics/#defining-and-using-providers","text":"Understand how the custommetrics task works; this is described in this section . Create your provider template and serve it from a URL. A sample provider template is in this section . Configure the custommetrics task with one or more provider templates. An example of custommetrics configuration is in this section . The metrics fetched by this task can be used to assess app versions in Iter8 experiments. An example that illustrates the use of both custommetrics and assess tasks together is in this section .","title":"Defining and using providers"},{"location":"user-guide/tasks/github/","text":"github \u00b6 Trigger GitHub workflows via a repository_dispatch . A repository_dispatch will trigger workflows in the default branch of the GitHub repository. By default, the experiment report will also be sent. Usage Example \u00b6 iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> \\ --set runner = job See here for a more in-depth tutorial. Parameters \u00b6 Name Type Required Default value Description owner string Yes N/A Owner of the GitHub repository repo string Yes N/A GitHub repository token string Yes N/A Authorization token payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/hub/iter8-0.13.0/templates/notify/_payload-github.tpl URL to a payload template softFailure bool No true Indicates the task and experiment should not fail if the task cannot successfully send the request if string No N/A An if condition that can be control when the task is run in a multi-looped experiment . To learn more, see here . Default payload \u00b6 A repository_dispatch requires a payload that contains the type of the event. The default payload template will set the event_type to iter8 . In addition, it will also provide the experiment report in the client_payload , which means that this data will be accessible in the GitHub workflow via ${{ toJson(github.event.client_payload) }} . However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default. if parameter \u00b6 The if parameter is used to control when the task is run in a multi-looped experiment . For example, if you would like for the github task to only run at the 10 th loop instead of every loop, you can do the following: iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set github.owner=<GitHub owner> \\ --set github.repo=<GitHub repository> \\ --set github.token=<GitHub token> \\ --set github.if=\"Result.NumLoops == 10\" --set runner=job You may use any field in the Result object for your logic.","title":"github"},{"location":"user-guide/tasks/github/#github","text":"Trigger GitHub workflows via a repository_dispatch . A repository_dispatch will trigger workflows in the default branch of the GitHub repository. By default, the experiment report will also be sent.","title":"github"},{"location":"user-guide/tasks/github/#usage-example","text":"iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> \\ --set runner = job See here for a more in-depth tutorial.","title":"Usage Example"},{"location":"user-guide/tasks/github/#parameters","text":"Name Type Required Default value Description owner string Yes N/A Owner of the GitHub repository repo string Yes N/A GitHub repository token string Yes N/A Authorization token payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/hub/iter8-0.13.0/templates/notify/_payload-github.tpl URL to a payload template softFailure bool No true Indicates the task and experiment should not fail if the task cannot successfully send the request if string No N/A An if condition that can be control when the task is run in a multi-looped experiment . To learn more, see here .","title":"Parameters"},{"location":"user-guide/tasks/github/#default-payload","text":"A repository_dispatch requires a payload that contains the type of the event. The default payload template will set the event_type to iter8 . In addition, it will also provide the experiment report in the client_payload , which means that this data will be accessible in the GitHub workflow via ${{ toJson(github.event.client_payload) }} . However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default.","title":"Default payload"},{"location":"user-guide/tasks/github/#if-parameter","text":"The if parameter is used to control when the task is run in a multi-looped experiment . For example, if you would like for the github task to only run at the 10 th loop instead of every loop, you can do the following: iter8 k launch \\ --set \"tasks={http,assess,github}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set github.owner=<GitHub owner> \\ --set github.repo=<GitHub repository> \\ --set github.token=<GitHub token> \\ --set github.if=\"Result.NumLoops == 10\" --set runner=job You may use any field in the Result object for your logic.","title":"if parameter"},{"location":"user-guide/tasks/grpc/","text":"grpc \u00b6 Generate requests for a gRPC service and and collect latency and error-related metrics . Usage example \u00b6 In this experiment, the grpc task generates call requests for a gRPC service hosted at hello.default:50051 , defined in the protobuf file located at grpc.protoURL , with a gRPC method named helloworld.Greeter.SayHello . Metrics collected by this task are used by the assess task to validate SLOs. Single method: iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set runner = job Multiple methods: iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set grpc.endpoints.routeChat.call = routeguide.RouteGuide.RouteChat \\ --set grpc.endpoints.routeChat.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/bidirectional.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set assess.SLOs.upper.grpc-routeChat/error-rate = 0 \\ --set runner = job Parameters \u00b6 Any field in the Config struct of the ghz runner package can be used as a parameter in this task. The JSON tags of the struct fields directly correspond to the names of the parameters of this task. In the usage example , the parameters host and call correspond to the Host and Call fields respectively in the Config struct. In addition, the following fields are defined by this task. Name Type Description protoURL string (URL) URL where the protobuf file that defines the gRPC service is located. dataURL string (URL) URL where JSON data to be used in call requests is located. binaryDataURL string (URL) URL where binary data to be used in call requests is located. metadataURL string (URL) URL where the JSON metadata data to be used in call requests is located. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above as well as those from the Config struct. Load testing and metric collection will be conducted separately for each endpoint. Precedence \u00b6 Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default timeout of 20s (from Config struct). iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In the following example, the getFeature and listFeatures endpoints will use the default timeout of 20s and the recordRoute endpoint will use a timeout of 30s . iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set grpc.endpoints.recordRoute.timeout = 30s \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In the following example, all three endpoints will use a qps of 40s . iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In the following example, the getFeature and listFeatures endpoints will use a timeout of 40s and the listFeatures endpoint will use a timeout of 30s . iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set grpc.endpoints.recordRoute.timeout = 30s \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job Further more, set parameters will trickle down to the endpoints. iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.skipFirst = 5 \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In this example, all three endpoints will have a skipFirst of 5. Metrics \u00b6 This task creates a built-in provider named grpc . The following metrics are collected by this task: grpc/request-count : total number of requests sent grpc/error-count : number of error responses grpc/error-rate : fraction of error responses The following latency metrics are also supported by this task. grpc/latency/mean : mean latency grpc/latency/stddev : standard deviation of latency grpc/latency/min : min latency grpc/latency/max : max latency grpc/latency/pX : X th percentile latency, for any X in the range 0.0 to 100.0 All latency metrics have msec units. In the case of multiple endpoints, the name of the endpoint will be appended to the name of the provider. For example, if the endpoint name is routeguide , then the following metrics would be collected by this task: grpc-routeguide/request-count : total number of requests sent grpc-routeguide/error-count : number of error responses grpc-routeguide/error-rate : fraction of error responses grpc-routeguide/latency/mean : mean latency grpc-routeguide/latency/stddev : standard deviation of latency grpc-routeguide/latency/min : min latency grpc-routeguide/latency/max : max latency grpc-routeguide/latency/pX : X th percentile latency, for any X in the range 0.0 to 100.0 To learn more about the names of metrics, please see here .","title":"grpc"},{"location":"user-guide/tasks/grpc/#grpc","text":"Generate requests for a gRPC service and and collect latency and error-related metrics .","title":"grpc"},{"location":"user-guide/tasks/grpc/#usage-example","text":"In this experiment, the grpc task generates call requests for a gRPC service hosted at hello.default:50051 , defined in the protobuf file located at grpc.protoURL , with a gRPC method named helloworld.Greeter.SayHello . Metrics collected by this task are used by the assess task to validate SLOs. Single method: iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set assess.SLOs.upper.grpc/error-rate = 0 \\ --set assess.SLOs.upper.grpc/latency/mean = 200 \\ --set runner = job Multiple methods: iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set grpc.endpoints.routeChat.call = routeguide.RouteGuide.RouteChat \\ --set grpc.endpoints.routeChat.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/bidirectional.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set assess.SLOs.upper.grpc-routeChat/error-rate = 0 \\ --set runner = job","title":"Usage example"},{"location":"user-guide/tasks/grpc/#parameters","text":"Any field in the Config struct of the ghz runner package can be used as a parameter in this task. The JSON tags of the struct fields directly correspond to the names of the parameters of this task. In the usage example , the parameters host and call correspond to the Host and Call fields respectively in the Config struct. In addition, the following fields are defined by this task. Name Type Description protoURL string (URL) URL where the protobuf file that defines the gRPC service is located. dataURL string (URL) URL where JSON data to be used in call requests is located. binaryDataURL string (URL) URL where binary data to be used in call requests is located. metadataURL string (URL) URL where the JSON metadata data to be used in call requests is located. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above as well as those from the Config struct. Load testing and metric collection will be conducted separately for each endpoint.","title":"Parameters"},{"location":"user-guide/tasks/grpc/#precedence","text":"Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default timeout of 20s (from Config struct). iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In the following example, the getFeature and listFeatures endpoints will use the default timeout of 20s and the recordRoute endpoint will use a timeout of 30s . iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set grpc.endpoints.recordRoute.timeout = 30s \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In the following example, all three endpoints will use a qps of 40s . iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In the following example, the getFeature and listFeatures endpoints will use a timeout of 40s and the listFeatures endpoint will use a timeout of 30s . iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set grpc.endpoints.recordRoute.timeout = 30s \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job Further more, set parameters will trickle down to the endpoints. iter8 k launch \\ --set \"tasks={grpc,assess}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.skipFirst = 5 \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.13.13/samples/grpc-payload/client.json \\ --set assess.SLOs.upper.grpc-getFeature/error-rate = 0 \\ --set assess.SLOs.upper.grpc-listFeatures/error-rate = 0 \\ --set assess.SLOs.upper.grpc-recordRoute/error-rate = 0 \\ --set runner = job In this example, all three endpoints will have a skipFirst of 5.","title":"Precedence"},{"location":"user-guide/tasks/grpc/#metrics","text":"This task creates a built-in provider named grpc . The following metrics are collected by this task: grpc/request-count : total number of requests sent grpc/error-count : number of error responses grpc/error-rate : fraction of error responses The following latency metrics are also supported by this task. grpc/latency/mean : mean latency grpc/latency/stddev : standard deviation of latency grpc/latency/min : min latency grpc/latency/max : max latency grpc/latency/pX : X th percentile latency, for any X in the range 0.0 to 100.0 All latency metrics have msec units. In the case of multiple endpoints, the name of the endpoint will be appended to the name of the provider. For example, if the endpoint name is routeguide , then the following metrics would be collected by this task: grpc-routeguide/request-count : total number of requests sent grpc-routeguide/error-count : number of error responses grpc-routeguide/error-rate : fraction of error responses grpc-routeguide/latency/mean : mean latency grpc-routeguide/latency/stddev : standard deviation of latency grpc-routeguide/latency/min : min latency grpc-routeguide/latency/max : max latency grpc-routeguide/latency/pX : X th percentile latency, for any X in the range 0.0 to 100.0 To learn more about the names of metrics, please see here .","title":"Metrics"},{"location":"user-guide/tasks/http/","text":"http \u00b6 Generate requests for an HTTP service and and collect latency and error-related metrics . Usage example \u00b6 In this experiment, the http task generates requests for https://httpbin.org/get , and collects latency and error-related metrics. The metrics are used by the assess task to validate SLOs. Single endpoint: iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.url = https://httpbin.org/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job Multiple endpoints: iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job Parameters \u00b6 Name Type Description url string (URL) URL where requests are sent. headers map[string]string HTTP headers to use in the requests. numRequests int Number of requests to be sent to the app. Default value is 100. duration string Duration of this task. Specified in the Go duration string format (example, 5s ). If both duration and numRequests are specified, then duration is ignored. qps float qps stands for queries-per-second. Number of requests per second sent to the app. Default value is 8.0. connections int Number of parallel connections used to send requests. Default value is 4. payloadURL string (URL) URL from which to download the content that will be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this content as the payload. payloadStr string String data to be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this string as the payload. contentType string Content type of the payload. This is intended to be used in conjunction with one of the payload* fields. If this field is specified, Iter8 will send HTTP POST requests to the app using this as the Content-Type header value. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above. Load testing and metric collection will be conducted separately for each endpoint. Precedence \u00b6 Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default qps (queries-per-second) of 8. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In the following example, the get and getAnything endpoints will use the default qps of 8 and the post endpoint will use a qps of 15. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In the following example, all three endpoints will use a qps (queries-per-second) of 10. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In the following example, the get and getAnything endpoints will use a qps of 10 and the post endpoint will use a qps of 15. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job Further more, set parameters will trickle down to the endpoints. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.numRequests = 50 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In this example, all three endpoints will have a numRequests of 50. Metrics \u00b6 This task creates a built-in provider named http . The following metrics are collected by this task: http/request-count : total number of requests sent http/error-count : number of error responses http/error-rate : fraction of error responses http/latency-mean : mean of observed latency values http/latency-stddev : standard deviation of observed latency values http/latency-min : min of observed latency values http/latency-max : max of observed latency values http/latency-pX : X th percentile latency, for X in [50.0, 75.0, 90.0, 95.0, 99.0, 99.9] All latency metrics have msec units. In the case of multiple endpoints, the name of the endpoint will be appended to the name of the provider. For example, if the endpoint name is httpbin , then the following metrics would be collected by this task: http-httpbin/request-count : total number of requests sent http-httpbin/error-count : number of error responses http-httpbin/error-rate : fraction of error responses http-httpbin/latency-mean : mean of observed latency values http-httpbin/latency-stddev : standard deviation of observed latency values http-httpbin/latency-min : min of observed latency values http-httpbin/latency-max : max of observed latency values http-httpbin/latency-pX : X th percentile latency, for X in [50.0, 75.0, 90.0, 95.0, 99.0, 99.9] To learn more about the names of metrics, please see here .","title":"http"},{"location":"user-guide/tasks/http/#http","text":"Generate requests for an HTTP service and and collect latency and error-related metrics .","title":"http"},{"location":"user-guide/tasks/http/#usage-example","text":"In this experiment, the http task generates requests for https://httpbin.org/get , and collects latency and error-related metrics. The metrics are used by the assess task to validate SLOs. Single endpoint: iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.url = https://httpbin.org/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set runner = job Multiple endpoints: iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job","title":"Usage example"},{"location":"user-guide/tasks/http/#parameters","text":"Name Type Description url string (URL) URL where requests are sent. headers map[string]string HTTP headers to use in the requests. numRequests int Number of requests to be sent to the app. Default value is 100. duration string Duration of this task. Specified in the Go duration string format (example, 5s ). If both duration and numRequests are specified, then duration is ignored. qps float qps stands for queries-per-second. Number of requests per second sent to the app. Default value is 8.0. connections int Number of parallel connections used to send requests. Default value is 4. payloadURL string (URL) URL from which to download the content that will be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this content as the payload. payloadStr string String data to be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this string as the payload. contentType string Content type of the payload. This is intended to be used in conjunction with one of the payload* fields. If this field is specified, Iter8 will send HTTP POST requests to the app using this as the Content-Type header value. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above. Load testing and metric collection will be conducted separately for each endpoint.","title":"Parameters"},{"location":"user-guide/tasks/http/#precedence","text":"Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default qps (queries-per-second) of 8. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In the following example, the get and getAnything endpoints will use the default qps of 8 and the post endpoint will use a qps of 15. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In the following example, all three endpoints will use a qps (queries-per-second) of 10. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In the following example, the get and getAnything endpoints will use a qps of 10 and the post endpoint will use a qps of 15. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job Further more, set parameters will trickle down to the endpoints. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.numRequests = 50 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set assess.SLOs.upper.http-get/error-count = 0 \\ --set assess.SLOs.upper.http-getAnything/error-count = 0 \\ --set assess.SLOs.upper.http-post/error-count = 0 \\ --set runner = job In this example, all three endpoints will have a numRequests of 50.","title":"Precedence"},{"location":"user-guide/tasks/http/#metrics","text":"This task creates a built-in provider named http . The following metrics are collected by this task: http/request-count : total number of requests sent http/error-count : number of error responses http/error-rate : fraction of error responses http/latency-mean : mean of observed latency values http/latency-stddev : standard deviation of observed latency values http/latency-min : min of observed latency values http/latency-max : max of observed latency values http/latency-pX : X th percentile latency, for X in [50.0, 75.0, 90.0, 95.0, 99.0, 99.9] All latency metrics have msec units. In the case of multiple endpoints, the name of the endpoint will be appended to the name of the provider. For example, if the endpoint name is httpbin , then the following metrics would be collected by this task: http-httpbin/request-count : total number of requests sent http-httpbin/error-count : number of error responses http-httpbin/error-rate : fraction of error responses http-httpbin/latency-mean : mean of observed latency values http-httpbin/latency-stddev : standard deviation of observed latency values http-httpbin/latency-min : min of observed latency values http-httpbin/latency-max : max of observed latency values http-httpbin/latency-pX : X th percentile latency, for X in [50.0, 75.0, 90.0, 95.0, 99.0, 99.9] To learn more about the names of metrics, please see here .","title":"Metrics"},{"location":"user-guide/tasks/ready/","text":"ready \u00b6 Check if a Kubernetes object exists and is ready. Usage example \u00b6 In the following example, the ready task checks if a deployment named httpbin-prod exists and its availability condition is set to true, and a service named httpbin exists. iter8 k launch \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin-prod \\ --set ready.service = httpbin \\ --set http.url = http://httpbin.default/get \\ --set runner = job Parameters \u00b6 Name Type Description deploy string Name of a Kubernetes deployment. The task checks if the deployment exists and its Available condition is set to true. service string Name of a Kubernetes service. The task checks if the service exists. ksvc string Name of a Knative service. The task checks if the service exists and its Ready condition is set to true. chaosengine string Name of a LitmusChaos ChaosEngine resource object. The task checks if the object exists. timeout string Timeout for readiness check to succeed. Default value is 60s . namespace string The namespace under which to look for the Kubernetes objects. For experiments that run inside a Kubernetes cluster, the default value of this field is the namespace of the Iter8 experiment ; for experiments that run in the local environment, it is the default namespace. Extensions \u00b6 Iter8 can be easily extended to support readiness checks for any type of Kubernetes object (including objects with custom resource types). Please consider submitting a pull request for such extensions. Readiness checking in Iter8 involves two templates, namely, task.ready and k.role . Extending the readiness checks to new resource types involves modifying these templates. Example \u00b6 Consider the Knative extension for this task; this extension enables Iter8 experiment authors to define readiness check for Knative services . In the following example, the ready task succeed if the Knative service named httpbin exists, and has its Ready condition set to true. iter8 k launch \\ --set \"tasks={ready,http}\" \\ --set ready.ksvc = httpbin \\ --set http.url = http://httpbin.default/get \\ --set runner = job The task.ready and k.role were changed in the following ways to create this extension. task.ready k.role The group/version/resource (GVR) and the condition that should be checked for a Knative Service are defined in this template. 1 2 3 4 5 6 7 8 9 10 11 {{ - if .Values.ready.ksvc }} # task: determine if Knative Service exists and is ready - task : ready with : name : {{ .Values.ready.ksvc | quote }} group : serving.knative.dev version : v1 resource : services condition : Ready {{ - include \"task.ready.tn\" . }} {{ - end }} The role named {{ .Release.Name }}-ready is extended with the Knative apiGroup . 1 2 3 4 5 6 {{ - if .Values.ready.ksvc }} - apiGroups : [ \"serving.knative.dev\" ] resourceNames : [{{ .Values.ready.ksvc | quote }}] resources : [ \"services\" ] verbs : [ \"get\" ] {{ - end }}","title":"ready"},{"location":"user-guide/tasks/ready/#ready","text":"Check if a Kubernetes object exists and is ready.","title":"ready"},{"location":"user-guide/tasks/ready/#usage-example","text":"In the following example, the ready task checks if a deployment named httpbin-prod exists and its availability condition is set to true, and a service named httpbin exists. iter8 k launch \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin-prod \\ --set ready.service = httpbin \\ --set http.url = http://httpbin.default/get \\ --set runner = job","title":"Usage example"},{"location":"user-guide/tasks/ready/#parameters","text":"Name Type Description deploy string Name of a Kubernetes deployment. The task checks if the deployment exists and its Available condition is set to true. service string Name of a Kubernetes service. The task checks if the service exists. ksvc string Name of a Knative service. The task checks if the service exists and its Ready condition is set to true. chaosengine string Name of a LitmusChaos ChaosEngine resource object. The task checks if the object exists. timeout string Timeout for readiness check to succeed. Default value is 60s . namespace string The namespace under which to look for the Kubernetes objects. For experiments that run inside a Kubernetes cluster, the default value of this field is the namespace of the Iter8 experiment ; for experiments that run in the local environment, it is the default namespace.","title":"Parameters"},{"location":"user-guide/tasks/ready/#extensions","text":"Iter8 can be easily extended to support readiness checks for any type of Kubernetes object (including objects with custom resource types). Please consider submitting a pull request for such extensions. Readiness checking in Iter8 involves two templates, namely, task.ready and k.role . Extending the readiness checks to new resource types involves modifying these templates.","title":"Extensions"},{"location":"user-guide/tasks/ready/#example","text":"Consider the Knative extension for this task; this extension enables Iter8 experiment authors to define readiness check for Knative services . In the following example, the ready task succeed if the Knative service named httpbin exists, and has its Ready condition set to true. iter8 k launch \\ --set \"tasks={ready,http}\" \\ --set ready.ksvc = httpbin \\ --set http.url = http://httpbin.default/get \\ --set runner = job The task.ready and k.role were changed in the following ways to create this extension. task.ready k.role The group/version/resource (GVR) and the condition that should be checked for a Knative Service are defined in this template. 1 2 3 4 5 6 7 8 9 10 11 {{ - if .Values.ready.ksvc }} # task: determine if Knative Service exists and is ready - task : ready with : name : {{ .Values.ready.ksvc | quote }} group : serving.knative.dev version : v1 resource : services condition : Ready {{ - include \"task.ready.tn\" . }} {{ - end }} The role named {{ .Release.Name }}-ready is extended with the Knative apiGroup . 1 2 3 4 5 6 {{ - if .Values.ready.ksvc }} - apiGroups : [ \"serving.knative.dev\" ] resourceNames : [{{ .Values.ready.ksvc | quote }}] resources : [ \"services\" ] verbs : [ \"get\" ] {{ - end }}","title":"Example"},{"location":"user-guide/tasks/slack/","text":"slack \u00b6 Send the experiment report in a message to a Slack channel using a incoming webhook . Usage Example \u00b6 iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST \\ --set runner = job See here for a more in-depth tutorial. Parameters \u00b6 Name Type Required Default value Description url string Yes N/A URL to the Slack webhook payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/hub/iter8-0.13.0/templates/notify/_payload-slack.tpl URL to a payload template softFailure bool No true Indicates the task and experiment should not fail if the task cannot successfully send the request if string No N/A An if condition that can be control when the task is run in a multi-looped experiment . To learn more, see here . Default payload \u00b6 The payload will determine what will be contained in the Slack message. The default payload template of the slack task is to send the experiment report in text form. However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default. if parameter \u00b6 The if parameter is used to control when the task is run in a multi-looped experiment . For example, if you would like for the slack task to run only at the 10 th loop instead of every loop, you can do the following: iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set slack.url=<Slack webhook> \\ --set slack.method=POST \\ --set github.if=\"Result.NumLoops == 10\" --set runner=job You may use any field in the Result object for your logic.","title":"slack"},{"location":"user-guide/tasks/slack/#slack","text":"Send the experiment report in a message to a Slack channel using a incoming webhook .","title":"slack"},{"location":"user-guide/tasks/slack/#usage-example","text":"iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST \\ --set runner = job See here for a more in-depth tutorial.","title":"Usage Example"},{"location":"user-guide/tasks/slack/#parameters","text":"Name Type Required Default value Description url string Yes N/A URL to the Slack webhook payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/hub/iter8-0.13.0/templates/notify/_payload-slack.tpl URL to a payload template softFailure bool No true Indicates the task and experiment should not fail if the task cannot successfully send the request if string No N/A An if condition that can be control when the task is run in a multi-looped experiment . To learn more, see here .","title":"Parameters"},{"location":"user-guide/tasks/slack/#default-payload","text":"The payload will determine what will be contained in the Slack message. The default payload template of the slack task is to send the experiment report in text form. However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default.","title":"Default payload"},{"location":"user-guide/tasks/slack/#if-parameter","text":"The if parameter is used to control when the task is run in a multi-looped experiment . For example, if you would like for the slack task to run only at the 10 th loop instead of every loop, you can do the following: iter8 k launch \\ --set \"tasks={http,assess,slack}\" \\ --set http.url=http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean=50 \\ --set assess.SLOs.upper.http/error-count=0 \\ --set slack.url=<Slack webhook> \\ --set slack.method=POST \\ --set github.if=\"Result.NumLoops == 10\" --set runner=job You may use any field in the Result object for your logic.","title":"if parameter"},{"location":"user-guide/topics/ab_testing/","text":"A/B/n Testing \u00b6 A/B/n testing relies on business metrics typically computed by a frontend, user-facing, application component. Metric values often depend on one or more interactions with backend (not user-facing) application components. To run an A/B/n test on a backend component, it is necessary to be able to associate a metric value (computed by the frontend component) to the version of the backend component that contributed to its computation. The challenge is that the frontend component often does not know which version of the backend component processed a given request. To address this challenge, Iter8 introduces an A/B/n SDK. The Iter8 SDK introduces the concept of a track identifier . A track is a logical version of a Kubernetes application. The set of valid track identifiers is fixed over the lifetime of the application. The version of the application associated with a given track identifier changes over time as new versions are developed. For a given application, the set of track identifiers is fixed; the number of track identifiers determines how many versions of the application can be deployed/tested at the same time. Because the set of track identifiers is fixed, they can be used to configure routing to the application. The Iter8 SDK provides two APIs to frontend application components: a. Lookup() - Given an application and user session, returns a track identifier. So long as there are no changes in configuration, the track identifier (and hence the route) will be same for the same user session, guaranteeing session stickiness. b. WriteMetric() - Given an application, a user session, a metric name its value, WriteMetric() associates the metric value with the appropriate version of the application. Configuring the Iter8 A/B/n Service \u00b6 An Iter8 A/B/n service implements the gRPC API. This service is configured, at deployment, to watch the resource objects for a set of applications so that it can identify new versions and their mapping to a track identifier. To watch for versions of an application, specify the list of the types of the objects that must be present and ready for a version to be considered ready: --set \"apps.<namespace>.<application_name>.resources={<comma separated list resoure types>}\" For example, to watch for versions of an application my_app in the namespace my_namespace where each version is composed of a Kubernetes service object and a Kubernetes deployment object, specify: --set \"apps.my_namespace.my_app.resources={service,deployment}\" Valid resource types are corresponding Kubernetes resource types (specified by group, version and resource) are listed below. When the required condition value is true , the resource object is considered ready. Type Name Kubernetes Resource Type (GVR) Required Condition service v1 services - deployment apps/v1 deployments Available ksvc serving.knative.dev/v1 services Ready If more than one candidate version can be deployed at the same time, specify the maximum number using maxNumCandidates : --set apps.<namespace>.<application_name>.maxNumCandidates=<number> From the above configuration, Iter8 infers the names of the expected resource objects using these assumptions: The baseline track identifier is the application name Track identifiers associated with candidate versions are of the form <application_name>-candidate-<index> All resource objects for all versions are deployed in the same namespace There is only 1 resource object of a given type in each version The name of each object in the version associated with the baseline track is the application name The name of each object in the version associated with a candidate track is of the form <application_name>-candidate-<index> where index is 1, 2, etc. Deployment Time Configuration of Backend Components \u00b6 As versions of the backend component are deployed or deleted, the Iter8 A/B/n service maintains a mapping of track identifier to available version. Using this mapping it is then able to respond appropriately to Lookup() and WriteMetric() requests. To build and maintain it's mapping, the A/B/n service watches the resource objects specified as part of the A/B/n service configuration (see above). In particular, the configuration requires that the Kubernetes objects comprising the backend component adhere to the specified naming convention. Further, they should have the label app.kubernetes.io/version set to the version identifier. Developing Frontend Components: Using the SDK \u00b6 The basic steps to author a frontend application component using the Iter8 SDK are outlined below for Node.js and Go . Similar steps would be required for any gRPC supported language. Use/Import language specific libraries \u00b6 The gRPC protocol buffer definition is used to generate language specific implementation. These files can be used directly or packaged and imported as a library. As examples, the Node.js sample uses manually generated files directly. On the other hand, the Go sample imports the library provided by the core Iter8 service implementation. In addition to the API specific methods, some general gRPC libraries are required. Node.js Go The manually generated node files abn_pd.js and abn_grpc_pb.js used in the sample application can be copied and used without modification. var grpc = require ( '@grpc/grpc-js' ); var messages = require ( './abn_pb.js' ); var services = require ( './abn_grpc_pb.js' ); import ( \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials/insecure\" pb \"github.com/iter8-tools/iter8/abn/grpc\" ) Instantiate a gRPC client \u00b6 Instantiate a client to the Iter8 A/B/n service: Node.js Go var client = new services . ABNClient ( abnEndpoint , grpc . credentials . createInsecure ()); opts := [] grpc . DialOption { grpc . WithTransportCredentials ( insecure . NewCredentials ())} conn , err := grpc . Dial ( fmt . Sprintf ( \"%s:%s\" , getAbnService (), getAbnServicePort ()), opts ... ) if err != nil { panic ( \"Cannot establish connection with abn service\" ) } c := pb . NewABNClient ( conn ) client = & c Define routing \u00b6 Track identifiers are mapped to a static set of endpoints. One approach is to maintain a map from track identifier to endpoint: Node.js Go const trackToRoute = { \"backend\" : \"http://backend.default.svc.cluster.local:8091\" , \"backend-candidate-1\" : \"http://backend-candidate-1.default.svc.cluster.local:8091\" , } trackToRoute = map [ string ] string { \"backend\" : \"http://backend.default.svc.cluster.local:8091\" , \"backend-candidate-1\" : \"http://backend-candidate-1.default.svc.cluster.local:8091\" , } Using Lookup() \u00b6 Given a user session identifier, Lookup() returns a track identifier that can be used to route requests. In code sample below, the user session identifier is assumed to be passed in the X-User header of user requests. The track identifier is used as an index to the trackToRoute map defined above. A default is used if the call to Lookup() fails for any reason. Node.js Go var application = new messages . Application (); application . setName ( 'default/backend' ); application . setUser ( req . header ( 'X-User' )); client . lookup ( application , function ( err , session ) { if ( err || ( session . getTrack () == '' )) { // use default route (see above) console . warn ( \"error or null\" ) } else { // use route determined by recommended track console . info ( 'lookup suggested track %s' , session . getTrack ()) route = trackToRoute [ session . getTrack ()]; } // call backend service using route ... }); route := trackToRoute [ \"backend\" ] user := req . Header [ \"X-User\" ][ 0 ] s , err := ( * client ). Lookup ( ctx , & pb . Application { Name : \"default/backend\" , User : user , }, ) if err == nil && s != nil { r , ok := trackToRoute [ s . GetTrack ()] if ok { route = r } } // call backend service using route ... Using WriteMetric() \u00b6 As an example, a single metric named sample_metric is assigned a random value between 0 and 100 and written. Node.js Go var mv = new messages . MetricValue (); mv . setName ( 'sample_metric' ); mv . setValue ( random ({ min : 0 , max : 100 , integer : true }). toString ()); mv . setApplication ( 'default/backend' ); mv . setUser ( user ); _ , _ = ( * client ). WriteMetric ( ctx , & pb . MetricValue { Name : \"sample_metric\" , Value : fmt . Sprintf ( \"%f\" , rand . Float64 () * 100.0 ), Application : \"default/backend\" , User : user , }, )","title":"A/B/n testing"},{"location":"user-guide/topics/ab_testing/#abn-testing","text":"A/B/n testing relies on business metrics typically computed by a frontend, user-facing, application component. Metric values often depend on one or more interactions with backend (not user-facing) application components. To run an A/B/n test on a backend component, it is necessary to be able to associate a metric value (computed by the frontend component) to the version of the backend component that contributed to its computation. The challenge is that the frontend component often does not know which version of the backend component processed a given request. To address this challenge, Iter8 introduces an A/B/n SDK. The Iter8 SDK introduces the concept of a track identifier . A track is a logical version of a Kubernetes application. The set of valid track identifiers is fixed over the lifetime of the application. The version of the application associated with a given track identifier changes over time as new versions are developed. For a given application, the set of track identifiers is fixed; the number of track identifiers determines how many versions of the application can be deployed/tested at the same time. Because the set of track identifiers is fixed, they can be used to configure routing to the application. The Iter8 SDK provides two APIs to frontend application components: a. Lookup() - Given an application and user session, returns a track identifier. So long as there are no changes in configuration, the track identifier (and hence the route) will be same for the same user session, guaranteeing session stickiness. b. WriteMetric() - Given an application, a user session, a metric name its value, WriteMetric() associates the metric value with the appropriate version of the application.","title":"A/B/n Testing"},{"location":"user-guide/topics/ab_testing/#configuring-the-iter8-abn-service","text":"An Iter8 A/B/n service implements the gRPC API. This service is configured, at deployment, to watch the resource objects for a set of applications so that it can identify new versions and their mapping to a track identifier. To watch for versions of an application, specify the list of the types of the objects that must be present and ready for a version to be considered ready: --set \"apps.<namespace>.<application_name>.resources={<comma separated list resoure types>}\" For example, to watch for versions of an application my_app in the namespace my_namespace where each version is composed of a Kubernetes service object and a Kubernetes deployment object, specify: --set \"apps.my_namespace.my_app.resources={service,deployment}\" Valid resource types are corresponding Kubernetes resource types (specified by group, version and resource) are listed below. When the required condition value is true , the resource object is considered ready. Type Name Kubernetes Resource Type (GVR) Required Condition service v1 services - deployment apps/v1 deployments Available ksvc serving.knative.dev/v1 services Ready If more than one candidate version can be deployed at the same time, specify the maximum number using maxNumCandidates : --set apps.<namespace>.<application_name>.maxNumCandidates=<number> From the above configuration, Iter8 infers the names of the expected resource objects using these assumptions: The baseline track identifier is the application name Track identifiers associated with candidate versions are of the form <application_name>-candidate-<index> All resource objects for all versions are deployed in the same namespace There is only 1 resource object of a given type in each version The name of each object in the version associated with the baseline track is the application name The name of each object in the version associated with a candidate track is of the form <application_name>-candidate-<index> where index is 1, 2, etc.","title":"Configuring the Iter8 A/B/n Service"},{"location":"user-guide/topics/ab_testing/#deployment-time-configuration-of-backend-components","text":"As versions of the backend component are deployed or deleted, the Iter8 A/B/n service maintains a mapping of track identifier to available version. Using this mapping it is then able to respond appropriately to Lookup() and WriteMetric() requests. To build and maintain it's mapping, the A/B/n service watches the resource objects specified as part of the A/B/n service configuration (see above). In particular, the configuration requires that the Kubernetes objects comprising the backend component adhere to the specified naming convention. Further, they should have the label app.kubernetes.io/version set to the version identifier.","title":"Deployment Time Configuration of Backend Components"},{"location":"user-guide/topics/ab_testing/#developing-frontend-components-using-the-sdk","text":"The basic steps to author a frontend application component using the Iter8 SDK are outlined below for Node.js and Go . Similar steps would be required for any gRPC supported language.","title":"Developing Frontend Components: Using the SDK"},{"location":"user-guide/topics/ab_testing/#useimport-language-specific-libraries","text":"The gRPC protocol buffer definition is used to generate language specific implementation. These files can be used directly or packaged and imported as a library. As examples, the Node.js sample uses manually generated files directly. On the other hand, the Go sample imports the library provided by the core Iter8 service implementation. In addition to the API specific methods, some general gRPC libraries are required. Node.js Go The manually generated node files abn_pd.js and abn_grpc_pb.js used in the sample application can be copied and used without modification. var grpc = require ( '@grpc/grpc-js' ); var messages = require ( './abn_pb.js' ); var services = require ( './abn_grpc_pb.js' ); import ( \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials/insecure\" pb \"github.com/iter8-tools/iter8/abn/grpc\" )","title":"Use/Import language specific libraries"},{"location":"user-guide/topics/ab_testing/#instantiate-a-grpc-client","text":"Instantiate a client to the Iter8 A/B/n service: Node.js Go var client = new services . ABNClient ( abnEndpoint , grpc . credentials . createInsecure ()); opts := [] grpc . DialOption { grpc . WithTransportCredentials ( insecure . NewCredentials ())} conn , err := grpc . Dial ( fmt . Sprintf ( \"%s:%s\" , getAbnService (), getAbnServicePort ()), opts ... ) if err != nil { panic ( \"Cannot establish connection with abn service\" ) } c := pb . NewABNClient ( conn ) client = & c","title":"Instantiate a gRPC client"},{"location":"user-guide/topics/ab_testing/#define-routing","text":"Track identifiers are mapped to a static set of endpoints. One approach is to maintain a map from track identifier to endpoint: Node.js Go const trackToRoute = { \"backend\" : \"http://backend.default.svc.cluster.local:8091\" , \"backend-candidate-1\" : \"http://backend-candidate-1.default.svc.cluster.local:8091\" , } trackToRoute = map [ string ] string { \"backend\" : \"http://backend.default.svc.cluster.local:8091\" , \"backend-candidate-1\" : \"http://backend-candidate-1.default.svc.cluster.local:8091\" , }","title":"Define routing"},{"location":"user-guide/topics/ab_testing/#using-lookup","text":"Given a user session identifier, Lookup() returns a track identifier that can be used to route requests. In code sample below, the user session identifier is assumed to be passed in the X-User header of user requests. The track identifier is used as an index to the trackToRoute map defined above. A default is used if the call to Lookup() fails for any reason. Node.js Go var application = new messages . Application (); application . setName ( 'default/backend' ); application . setUser ( req . header ( 'X-User' )); client . lookup ( application , function ( err , session ) { if ( err || ( session . getTrack () == '' )) { // use default route (see above) console . warn ( \"error or null\" ) } else { // use route determined by recommended track console . info ( 'lookup suggested track %s' , session . getTrack ()) route = trackToRoute [ session . getTrack ()]; } // call backend service using route ... }); route := trackToRoute [ \"backend\" ] user := req . Header [ \"X-User\" ][ 0 ] s , err := ( * client ). Lookup ( ctx , & pb . Application { Name : \"default/backend\" , User : user , }, ) if err == nil && s != nil { r , ok := trackToRoute [ s . GetTrack ()] if ok { route = r } } // call backend service using route ...","title":"Using Lookup()"},{"location":"user-guide/topics/ab_testing/#using-writemetric","text":"As an example, a single metric named sample_metric is assigned a random value between 0 and 100 and written. Node.js Go var mv = new messages . MetricValue (); mv . setName ( 'sample_metric' ); mv . setValue ( random ({ min : 0 , max : 100 , integer : true }). toString ()); mv . setApplication ( 'default/backend' ); mv . setUser ( user ); _ , _ = ( * client ). WriteMetric ( ctx , & pb . MetricValue { Name : \"sample_metric\" , Value : fmt . Sprintf ( \"%f\" , rand . Float64 () * 100.0 ), Application : \"default/backend\" , User : user , }, )","title":"Using WriteMetric()"},{"location":"user-guide/topics/autox/","text":"Automated Experiments: AutoX \u00b6 AutoX, short for \"automated experiments\", allows Iter8 to detect changes to your Kubernetes resources objects and automatically start new experiments, allowing you to test your applications as soon as you release a new version. To configure AutoX, you will need to specify a set of experiment groups and, for each group, the Kubernetes resource object (trigger object) that you expect AutoX to watch and one or more experiments to be performed in response to new versions of this object. The trigger object is specified by providing the name, namespace, and the group-version-resource (GVR) metadata of the trigger object. See the following example: helm install autox autox --repo https://iter8-tools.github.io/hub/ --version 0 .1.6 \\ --set 'groups.myApp.trigger.name=myApp' \\ --set 'groups.myApp.trigger.namespace=default' \\ --set 'groups.myApp.trigger.group=apps' \\ --set 'groups.myApp.trigger.version=v1' \\ --set 'groups.myApp.trigger.resource=deployments' \\ --set 'groups.myApp.specs.iter8-http.name=iter8' \\ --set 'groups.myApp.specs.iter8-http.values.tasks={ready,http,assess}' \\ --set 'groups.myApp.specs.iter8-http.values.ready.deploy=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.service=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.timeout=60s' \\ --set 'groups.myApp.specs.iter8-http.values.http.url=http://myApp.default/get' --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/error-count=0' --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/latency-mean=50' \\ --set 'groups.myApp.specs.iter8-http.version=0.13.0' \\ --set 'groups.myApp.specs.iter8-http.values.runner=job' In this example, there is only one experiment group named myApp ( groups.myApp... ), and within that group, there is the trigger object definition ( groups.myApp.trigger... ) and a single experiment spec named iter8-http ( groups.myApp.specs.iter8... ). In this next example, we have augmented the previous example with an additional experiment spec. helm install autox autox --repo https://iter8-tools.github.io/hub/ --version 0 .1.6 \\ --set 'groups.myApp.trigger.name=myApp' \\ --set 'groups.myApp.trigger.namespace=default' \\ --set 'groups.myApp.trigger.group=apps' \\ --set 'groups.myApp.trigger.version=v1' \\ --set 'groups.myApp.trigger.resource=deployments' \\ --set 'groups.myApp.specs.iter8-http.name=iter8' \\ --set 'groups.myApp.specs.iter8-http.values.tasks={ready,http,assess}' \\ --set 'groups.myApp.specs.iter8-http.values.ready.deploy=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.service=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.timeout=60s' \\ --set 'groups.myApp.specs.iter8-http.values.http.url=http://myApp.default/get' \\ --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/error-count=0' \\ --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/latency-mean=50' \\ --set 'groups.myApp.specs.iter8-http.version=0.13.0' \\ --set 'groups.myApp.specs.iter8-http.values.runner=job' \\ --set 'groups.myApp.specs.iter8-grpc.values.tasks={ready,grpc,assess}' \\ --set 'groups.myApp.specs.iter8-grpc.values.ready.deploy=myApp' \\ --set 'groups.myApp.specs.iter8-grpc.values.ready.service=myApp' \\ --set 'groups.myApp.specs.iter8-grpc.values.ready.timeout=60s' \\ --set 'groups.myApp.specs.iter8-grpc.values.grpc.host=...' \\ --set 'groups.myApp.specs.iter8-grpc.values.grpc.call=...' \\ --set 'groups.myApp.specs.iter8-grpc.values.grpc.protoURL=...' \\ --set 'groups.myApp.specs.iter8-grpc.values.assess.SLOs.upper.grpc/error-rate=0' \\ --set 'groups.myApp.specs.iter8-grpc.values.assess.SLOs.upper.grpc/latency/latency-mean=50' \\ --set 'groups.myApp.specs.iter8-grpc.values.runner=job' Now, when a new version of the trigger is released, AutoX will relaunch not only an HTTP SLO validation test but also a gRPC SLO validation test. A trigger object must have a app.kubernetes.io/version label (version label. This label is used to identify new versions of the trigger object, which will cause AutoX to relaunch experiments. If the trigger does not have a version label, the AutoX will attempt to remove any preexisting experiments.","title":"Automated experiments"},{"location":"user-guide/topics/autox/#automated-experiments-autox","text":"AutoX, short for \"automated experiments\", allows Iter8 to detect changes to your Kubernetes resources objects and automatically start new experiments, allowing you to test your applications as soon as you release a new version. To configure AutoX, you will need to specify a set of experiment groups and, for each group, the Kubernetes resource object (trigger object) that you expect AutoX to watch and one or more experiments to be performed in response to new versions of this object. The trigger object is specified by providing the name, namespace, and the group-version-resource (GVR) metadata of the trigger object. See the following example: helm install autox autox --repo https://iter8-tools.github.io/hub/ --version 0 .1.6 \\ --set 'groups.myApp.trigger.name=myApp' \\ --set 'groups.myApp.trigger.namespace=default' \\ --set 'groups.myApp.trigger.group=apps' \\ --set 'groups.myApp.trigger.version=v1' \\ --set 'groups.myApp.trigger.resource=deployments' \\ --set 'groups.myApp.specs.iter8-http.name=iter8' \\ --set 'groups.myApp.specs.iter8-http.values.tasks={ready,http,assess}' \\ --set 'groups.myApp.specs.iter8-http.values.ready.deploy=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.service=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.timeout=60s' \\ --set 'groups.myApp.specs.iter8-http.values.http.url=http://myApp.default/get' --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/error-count=0' --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/latency-mean=50' \\ --set 'groups.myApp.specs.iter8-http.version=0.13.0' \\ --set 'groups.myApp.specs.iter8-http.values.runner=job' In this example, there is only one experiment group named myApp ( groups.myApp... ), and within that group, there is the trigger object definition ( groups.myApp.trigger... ) and a single experiment spec named iter8-http ( groups.myApp.specs.iter8... ). In this next example, we have augmented the previous example with an additional experiment spec. helm install autox autox --repo https://iter8-tools.github.io/hub/ --version 0 .1.6 \\ --set 'groups.myApp.trigger.name=myApp' \\ --set 'groups.myApp.trigger.namespace=default' \\ --set 'groups.myApp.trigger.group=apps' \\ --set 'groups.myApp.trigger.version=v1' \\ --set 'groups.myApp.trigger.resource=deployments' \\ --set 'groups.myApp.specs.iter8-http.name=iter8' \\ --set 'groups.myApp.specs.iter8-http.values.tasks={ready,http,assess}' \\ --set 'groups.myApp.specs.iter8-http.values.ready.deploy=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.service=myApp' \\ --set 'groups.myApp.specs.iter8-http.values.ready.timeout=60s' \\ --set 'groups.myApp.specs.iter8-http.values.http.url=http://myApp.default/get' \\ --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/error-count=0' \\ --set 'groups.myApp.specs.iter8-http.values.assess.SLOs.upper.http/latency-mean=50' \\ --set 'groups.myApp.specs.iter8-http.version=0.13.0' \\ --set 'groups.myApp.specs.iter8-http.values.runner=job' \\ --set 'groups.myApp.specs.iter8-grpc.values.tasks={ready,grpc,assess}' \\ --set 'groups.myApp.specs.iter8-grpc.values.ready.deploy=myApp' \\ --set 'groups.myApp.specs.iter8-grpc.values.ready.service=myApp' \\ --set 'groups.myApp.specs.iter8-grpc.values.ready.timeout=60s' \\ --set 'groups.myApp.specs.iter8-grpc.values.grpc.host=...' \\ --set 'groups.myApp.specs.iter8-grpc.values.grpc.call=...' \\ --set 'groups.myApp.specs.iter8-grpc.values.grpc.protoURL=...' \\ --set 'groups.myApp.specs.iter8-grpc.values.assess.SLOs.upper.grpc/error-rate=0' \\ --set 'groups.myApp.specs.iter8-grpc.values.assess.SLOs.upper.grpc/latency/latency-mean=50' \\ --set 'groups.myApp.specs.iter8-grpc.values.runner=job' Now, when a new version of the trigger is released, AutoX will relaunch not only an HTTP SLO validation test but also a gRPC SLO validation test. A trigger object must have a app.kubernetes.io/version label (version label. This label is used to identify new versions of the trigger object, which will cause AutoX to relaunch experiments. If the trigger does not have a version label, the AutoX will attempt to remove any preexisting experiments.","title":"Automated Experiments: AutoX"},{"location":"user-guide/topics/group/","text":"Namespaces and groups for Kubernetes experiments \u00b6 Kubernetes experiments are launched within a namespace , and are associated with a unique group within that namespace. For example, consider the following invocation: iter8 k launch -g hbin \\ --set \"tasks={http,assess}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 In the above invocation, the iter8 k launch command implicitly specifies the namespace as default , and explicitly specifies the group as hbin . If the group name is not specified explicitly, then it is set to default . The namespace can be specified explicitly using the -n or --namespace flags (see here ). The following example illustrates the relationship between namespaces, groups, and experiments. . \u251c\u2500\u2500 namespace1 \u2502 \u251c\u2500\u2500 group-a \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u251c\u2500\u2500 group-b \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u2514\u2500\u2500 group-c \u2502 \u2514\u2500\u2500 experiment \u251c\u2500\u2500 namespace2 \u2502 \u251c\u2500\u2500 group-a \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u251c\u2500\u2500 group-b \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u2514\u2500\u2500 group-c \u2502 \u2514\u2500\u2500 experiment \u2514\u2500\u2500 namespace3 \u2514\u2500\u2500 group-x \u2514\u2500\u2500 experiment Use-cases \u00b6 Run multiple experiments concurrently within a Kubernetes namespace by associating them with distinct groups. These experiments may be associated with the same app or with different apps. Replace a currently running experiment in Kubernetes with a new one. When you invoke iter8 k launch , any previous experiment executions within the group is wiped out and replaced with a fresh experiment that starts to execute. How groups work \u00b6 Under the covers, Iter8 implements each experiment group as a Helm release and each new experiment run within the group as an update of that release.","title":"Namespace and group"},{"location":"user-guide/topics/group/#namespaces-and-groups-for-kubernetes-experiments","text":"Kubernetes experiments are launched within a namespace , and are associated with a unique group within that namespace. For example, consider the following invocation: iter8 k launch -g hbin \\ --set \"tasks={http,assess}\" \\ --set http.url = http://httpbin.default/get \\ --set assess.SLOs.upper.http/latency-mean = 50 In the above invocation, the iter8 k launch command implicitly specifies the namespace as default , and explicitly specifies the group as hbin . If the group name is not specified explicitly, then it is set to default . The namespace can be specified explicitly using the -n or --namespace flags (see here ). The following example illustrates the relationship between namespaces, groups, and experiments. . \u251c\u2500\u2500 namespace1 \u2502 \u251c\u2500\u2500 group-a \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u251c\u2500\u2500 group-b \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u2514\u2500\u2500 group-c \u2502 \u2514\u2500\u2500 experiment \u251c\u2500\u2500 namespace2 \u2502 \u251c\u2500\u2500 group-a \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u251c\u2500\u2500 group-b \u2502 \u2502 \u2514\u2500\u2500 experiment \u2502 \u2514\u2500\u2500 group-c \u2502 \u2514\u2500\u2500 experiment \u2514\u2500\u2500 namespace3 \u2514\u2500\u2500 group-x \u2514\u2500\u2500 experiment","title":"Namespaces and groups for Kubernetes experiments"},{"location":"user-guide/topics/group/#use-cases","text":"Run multiple experiments concurrently within a Kubernetes namespace by associating them with distinct groups. These experiments may be associated with the same app or with different apps. Replace a currently running experiment in Kubernetes with a new one. When you invoke iter8 k launch , any previous experiment executions within the group is wiped out and replaced with a fresh experiment that starts to execute.","title":"Use-cases"},{"location":"user-guide/topics/group/#how-groups-work","text":"Under the covers, Iter8 implements each experiment group as a Helm release and each new experiment run within the group as an update of that release.","title":"How groups work"},{"location":"user-guide/topics/metrics/","text":"Metrics \u00b6 Provider \u00b6 A provider in Iter8 is a data source that supplies metric values. Fully qualified names \u00b6 Metrics are scoped by providers. Providers have unique names, and within the scope of a provider, metrics have unique names. In addition, multiple endpoints and metric aggregation will determine the metric name. The fully qualified metric name will be in the form provider[-endpoint]/metric[/aggregation] . Following are some examples of fully qualified metric names: 1. http/latency-mean 2. grpc/latency/mean 3. http-httpbin/latency-mean 4. grpc-getFeature/latency/mean 5. abn/sample_metric/count Built-in metrics provider \u00b6 Iter8 has built-in metrics providers, namely, http and grpc . Custom metrics provider \u00b6 You can use metrics from any (RESTful) database in Iter8 experiments. Metrics fetched by Iter8 from databases are also referred to as custom metrics. See here to learn more about custom metrics. Metric types \u00b6 Iter8 defines counter and gauge metric types which are analogous to the corresponding metric types defined by Prometheus . We quote from the Prometheus documentation below for their definitions. Counter A counter is a cumulative metric that represents a single monotonically increasing counter whose value can only increase or be reset to zero on restart. For example, you can use a counter to represent the number of requests served, tasks completed, or errors. Do not use a counter to expose a value that can decrease. For example, do not use a counter for the number of currently running processes; instead use a gauge. Gauge A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \"counts\" that can go up and down, like the number of concurrent requests. Multiple endpoints \u00b6 Some built-in metrics providers, such as http and grpc , allow you to specify and test multiple endpoints. In these cases, the endpoint name will be appended to the provider name in the metric name. See the documentation for examples.","title":"Metrics"},{"location":"user-guide/topics/metrics/#metrics","text":"","title":"Metrics"},{"location":"user-guide/topics/metrics/#provider","text":"A provider in Iter8 is a data source that supplies metric values.","title":"Provider"},{"location":"user-guide/topics/metrics/#fully-qualified-names","text":"Metrics are scoped by providers. Providers have unique names, and within the scope of a provider, metrics have unique names. In addition, multiple endpoints and metric aggregation will determine the metric name. The fully qualified metric name will be in the form provider[-endpoint]/metric[/aggregation] . Following are some examples of fully qualified metric names: 1. http/latency-mean 2. grpc/latency/mean 3. http-httpbin/latency-mean 4. grpc-getFeature/latency/mean 5. abn/sample_metric/count","title":"Fully qualified names"},{"location":"user-guide/topics/metrics/#built-in-metrics-provider","text":"Iter8 has built-in metrics providers, namely, http and grpc .","title":"Built-in metrics provider"},{"location":"user-guide/topics/metrics/#custom-metrics-provider","text":"You can use metrics from any (RESTful) database in Iter8 experiments. Metrics fetched by Iter8 from databases are also referred to as custom metrics. See here to learn more about custom metrics.","title":"Custom metrics provider"},{"location":"user-guide/topics/metrics/#metric-types","text":"Iter8 defines counter and gauge metric types which are analogous to the corresponding metric types defined by Prometheus . We quote from the Prometheus documentation below for their definitions. Counter A counter is a cumulative metric that represents a single monotonically increasing counter whose value can only increase or be reset to zero on restart. For example, you can use a counter to represent the number of requests served, tasks completed, or errors. Do not use a counter to expose a value that can decrease. For example, do not use a counter for the number of currently running processes; instead use a gauge. Gauge A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \"counts\" that can go up and down, like the number of concurrent requests.","title":"Metric types"},{"location":"user-guide/topics/metrics/#multiple-endpoints","text":"Some built-in metrics providers, such as http and grpc , allow you to specify and test multiple endpoints. In these cases, the endpoint name will be appended to the provider name in the metric name. See the documentation for examples.","title":"Multiple endpoints"},{"location":"user-guide/topics/parameters/","text":"Experiment Parameters \u00b6 Iter8 is built on Helm . Iter8 experiments can be configured with parameters using the same mechanisms provided by Helm for setting chart values . The set of configurable parameters of an experiment includes the parameters of the tasks involved in the experiment. Iter8 uses the convention that the parameters of a task are nested under the name of that task. In the following example, the url parameter of the http task is nested under the http object, and the SLOs parameter of the assess task is nested under the assess object. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.url = https://httpbin.org/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 --set runner = job All the parameters of a task or an experiment are optional unless indicated otherwise in the documentation of the task or experiment. Parameters \u00b6 Global experiment parameters are described here. Task specific parameters are documented in each task description. Name Type Description serviceAccountName string Optional name of a service account to use. If specified, it is assumed the service account has the necessary permissions to run an experiment. If not specified, Iter8 will create a service account. runner string One of job or cronjob indicating whether the experiment has a single loop or multiple loops, respectively. cronjobSchedule string Schedule for a multi-loop experiment. Required if runner is cronjob . Ignored otherwise. Expressed using Unix cronjob notation. logLevel string Log level. Must be one of trace , debug , info (default), warning , or error .","title":"Experiment parameters"},{"location":"user-guide/topics/parameters/#experiment-parameters","text":"Iter8 is built on Helm . Iter8 experiments can be configured with parameters using the same mechanisms provided by Helm for setting chart values . The set of configurable parameters of an experiment includes the parameters of the tasks involved in the experiment. Iter8 uses the convention that the parameters of a task are nested under the name of that task. In the following example, the url parameter of the http task is nested under the http object, and the SLOs parameter of the assess task is nested under the assess object. iter8 k launch \\ --set \"tasks={http,assess}\" \\ --set http.url = https://httpbin.org/get \\ --set assess.SLOs.upper.http/latency-mean = 50 \\ --set assess.SLOs.upper.http/error-count = 0 --set runner = job All the parameters of a task or an experiment are optional unless indicated otherwise in the documentation of the task or experiment.","title":"Experiment Parameters"},{"location":"user-guide/topics/parameters/#parameters","text":"Global experiment parameters are described here. Task specific parameters are documented in each task description. Name Type Description serviceAccountName string Optional name of a service account to use. If specified, it is assumed the service account has the necessary permissions to run an experiment. If not specified, Iter8 will create a service account. runner string One of job or cronjob indicating whether the experiment has a single loop or multiple loops, respectively. cronjobSchedule string Schedule for a multi-loop experiment. Required if runner is cronjob . Ignored otherwise. Expressed using Unix cronjob notation. logLevel string Log level. Must be one of trace , debug , info (default), warning , or error .","title":"Parameters"}]}