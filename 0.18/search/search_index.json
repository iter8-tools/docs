{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Iter8 \u00b6","title":"Home"},{"location":"#iter8","text":"","title":"Iter8"},{"location":"contributing/","text":"Overview \u00b6 Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on key areas including: Problems found during setup of Iter8 Gaps in our getting started tutorial and other documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know! Ways to contribute \u00b6 We welcome many types of contributions including: CLI and Iter8 performance test charts Docs CI, builds, and tests Reviewing pull requests Ask for help \u00b6 The best ways to reach us with a question is to ask... On the original GitHub issue In the #development channel in the Iter8 Slack workspace Find an issue \u00b6 Iter8 issues are tracked here . Pull request lifecycle \u00b6 Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools . Sign your commits \u00b6 Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the Git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running: git commit --amend -s Development environment setup \u00b6 The Iter8 project consists of the following repos. iter8-tools/iter8 : source for the Iter8 CLI, performance test, and controller charts iter8-tools/docs : source for Iter8 docs iter8-tools/iter8 \u00b6 This is the source repo for Iter8 CLI. Clone iter8 \u00b6 git clone https://github.com/iter8-tools/iter8.git Build Iter8 \u00b6 make build Install Iter8 locally \u00b6 make clean install iter8 version Run unit tests and see coverage information \u00b6 make tests make coverage make htmlcov Lint Iter8 \u00b6 make lint Build and push Iter8 image \u00b6 Define a name for your Docker image IMG =[ Docker image name ] Build and push Iter8 image to Docker docker build -f docker/Dockerfile -t $IMG . docker push $IMG iter8-tools/docs \u00b6 This is the source repo for Iter8 documentation. Clone docs \u00b6 git clone https://github.com/iter8-tools/docs.git Locally serve docs \u00b6 From the root of this repo: python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s You can now see your local docs at http://localhost:8000 . You will also see live updates to http://localhost:8000 as you update the contents of the docs folder.","title":"Contributing"},{"location":"contributing/#overview","text":"Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on key areas including: Problems found during setup of Iter8 Gaps in our getting started tutorial and other documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!","title":"Overview"},{"location":"contributing/#ways-to-contribute","text":"We welcome many types of contributions including: CLI and Iter8 performance test charts Docs CI, builds, and tests Reviewing pull requests","title":"Ways to contribute"},{"location":"contributing/#ask-for-help","text":"The best ways to reach us with a question is to ask... On the original GitHub issue In the #development channel in the Iter8 Slack workspace","title":"Ask for help"},{"location":"contributing/#find-an-issue","text":"Iter8 issues are tracked here .","title":"Find an issue"},{"location":"contributing/#pull-request-lifecycle","text":"Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools .","title":"Pull request lifecycle"},{"location":"contributing/#sign-your-commits","text":"Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the Git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running: git commit --amend -s","title":"Sign your commits"},{"location":"contributing/#development-environment-setup","text":"The Iter8 project consists of the following repos. iter8-tools/iter8 : source for the Iter8 CLI, performance test, and controller charts iter8-tools/docs : source for Iter8 docs","title":"Development environment setup"},{"location":"contributing/#iter8-toolsiter8","text":"This is the source repo for Iter8 CLI.","title":"iter8-tools/iter8"},{"location":"contributing/#clone-iter8","text":"git clone https://github.com/iter8-tools/iter8.git","title":"Clone iter8"},{"location":"contributing/#build-iter8","text":"make build","title":"Build Iter8"},{"location":"contributing/#install-iter8-locally","text":"make clean install iter8 version","title":"Install Iter8 locally"},{"location":"contributing/#run-unit-tests-and-see-coverage-information","text":"make tests make coverage make htmlcov","title":"Run unit tests and see coverage information"},{"location":"contributing/#lint-iter8","text":"make lint","title":"Lint Iter8"},{"location":"contributing/#build-and-push-iter8-image","text":"Define a name for your Docker image IMG =[ Docker image name ] Build and push Iter8 image to Docker docker build -f docker/Dockerfile -t $IMG . docker push $IMG","title":"Build and push Iter8 image"},{"location":"contributing/#iter8-toolsdocs","text":"This is the source repo for Iter8 documentation.","title":"iter8-tools/docs"},{"location":"contributing/#clone-docs","text":"git clone https://github.com/iter8-tools/docs.git","title":"Clone docs"},{"location":"contributing/#locally-serve-docs","text":"From the root of this repo: python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s You can now see your local docs at http://localhost:8000 . You will also see live updates to http://localhost:8000 as you update the contents of the docs folder.","title":"Locally serve docs"},{"location":"roadmap/","text":"Roadmap \u00b6 Stabilizing Iter8 APIs for CNCF sandboxing Autoscaling the metrics service Install infrastructure components such as Istio Install ML components such as KServe and KServe ModelMesh Extend routing templates to include application management Support multi-cluster installs Open Data Hub tier 1 project Metrics & evaluation for foundation model/LLM-based apps Hyperparameter tuning for foundation model/LLM-based inference pipelines Data/concept drift detection for ML models","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Stabilizing Iter8 APIs for CNCF sandboxing Autoscaling the metrics service Install infrastructure components such as Istio Install ML components such as KServe and KServe ModelMesh Extend routing templates to include application management Support multi-cluster installs Open Data Hub tier 1 project Metrics & evaluation for foundation model/LLM-based apps Hyperparameter tuning for foundation model/LLM-based inference pipelines Data/concept drift detection for ML models","title":"Roadmap"},{"location":"community/community/","text":"Community \u00b6 Slack \u00b6 Iter8 Slack workspace is here . Join the Iter8 Slack for usage and development related discussions. GitHub issues \u00b6 GitHub issues for all Iter8 repositories are managed here .","title":"Community"},{"location":"community/community/#community","text":"","title":"Community"},{"location":"community/community/#slack","text":"Iter8 Slack workspace is here . Join the Iter8 Slack for usage and development related discussions.","title":"Slack"},{"location":"community/community/#github-issues","text":"GitHub issues for all Iter8 repositories are managed here .","title":"GitHub issues"},{"location":"community/news/","text":"News and announcements \u00b6 September 2023: Iter8 has been accepted as a tier 2 component for Open Data Hub. June 2023: Our proposal to integrate with Open Data Hub has been accepted! March 2023: New Stack blog article by Michael Kalantar. Iter8: Simple A/B/n Testing of Kubernetes Apps, ML Models February 2023: DZone article by Alan Cha. Automated Performance Testing With ArgoCD and Iter8 December 2022: DZone article by Michael Kalantar. Simplifying A/B/n Testing of Backend Services October 2022: Iter8 at KubeCon. Conference details Presentation by Srinivasan Parthasarathy. Video coming soon Lightning talk by Alan Cha. Video link here August 2022: ITNEXT article by Alan Cha. Performance testing with Iter8, now with custom metrics! August 2022: Knative blog article by Srinivasan Parthasarathy. Simple Performance Testing with SLOs June 2022: Iter8 at Open Source Summit. Video coming soon. Conference details May 2022: IBM Developer blog article by Srinivasan Parthasarathy. Dead simple benchmarking and SLO validation for Kubernetes services May 2022: New Stack blog article by Srinivasan Parthasarathy. Iter8 Unifies Performance Validation for gRPC and HTTP March 2022: New Stack blog article by Michael Kalantar. Simple Load Testing with GitHub Actions Feb 2022: New Stack blog article on Simple HTTP Load Testing with SLOs Nov 2021: Iter8 at ACM Symposium on Cloud Computing. Full paper here Iter8 v0.7 and older Oct 2021: New Stack blog article by Hai Huang: Progressive Delivery on OpenShift Oct 2021: Iter8 at PREVAIL conference. Video coming soon. Conference details Oct 2021: New Stack blog article by Srinivasan Parthasarathy: Validate Service-Level Objectives of REST APIs Using Iter8 Jul 2021: Blog article by Clive Cox: ML\u200c \u200cProgressive\u200c \u200cRollouts\u200c \u200cwith\u200c \u200cSeldon\u200c \u200cand\u200c \u200cIter8\u200c Jul 2021: Iter8 at Knative meetup May 2021: Iter8 at KubeCon + CloudNativeCon Europe Mar 2021: Iter8 at Knative meetup Mar 2021: Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing Oct 2020: Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 Oct 2020: Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application Oct 2020: Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes Oct 2020: Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood Aug 2020: Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control","title":"News"},{"location":"community/news/#news-and-announcements","text":"September 2023: Iter8 has been accepted as a tier 2 component for Open Data Hub. June 2023: Our proposal to integrate with Open Data Hub has been accepted! March 2023: New Stack blog article by Michael Kalantar. Iter8: Simple A/B/n Testing of Kubernetes Apps, ML Models February 2023: DZone article by Alan Cha. Automated Performance Testing With ArgoCD and Iter8 December 2022: DZone article by Michael Kalantar. Simplifying A/B/n Testing of Backend Services October 2022: Iter8 at KubeCon. Conference details Presentation by Srinivasan Parthasarathy. Video coming soon Lightning talk by Alan Cha. Video link here August 2022: ITNEXT article by Alan Cha. Performance testing with Iter8, now with custom metrics! August 2022: Knative blog article by Srinivasan Parthasarathy. Simple Performance Testing with SLOs June 2022: Iter8 at Open Source Summit. Video coming soon. Conference details May 2022: IBM Developer blog article by Srinivasan Parthasarathy. Dead simple benchmarking and SLO validation for Kubernetes services May 2022: New Stack blog article by Srinivasan Parthasarathy. Iter8 Unifies Performance Validation for gRPC and HTTP March 2022: New Stack blog article by Michael Kalantar. Simple Load Testing with GitHub Actions Feb 2022: New Stack blog article on Simple HTTP Load Testing with SLOs Nov 2021: Iter8 at ACM Symposium on Cloud Computing. Full paper here Iter8 v0.7 and older Oct 2021: New Stack blog article by Hai Huang: Progressive Delivery on OpenShift Oct 2021: Iter8 at PREVAIL conference. Video coming soon. Conference details Oct 2021: New Stack blog article by Srinivasan Parthasarathy: Validate Service-Level Objectives of REST APIs Using Iter8 Jul 2021: Blog article by Clive Cox: ML\u200c \u200cProgressive\u200c \u200cRollouts\u200c \u200cwith\u200c \u200cSeldon\u200c \u200cand\u200c \u200cIter8\u200c Jul 2021: Iter8 at Knative meetup May 2021: Iter8 at KubeCon + CloudNativeCon Europe Mar 2021: Iter8 at Knative meetup Mar 2021: Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing Oct 2020: Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 Oct 2020: Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application Oct 2020: Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes Oct 2020: Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood Aug 2020: Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control","title":"News and announcements"},{"location":"getting-started/concepts/","text":"Iter8 \u00b6 Iter8 is the Kubernetes release optimizer built for DevOps, MLOps, SRE and data science teams. Iter8 automates traffic control for new versions of apps/ML models in the cluster and visualizes their performance metrics. Use-cases \u00b6 Iter8 simplifies a variety of traffic engineering and metrics-driven validation use-cases. They are illustrated below. Design \u00b6 Iter8 provides three inter-related components to support the above use-cases. Iter8 controller Client SDK Composable tasks Iter8 controller Client SDK Composable tasks Iter8 provides a controller that automatically and dynamically reconfigures routing resources based on the state of Kubernetes apps/ML models. The following picture illustrates a blue-green rollout scenario that is orchestrated by this controller. As part of the dynamic reconfiguration of route resources, the Iter8 controller also checks for readiness (for e.g., in KServe ModelMesh), availability (for e.g., in Kubernetes deployments) and other relevant status conditions before configuring traffic splits to candidate versions. Similarly, before candidate versions are deleted, the Iter8 controller uses finalizers to first ensure that all traffic flows to the primary version of the ML model. This makes for a very high-degree of reliability and zero-downtime/loss-less rollouts of new app/ML model versions. Users do not get this level of reliability out-of-the-box with a vanilla service mesh. With Iter8, the barrier to entry for end-users is significantly reduced. In particular, by just providing names of their ML serving resources, and (optional) traffic weights/labels, end users can get started with their release optimization use cases rapidly. Further, Iter8 does not limit the capabilities of the underlying service mesh in any way. This means more advanced teams still get to use all the power of the service-mesh alongside the reliability and ease-of-use that Iter8 brings. In addition, Iter8 provides a simple metrics store, eliminating the need for an external database. Iter8 provides a client-side SDK to facilitate routing as well as metrics collection task associated with distributed (i.e., client-server architecture-based) A/B/n testing in Kubernetes. The following picture illustrates the use of the SDK for A/B testing. Iter8's SDK is designed to handle user stickiness, collection of business metrics, and decoupling of front-end and back-end releases processes during A/B/n testing. Iter8 introduces a set of tasks which which can be composed in order to conduct a variety of performance tests. The following picture illustrates a performance test for an HTTP application, and this test consists of two tasks. In addition to load testing HTTP and gRPC services, Iter8 tasks can perform other actions such as sending notifications to Slack or GitHub. Advantages \u00b6 Iter8 has several advantages compared to other tooling in this space. First, Iter8 has no restrictions on the types of resources that make up a version of an application. This includes custom resources; that is, those defined by a custom resource definition (CRD). Because the set of resources that comprise a version is declarative, it is easy to extend . Note that this same extension mechanism also allows Iter8 to be used with any service mesh. Second, the Iter8 client SDK addresses a key challenge to A/B/n testing : the decoupling of the front-end release process from that of the back-end. It allows the front-end to reliably associate business metrics with the contributing version of the back-end. Finally, Iter8 simplifies performance testing by reducing the set up time needed to start testing. Tests can be easily specified as a sequence of easily configured tasks . Further, there is no need to setup and configure an external metrics database -- Iter8 captures the metrics data and provides a REST API allowing it to be visualized and evaluated in Grafana. Implementation \u00b6 Iter8 is written in go and builds on a few awesome open source projects including: Helm Istio plotly.js Fortio ghz Grafana","title":"Concepts"},{"location":"getting-started/concepts/#iter8","text":"Iter8 is the Kubernetes release optimizer built for DevOps, MLOps, SRE and data science teams. Iter8 automates traffic control for new versions of apps/ML models in the cluster and visualizes their performance metrics.","title":"Iter8"},{"location":"getting-started/concepts/#use-cases","text":"Iter8 simplifies a variety of traffic engineering and metrics-driven validation use-cases. They are illustrated below.","title":"Use-cases"},{"location":"getting-started/concepts/#design","text":"Iter8 provides three inter-related components to support the above use-cases. Iter8 controller Client SDK Composable tasks Iter8 controller Client SDK Composable tasks Iter8 provides a controller that automatically and dynamically reconfigures routing resources based on the state of Kubernetes apps/ML models. The following picture illustrates a blue-green rollout scenario that is orchestrated by this controller. As part of the dynamic reconfiguration of route resources, the Iter8 controller also checks for readiness (for e.g., in KServe ModelMesh), availability (for e.g., in Kubernetes deployments) and other relevant status conditions before configuring traffic splits to candidate versions. Similarly, before candidate versions are deleted, the Iter8 controller uses finalizers to first ensure that all traffic flows to the primary version of the ML model. This makes for a very high-degree of reliability and zero-downtime/loss-less rollouts of new app/ML model versions. Users do not get this level of reliability out-of-the-box with a vanilla service mesh. With Iter8, the barrier to entry for end-users is significantly reduced. In particular, by just providing names of their ML serving resources, and (optional) traffic weights/labels, end users can get started with their release optimization use cases rapidly. Further, Iter8 does not limit the capabilities of the underlying service mesh in any way. This means more advanced teams still get to use all the power of the service-mesh alongside the reliability and ease-of-use that Iter8 brings. In addition, Iter8 provides a simple metrics store, eliminating the need for an external database. Iter8 provides a client-side SDK to facilitate routing as well as metrics collection task associated with distributed (i.e., client-server architecture-based) A/B/n testing in Kubernetes. The following picture illustrates the use of the SDK for A/B testing. Iter8's SDK is designed to handle user stickiness, collection of business metrics, and decoupling of front-end and back-end releases processes during A/B/n testing. Iter8 introduces a set of tasks which which can be composed in order to conduct a variety of performance tests. The following picture illustrates a performance test for an HTTP application, and this test consists of two tasks. In addition to load testing HTTP and gRPC services, Iter8 tasks can perform other actions such as sending notifications to Slack or GitHub.","title":"Design"},{"location":"getting-started/concepts/#advantages","text":"Iter8 has several advantages compared to other tooling in this space. First, Iter8 has no restrictions on the types of resources that make up a version of an application. This includes custom resources; that is, those defined by a custom resource definition (CRD). Because the set of resources that comprise a version is declarative, it is easy to extend . Note that this same extension mechanism also allows Iter8 to be used with any service mesh. Second, the Iter8 client SDK addresses a key challenge to A/B/n testing : the decoupling of the front-end release process from that of the back-end. It allows the front-end to reliably associate business metrics with the contributing version of the back-end. Finally, Iter8 simplifies performance testing by reducing the set up time needed to start testing. Tests can be easily specified as a sequence of easily configured tasks . Further, there is no need to setup and configure an external metrics database -- Iter8 captures the metrics data and provides a REST API allowing it to be visualized and evaluated in Grafana.","title":"Advantages"},{"location":"getting-started/concepts/#implementation","text":"Iter8 is written in go and builds on a few awesome open source projects including: Helm Istio plotly.js Fortio ghz Grafana","title":"Implementation"},{"location":"getting-started/first-abn/","text":"A/B Testing with the Iter8 SDK \u00b6 This tutorial describes how to do A/B testing of a backend component using the Iter8 SDK . Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample application \u00b6 A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend Install the frontend component using an implementation in the language of your choice: node Go kubectl create deployment frontend --image = iter8/abn-sample-frontend-node:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 kubectl create deployment frontend --image = iter8/abn-sample-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call Lookup() before each call to the backend component. The frontend component uses the returned version number to route the request to the recommended version of the backend component. Deploy an initial version of the backend component: kubectl create deployment backend --image = iter8/abn-sample-backend:0.17-v1 kubectl label deployment backend iter8.tools/watch = \"true\" kubectl expose deployment backend --name = backend --port = 8091 Describe the application \u00b6 In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: deployment appName: backend action: initialize appVersions: - name: backend - name: backend-candidate-1 EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, two versions: backend and backend-candidate-1 . Each version is comprised of a Service and a Deployment . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests. Generate load \u00b6 In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s -- Deploy candidate \u00b6 Deploy the candidate version of the backend component, naming it backend-candidate-1 . kubectl create deployment backend-candidate-1 --image = iter8/abn-sample-backend:0.17-v2 kubectl label deployment backend-candidate-1 iter8.tools/watch = \"true\" kubectl expose deployment backend-candidate-1 --name = backend-candidate-1 --port = 8091 Until the candidate version is ready; that is, until all expected resources are deployed and available, calls to Lookup() will return only the version number 0 ; the existing version. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions. Compare versions using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.default:8080/abnDashboard Query string: namespace=default&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted. Promote candidate \u00b6 To promote the candidate version ( backend-candidate-1 ), first update the primary version, backend , using the new image. You can also overwrite any metadata describing the version. kubectl set image deployment/backend abn-sample-backend = iter8/abn-sample-backend:0.17-v2 Finally, delete the candidate version: kubectl delete svc/backend-candidate-1 deploy/backend-candidate-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend (currently serving the promoted version of the code). Cleanup \u00b6 Delete the sample application: kubectl delete \\ svc/frontend deploy/frontend \\ svc/backend deploy/backend \\ svc/backend-candidate-1 deploy/backend-candidate-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: deployment appName: backend action: initialize appVersions: - name: backend - name: backend-candidate-1 EOF Uninstall the Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Your first A/B test"},{"location":"getting-started/first-abn/#ab-testing-with-the-iter8-sdk","text":"This tutorial describes how to do A/B testing of a backend component using the Iter8 SDK . Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"A/B Testing with the Iter8 SDK"},{"location":"getting-started/first-abn/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"getting-started/first-abn/#deploy-the-sample-application","text":"A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend Install the frontend component using an implementation in the language of your choice: node Go kubectl create deployment frontend --image = iter8/abn-sample-frontend-node:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 kubectl create deployment frontend --image = iter8/abn-sample-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call Lookup() before each call to the backend component. The frontend component uses the returned version number to route the request to the recommended version of the backend component. Deploy an initial version of the backend component: kubectl create deployment backend --image = iter8/abn-sample-backend:0.17-v1 kubectl label deployment backend iter8.tools/watch = \"true\" kubectl expose deployment backend --name = backend --port = 8091","title":"Deploy the sample application"},{"location":"getting-started/first-abn/#describe-the-application","text":"In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: deployment appName: backend action: initialize appVersions: - name: backend - name: backend-candidate-1 EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, two versions: backend and backend-candidate-1 . Each version is comprised of a Service and a Deployment . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests.","title":"Describe the application"},{"location":"getting-started/first-abn/#generate-load","text":"In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s --","title":"Generate load"},{"location":"getting-started/first-abn/#deploy-candidate","text":"Deploy the candidate version of the backend component, naming it backend-candidate-1 . kubectl create deployment backend-candidate-1 --image = iter8/abn-sample-backend:0.17-v2 kubectl label deployment backend-candidate-1 iter8.tools/watch = \"true\" kubectl expose deployment backend-candidate-1 --name = backend-candidate-1 --port = 8091 Until the candidate version is ready; that is, until all expected resources are deployed and available, calls to Lookup() will return only the version number 0 ; the existing version. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions.","title":"Deploy candidate"},{"location":"getting-started/first-abn/#compare-versions-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.default:8080/abnDashboard Query string: namespace=default&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted.","title":"Compare versions using Grafana"},{"location":"getting-started/first-abn/#promote-candidate","text":"To promote the candidate version ( backend-candidate-1 ), first update the primary version, backend , using the new image. You can also overwrite any metadata describing the version. kubectl set image deployment/backend abn-sample-backend = iter8/abn-sample-backend:0.17-v2 Finally, delete the candidate version: kubectl delete svc/backend-candidate-1 deploy/backend-candidate-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend (currently serving the promoted version of the code).","title":"Promote candidate"},{"location":"getting-started/first-abn/#cleanup","text":"Delete the sample application: kubectl delete \\ svc/frontend deploy/frontend \\ svc/backend deploy/backend \\ svc/backend-candidate-1 deploy/backend-candidate-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: deployment appName: backend action: initialize appVersions: - name: backend - name: backend-candidate-1 EOF Uninstall the Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"getting-started/first-performance/","text":"Load test HTTP endpoint \u00b6 Run your first performance test by load testing a Kubernetes HTTP service and visualizing the performance with an Iter8 Grafana dashboard. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Launch performance test \u00b6 GET example POST example helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/get helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/post \\ --set http.payloadStr = hello About this performance test This performance test consists of two tasks , namely, ready and http . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to the cluster-local HTTP service using the specified url , and collects Iter8's built-in HTTP load test metrics . This tasks supports both GET and POST requests, and for POST requests, a payload can be provided by using either payloadStr or payloadURL . View results using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source httpbin-test with the following parameters: URL: http://iter8.default:8080/httpDashboard Query string: namespace=default&test=httpbin-test Create a new dashboard by import . Paste the contents of the http Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following: View logs \u00b6 Logs are useful for debugging. kubectl logs -l iter8.tools/test = httpbin-test Sample performance test logs time = 2023 -09-01 19 :31:40 level = info msg = task 2 : ready: started time = 2023 -09-01 19 :31:40 level = info msg = task 2 : ready: completed time = 2023 -09-01 19 :31:40 level = info msg = task 3 : http: started { \"ts\" :1693596700.013507, \"level\" : \"info\" , \"file\" : \"httprunner.go\" , \"line\" :100, \"msg\" : \"Starting http test\" , \"run\" : \"0\" , \"url\" : \"http://httpbin.default/get\" , \"threads\" : \"4\" , \"qps\" : \"8.0\" , \"warmup\" : \"parallel\" , \"conn-reuse\" : \"\" } { \"ts\" :1693596712.606946, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T001 ended after 12.534760214s : 25 calls. qps=1.9944537887591696\" } { \"ts\" :1693596712.616122, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T002 ended after 12.544591006s : 25 calls. qps=1.9928907995519867\" } { \"ts\" :1693596712.623089, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T003 ended after 12.551572714s : 25 calls. qps=1.9917822706086104\" } { \"ts\" :1693596712.628555, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T000 ended after 12.557040548s : 25 calls. qps=1.9909149695293316\" } { \"ts\" :1693596712.629567, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :564, \"msg\" : \"Run ended\" , \"run\" : \"0\" , \"elapsed\" : \"12.557657464s\" , \"calls\" : \"100\" , \"qps\" : \"7.963268649959411\" } time = 2023 -09-01 19 :31:52 level = info msg = task 3 : http: completed Cleanup \u00b6 Remove the performance test and the sample app from the Kubernetes cluster. helm delete httpbin-test kubectl delete svc/httpbin kubectl delete deploy/httpbin Uninstall the Iter8 controller \u00b6 helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Congratulations! You completed your first performance test with Iter8. Some variations and extensions of this performance test The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload.","title":"Your first performance test"},{"location":"getting-started/first-performance/#load-test-http-endpoint","text":"Run your first performance test by load testing a Kubernetes HTTP service and visualizing the performance with an Iter8 Grafana dashboard. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"Load test HTTP endpoint"},{"location":"getting-started/first-performance/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"getting-started/first-performance/#launch-performance-test","text":"GET example POST example helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/get helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.url = http://httpbin.default/post \\ --set http.payloadStr = hello About this performance test This performance test consists of two tasks , namely, ready and http . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to the cluster-local HTTP service using the specified url , and collects Iter8's built-in HTTP load test metrics . This tasks supports both GET and POST requests, and for POST requests, a payload can be provided by using either payloadStr or payloadURL .","title":"Launch performance test"},{"location":"getting-started/first-performance/#view-results-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source httpbin-test with the following parameters: URL: http://iter8.default:8080/httpDashboard Query string: namespace=default&test=httpbin-test Create a new dashboard by import . Paste the contents of the http Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following:","title":"View results using Grafana"},{"location":"getting-started/first-performance/#view-logs","text":"Logs are useful for debugging. kubectl logs -l iter8.tools/test = httpbin-test Sample performance test logs time = 2023 -09-01 19 :31:40 level = info msg = task 2 : ready: started time = 2023 -09-01 19 :31:40 level = info msg = task 2 : ready: completed time = 2023 -09-01 19 :31:40 level = info msg = task 3 : http: started { \"ts\" :1693596700.013507, \"level\" : \"info\" , \"file\" : \"httprunner.go\" , \"line\" :100, \"msg\" : \"Starting http test\" , \"run\" : \"0\" , \"url\" : \"http://httpbin.default/get\" , \"threads\" : \"4\" , \"qps\" : \"8.0\" , \"warmup\" : \"parallel\" , \"conn-reuse\" : \"\" } { \"ts\" :1693596712.606946, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T001 ended after 12.534760214s : 25 calls. qps=1.9944537887591696\" } { \"ts\" :1693596712.616122, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T002 ended after 12.544591006s : 25 calls. qps=1.9928907995519867\" } { \"ts\" :1693596712.623089, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T003 ended after 12.551572714s : 25 calls. qps=1.9917822706086104\" } { \"ts\" :1693596712.628555, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T000 ended after 12.557040548s : 25 calls. qps=1.9909149695293316\" } { \"ts\" :1693596712.629567, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :564, \"msg\" : \"Run ended\" , \"run\" : \"0\" , \"elapsed\" : \"12.557657464s\" , \"calls\" : \"100\" , \"qps\" : \"7.963268649959411\" } time = 2023 -09-01 19 :31:52 level = info msg = task 3 : http: completed","title":"View logs"},{"location":"getting-started/first-performance/#cleanup","text":"Remove the performance test and the sample app from the Kubernetes cluster. helm delete httpbin-test kubectl delete svc/httpbin kubectl delete deploy/httpbin","title":"Cleanup"},{"location":"getting-started/first-performance/#uninstall-the-iter8-controller","text":"helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Congratulations! You completed your first performance test with Iter8. Some variations and extensions of this performance test The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload.","title":"Uninstall the Iter8 controller"},{"location":"getting-started/first-routing/","text":"Automated blue-green rollout \u00b6 This tutorial shows how Iter8 can be used to implement a blue-green rollout of a Kubernetes application. In a blue-green rollout, a percentage of requests are directed to a candidate version of the application. The remaining requests go to the primary, or initial, version. Iter8 enables a blue-green rollout by automatically configuring routing resources to distribute requests. After a one-time initialization step, the end user merely deploys candidate versions, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of requests being sent to the candidate. Iter8 automatically handles all underlying routing configuration. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Install Istio . You can install the demo profile . Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Initialize primary \u00b6 Application \u00b6 Deploy the primary version of the application. In this tutorial, the application is a Kubernetes Deployment . We use httpbin as our application. Initialize the resources for the primary version ( v0 ) as follows: kubectl create deployment httpbin-0 --image = kennethreitz/httpbin --port = 80 kubectl label deployment httpbin-0 app.kubernetes.io/version = v0 kubectl label deployment httpbin-0 iter8.tools/watch = true kubectl expose deployment httpbin-0 --port = 80 About the primary Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. The label app.kubernetes.io/version is not required; we include it here as a means to distinguish between deployed versions in this tutorial. You can inspect the deployed Deployment . When the AVAILABLE field becomes 1 , the application is fully deployed. kubectl get deployment httpbin-0 Routing \u00b6 Initialize the routing resources for the application to use a blue-green rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: deployment appName: httpbin action: initialize strategy: blue-green EOF The initialize action (with strategy blue-green ) configures the (Istio) service mesh to route all requests to the primary version of the application ( httpbin-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy splits requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart . Verify routing \u00b6 To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml httpbin To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Send requests: curl httpbin.default -s -D - \\ | grep -e '^HTTP' -e app-version In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Send requests: curl -H 'Host: httpbin.default' localhost:8080 -s -D - \\ | grep -e '^HTTP' -e app-version Sample output The primary version of the application httpbin-0 will output the following: HTTP/1.1 200 OK app-version: httpbin-0 Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only the response code and this header. Deploy candidate \u00b6 Deploy a candidate model using a second Deployment : kubectl create deployment httpbin-1 --image = kennethreitz/httpbin --port = 80 kubectl label deployment httpbin-1 app.kubernetes.io/version = v1 kubectl label deployment httpbin-1 iter8.tools/watch = true kubectl expose deployment httpbin-1 --port = 80 About the candidate In this tutorial, the candidate image is the same as the one for the primary version. In a real world example, it would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Verify routing changes \u00b6 The deployment of the candidate triggers an automatic routing reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary model and the secondary model: kubectl get virtualservice httpbin -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Sample output You will see output from both the primary and candidate version of the application, httpbin-0 and httpbin-1 respectively. httpbin-0 output: HTTP/1.1 200 OK app-version: httpbin-0 httpbin-1 output: HTTP/1.1 200 OK app-version: httpbin-1 Modify weights (optional) \u00b6 You can modify the weight distribution of inference requests as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: deployment appName: httpbin action: modify-weights strategy: blue-green appVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights action overrides the default traffic split for all future candidate deployments. As above, you can verify the routing changes. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the application and deleting the candidate version. Redefine primary \u00b6 kubectl set image deployment/httpbin-0 httpbin = kennethreitz/httpbin kubectl label deployment httpbin-0 app.kubernetes.io/version = v1 --overwrite What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, the image would also have been updated. Delete candidate \u00b6 Once the primary has been redeployed, delete the candidate: kubectl delete deployment/httpbin-1 service/httpbin-1 Verify routing changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary. Cleanup \u00b6 If not already deleted, delete the candidate: kubectl delete deployment/httpbin-1 service/httpbin-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: deployment appName: httpbin action: initialize strategy: blue-green EOF Delete primary: kubectl delete deployment/httpbin-0 service/httpbin-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Your first automated routing"},{"location":"getting-started/first-routing/#automated-blue-green-rollout","text":"This tutorial shows how Iter8 can be used to implement a blue-green rollout of a Kubernetes application. In a blue-green rollout, a percentage of requests are directed to a candidate version of the application. The remaining requests go to the primary, or initial, version. Iter8 enables a blue-green rollout by automatically configuring routing resources to distribute requests. After a one-time initialization step, the end user merely deploys candidate versions, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of requests being sent to the candidate. Iter8 automatically handles all underlying routing configuration. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Install Istio . You can install the demo profile .","title":"Automated blue-green rollout"},{"location":"getting-started/first-routing/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"getting-started/first-routing/#initialize-primary","text":"","title":"Initialize primary"},{"location":"getting-started/first-routing/#application","text":"Deploy the primary version of the application. In this tutorial, the application is a Kubernetes Deployment . We use httpbin as our application. Initialize the resources for the primary version ( v0 ) as follows: kubectl create deployment httpbin-0 --image = kennethreitz/httpbin --port = 80 kubectl label deployment httpbin-0 app.kubernetes.io/version = v0 kubectl label deployment httpbin-0 iter8.tools/watch = true kubectl expose deployment httpbin-0 --port = 80 About the primary Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. The label app.kubernetes.io/version is not required; we include it here as a means to distinguish between deployed versions in this tutorial. You can inspect the deployed Deployment . When the AVAILABLE field becomes 1 , the application is fully deployed. kubectl get deployment httpbin-0","title":"Application"},{"location":"getting-started/first-routing/#routing","text":"Initialize the routing resources for the application to use a blue-green rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: deployment appName: httpbin action: initialize strategy: blue-green EOF The initialize action (with strategy blue-green ) configures the (Istio) service mesh to route all requests to the primary version of the application ( httpbin-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy splits requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart .","title":"Routing"},{"location":"getting-started/first-routing/#verify-routing","text":"To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml httpbin To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Send requests: curl httpbin.default -s -D - \\ | grep -e '^HTTP' -e app-version In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Send requests: curl -H 'Host: httpbin.default' localhost:8080 -s -D - \\ | grep -e '^HTTP' -e app-version Sample output The primary version of the application httpbin-0 will output the following: HTTP/1.1 200 OK app-version: httpbin-0 Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only the response code and this header.","title":"Verify routing"},{"location":"getting-started/first-routing/#deploy-candidate","text":"Deploy a candidate model using a second Deployment : kubectl create deployment httpbin-1 --image = kennethreitz/httpbin --port = 80 kubectl label deployment httpbin-1 app.kubernetes.io/version = v1 kubectl label deployment httpbin-1 iter8.tools/watch = true kubectl expose deployment httpbin-1 --port = 80 About the candidate In this tutorial, the candidate image is the same as the one for the primary version. In a real world example, it would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions.","title":"Deploy candidate"},{"location":"getting-started/first-routing/#verify-routing-changes","text":"The deployment of the candidate triggers an automatic routing reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary model and the secondary model: kubectl get virtualservice httpbin -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Sample output You will see output from both the primary and candidate version of the application, httpbin-0 and httpbin-1 respectively. httpbin-0 output: HTTP/1.1 200 OK app-version: httpbin-0 httpbin-1 output: HTTP/1.1 200 OK app-version: httpbin-1","title":"Verify routing changes"},{"location":"getting-started/first-routing/#modify-weights-optional","text":"You can modify the weight distribution of inference requests as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: deployment appName: httpbin action: modify-weights strategy: blue-green appVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights action overrides the default traffic split for all future candidate deployments. As above, you can verify the routing changes.","title":"Modify weights (optional)"},{"location":"getting-started/first-routing/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the application and deleting the candidate version.","title":"Promote candidate"},{"location":"getting-started/first-routing/#redefine-primary","text":"kubectl set image deployment/httpbin-0 httpbin = kennethreitz/httpbin kubectl label deployment httpbin-0 app.kubernetes.io/version = v1 --overwrite What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, the image would also have been updated.","title":"Redefine primary"},{"location":"getting-started/first-routing/#delete-candidate","text":"Once the primary has been redeployed, delete the candidate: kubectl delete deployment/httpbin-1 service/httpbin-1","title":"Delete candidate"},{"location":"getting-started/first-routing/#verify-routing-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary.","title":"Verify routing changes"},{"location":"getting-started/first-routing/#cleanup","text":"If not already deleted, delete the candidate: kubectl delete deployment/httpbin-1 service/httpbin-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: deployment appName: httpbin action: initialize strategy: blue-green EOF Delete primary: kubectl delete deployment/httpbin-0 service/httpbin-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"getting-started/help/","text":"Get Help \u00b6 Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo .","title":"Get help"},{"location":"getting-started/help/#get-help","text":"Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo .","title":"Get Help"},{"location":"getting-started/install/","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install"},{"location":"getting-started/logs/","text":"Sample performance test logs time = 2023 -09-01 19 :31:40 level = info msg = task 2 : ready: started time = 2023 -09-01 19 :31:40 level = info msg = task 2 : ready: completed time = 2023 -09-01 19 :31:40 level = info msg = task 3 : http: started { \"ts\" :1693596700.013507, \"level\" : \"info\" , \"file\" : \"httprunner.go\" , \"line\" :100, \"msg\" : \"Starting http test\" , \"run\" : \"0\" , \"url\" : \"http://httpbin.default/get\" , \"threads\" : \"4\" , \"qps\" : \"8.0\" , \"warmup\" : \"parallel\" , \"conn-reuse\" : \"\" } { \"ts\" :1693596712.606946, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T001 ended after 12.534760214s : 25 calls. qps=1.9944537887591696\" } { \"ts\" :1693596712.616122, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T002 ended after 12.544591006s : 25 calls. qps=1.9928907995519867\" } { \"ts\" :1693596712.623089, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T003 ended after 12.551572714s : 25 calls. qps=1.9917822706086104\" } { \"ts\" :1693596712.628555, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :832, \"msg\" : \"T000 ended after 12.557040548s : 25 calls. qps=1.9909149695293316\" } { \"ts\" :1693596712.629567, \"level\" : \"info\" , \"file\" : \"periodic.go\" , \"line\" :564, \"msg\" : \"Run ended\" , \"run\" : \"0\" , \"elapsed\" : \"12.557657464s\" , \"calls\" : \"100\" , \"qps\" : \"7.963268649959411\" } time = 2023 -09-01 19 :31:52 level = info msg = task 3 : http: completed","title":"Logs"},{"location":"getting-started/uninstall/","text":"helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Uninstall"},{"location":"tutorials/abn/","text":"A/B Testing with the Iter8 SDK \u00b6 Your first A/B/n test describes how to perform an A/B test of an backend component using the with the Iter8 SDK .","title":"A/B testing with Iter8 SDK"},{"location":"tutorials/abn/#ab-testing-with-the-iter8-sdk","text":"Your first A/B/n test describes how to perform an A/B test of an backend component using the with the Iter8 SDK .","title":"A/B Testing with the Iter8 SDK"},{"location":"tutorials/automated-routing/","text":"Automated blue-green rollout \u00b6 Your first automated routing describes how to implement a blue-green rollout of a Kubernetes application.","title":"Automated routing"},{"location":"tutorials/automated-routing/#automated-blue-green-rollout","text":"Your first automated routing describes how to implement a blue-green rollout of a Kubernetes application.","title":"Automated blue-green rollout"},{"location":"tutorials/load-test-grpc-multiple/","text":"Load test multiple gRPC endpoints \u00b6 Load Test gRPC describes how to load test a single method from a gRPC service inside Kubernetes. This tutorial expands on the previous tutorial and describes how to load test multiple endpoints from an HTTP service. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i \" '' \" \" '\"s/localhost//\"' \" server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Launch performance test \u00b6 helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json About this performance test This performance test consists of two tasks , namely, ready , and grpc . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to two methods of the cluster-local gRPC service, and collects Iter8's built-in gRPC load test metrics . The two methods are routeguide.RouteGuide.GetFeature and routeguide.RouteGuide.ListFeatures . Note that each method also has its own dataURL for the request payload. View results using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source routeguide-test with the following parameters: URL: http://iter8.default:8080/grpcDashboard Query string: namespace=default&test=routeguide-test Create a new dashboard by import . Paste the contents of the grpc Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following: View logs \u00b6 Logs are useful for debugging. kubectl logs -l iter8.tools/test = routeguide-test Cleanup \u00b6 Remove the performance test and the sample app from the Kubernetes cluster. helm delete routeguide-test kubectl delete svc/routeguide kubectl delete deploy/routeguide Uninstall the Iter8 controller \u00b6 helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections.","title":"Performance test multiple gRPC methods"},{"location":"tutorials/load-test-grpc-multiple/#load-test-multiple-grpc-endpoints","text":"Load Test gRPC describes how to load test a single method from a gRPC service inside Kubernetes. This tutorial expands on the previous tutorial and describes how to load test multiple endpoints from an HTTP service. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i \" '' \" \" '\"s/localhost//\"' \" server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"Load test multiple gRPC endpoints"},{"location":"tutorials/load-test-grpc-multiple/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/load-test-grpc-multiple/#launch-performance-test","text":"helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json About this performance test This performance test consists of two tasks , namely, ready , and grpc . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to two methods of the cluster-local gRPC service, and collects Iter8's built-in gRPC load test metrics . The two methods are routeguide.RouteGuide.GetFeature and routeguide.RouteGuide.ListFeatures . Note that each method also has its own dataURL for the request payload.","title":"Launch performance test"},{"location":"tutorials/load-test-grpc-multiple/#view-results-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source routeguide-test with the following parameters: URL: http://iter8.default:8080/grpcDashboard Query string: namespace=default&test=routeguide-test Create a new dashboard by import . Paste the contents of the grpc Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following:","title":"View results using Grafana"},{"location":"tutorials/load-test-grpc-multiple/#view-logs","text":"Logs are useful for debugging. kubectl logs -l iter8.tools/test = routeguide-test","title":"View logs"},{"location":"tutorials/load-test-grpc-multiple/#cleanup","text":"Remove the performance test and the sample app from the Kubernetes cluster. helm delete routeguide-test kubectl delete svc/routeguide kubectl delete deploy/routeguide","title":"Cleanup"},{"location":"tutorials/load-test-grpc-multiple/#uninstall-the-iter8-controller","text":"helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections.","title":"Uninstall the Iter8 controller"},{"location":"tutorials/load-test-grpc/","text":"Load test gRPC endpoint \u00b6 Load test a Kubernetes gRPC service and visualizing the performance metrics with an Iter8 Grafana dashboard. See Load Test multiple gRPC methods to see a tutorial that describes how to load test multiple methods from an gRPC service. Before you begin Try Your first performance test . Understand the main concepts behind. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i \" '' \" \" '\"s/localhost//\"' \" server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Launch performance test \u00b6 Unary example Server streaming example Client streaming example Bidirectional example helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RouteChat \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/bidirectional.json About this performance test This performance test consists of two tasks , namely, ready , and grpc . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to the specified method of the cluster-local gRPC service with host address routeguide.default:50051 and collects Iter8's built-in gRPC load test metrics . This task supports all four gRPC service methods: unary, server streaming, client streaming, and bidirectional streaming, and will provide payload in the appropriate manner using dataURL . View results using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source routeguide-test with the following parameters: URL: http://iter8.default:8080/grpcDashboard Query string: namespace=default&test=routeguide-test Create a new dashboard by import . Paste the contents of the grpc Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following: View logs \u00b6 Logs are useful for debugging. kubectl logs -l iter8.tools/test = routeguide-test Cleanup \u00b6 Remove the performance test and the sample app from the Kubernetes cluster. helm delete routeguide-test kubectl delete svc/routeguide kubectl delete deploy/routeguide Uninstall the Iter8 controller \u00b6 helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections.","title":"Performance test gRPC"},{"location":"tutorials/load-test-grpc/#load-test-grpc-endpoint","text":"Load test a Kubernetes gRPC service and visualizing the performance metrics with an Iter8 Grafana dashboard. See Load Test multiple gRPC methods to see a tutorial that describes how to load test multiple methods from an gRPC service. Before you begin Try Your first performance test . Understand the main concepts behind. Deploy the sample gRPC service in the Kubernetes cluster. kubectl create deployment routeguide --image = golang --port = 50051 \\ -- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i \" '' \" \" '\"s/localhost//\"' \" server/server.go; go run server/server.go\" kubectl expose deployment routeguide --port = 50051 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"Load test gRPC endpoint"},{"location":"tutorials/load-test-grpc/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/load-test-grpc/#launch-performance-test","text":"Unary example Server streaming example Client streaming example Bidirectional example helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.deploy = routeguide \\ --set ready.service = routeguide \\ --set ready.timeout = 60s \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.RouteChat \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/bidirectional.json About this performance test This performance test consists of two tasks , namely, ready , and grpc . The ready task checks if the routeguide deployment exists and is available, and the routeguide service exists. The grpc task sends call requests to the specified method of the cluster-local gRPC service with host address routeguide.default:50051 and collects Iter8's built-in gRPC load test metrics . This task supports all four gRPC service methods: unary, server streaming, client streaming, and bidirectional streaming, and will provide payload in the appropriate manner using dataURL .","title":"Launch performance test"},{"location":"tutorials/load-test-grpc/#view-results-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source routeguide-test with the following parameters: URL: http://iter8.default:8080/grpcDashboard Query string: namespace=default&test=routeguide-test Create a new dashboard by import . Paste the contents of the grpc Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following:","title":"View results using Grafana"},{"location":"tutorials/load-test-grpc/#view-logs","text":"Logs are useful for debugging. kubectl logs -l iter8.tools/test = routeguide-test","title":"View logs"},{"location":"tutorials/load-test-grpc/#cleanup","text":"Remove the performance test and the sample app from the Kubernetes cluster. helm delete routeguide-test kubectl delete svc/routeguide kubectl delete deploy/routeguide","title":"Cleanup"},{"location":"tutorials/load-test-grpc/#uninstall-the-iter8-controller","text":"helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections.","title":"Uninstall the Iter8 controller"},{"location":"tutorials/load-test-http-multiple/","text":"Load test multiple HTTP endpoints \u00b6 Your first performance test describes how to load test a HTTP service. This tutorial expands on the previous tutorial and describes how to load test multiple HTTP endpoints. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Launch performance test \u00b6 helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello About this performance test This performance test consists of two tasks , namely, ready and http . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to three endpoints from the cluster-local HTTP service, and collects Iter8's built-in HTTP load test metrics . The three endpoints are http://httpbin.default/get , http://httpbin.default/anything , and http://httpbin.default/post . The last endpoint also has a payload string hello . View results using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source httpbin-test with the following parameters: URL: http://iter8.default:8080/httpDashboard Query string: namespace=default&test=httpbin-test Create a new dashboard by import . Paste the contents of the http Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following: View logs \u00b6 Logs are useful for debugging. kubectl logs -l iter8.tools/test = httpbin-test Cleanup \u00b6 Remove the performance test and the sample app from the Kubernetes cluster. helm delete httpbin-test kubectl delete svc/httpbin kubectl delete deploy/httpbin Uninstall the Iter8 controller \u00b6 helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload.","title":"Performance test multiple HTTP endpoints"},{"location":"tutorials/load-test-http-multiple/#load-test-multiple-http-endpoints","text":"Your first performance test describes how to load test a HTTP service. This tutorial expands on the previous tutorial and describes how to load test multiple HTTP endpoints. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"Load test multiple HTTP endpoints"},{"location":"tutorials/load-test-http-multiple/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/load-test-http-multiple/#launch-performance-test","text":"helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin \\ --set ready.service = httpbin \\ --set ready.timeout = 60s \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello About this performance test This performance test consists of two tasks , namely, ready and http . The ready task checks if the httpbin deployment exists and is available, and the httpbin service exists. The http task sends requests to three endpoints from the cluster-local HTTP service, and collects Iter8's built-in HTTP load test metrics . The three endpoints are http://httpbin.default/get , http://httpbin.default/anything , and http://httpbin.default/post . The last endpoint also has a payload string hello .","title":"Launch performance test"},{"location":"tutorials/load-test-http-multiple/#view-results-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source httpbin-test with the following parameters: URL: http://iter8.default:8080/httpDashboard Query string: namespace=default&test=httpbin-test Create a new dashboard by import . Paste the contents of the http Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following:","title":"View results using Grafana"},{"location":"tutorials/load-test-http-multiple/#view-logs","text":"Logs are useful for debugging. kubectl logs -l iter8.tools/test = httpbin-test","title":"View logs"},{"location":"tutorials/load-test-http-multiple/#cleanup","text":"Remove the performance test and the sample app from the Kubernetes cluster. helm delete httpbin-test kubectl delete svc/httpbin kubectl delete deploy/httpbin","title":"Cleanup"},{"location":"tutorials/load-test-http-multiple/#uninstall-the-iter8-controller","text":"helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload.","title":"Uninstall the Iter8 controller"},{"location":"tutorials/load-test-http/","text":"Load test HTTP endpoint \u00b6 Your first performance test describes how to load test a HTTP service.","title":"Performance test HTTP"},{"location":"tutorials/load-test-http/#load-test-http-endpoint","text":"Your first performance test describes how to load test a HTTP service.","title":"Load test HTTP endpoint"},{"location":"tutorials/integrations/ghactions/","text":"Trigger a GitHub Actions workflow during a performance test \u00b6 Iter8 provides a github task that sends a repository_dispatch which can trigger the workflows in the default branch of a GitHub repository. Example \u00b6 In this example, you will run the Your first performance test but at the end of the performance test, Iter8 will trigger a workflow on GitHub. In this simple example, the workflow will simply print out a test summary that it will receive with the repository_dispatch . In a more sophisticated scenario, the workflow could, for example, read from the test summary and determine what to do next. To summarize what will happen, you will create a new GitHub repository, add a workflow that will respond to the github task, set up and run a performance test, and check if the workflow was triggered. The github task requires the name of a repository, the name of the owner, as well as an authentication token in order to send the repository_dispatch . To see a full list of the github task parameters, see here . Create a new repository on GitHub. Add the following workflow. name : iter8 `github` task test on : repository_dispatch : types : iter8 jobs : my-job : runs-on : ubuntu-latest steps : - run : 'echo \"payload: ${{ toJson(github.event.client_payload) }}\"' Note that this workflow has one job that will print out the client_payload . The default github task payload is configured with client_payload set to .Report , a summary of the performance test. Also note that the on.repository_dispatch.types is set to iter8 . The default github task payload is configured with event_type set to iter8 . This indicates that once the repository_dispatch has been sent, only workflows on the default branch with on.repository_dispatch.types set to iter8 will be triggered. Create a GitHub personal access token for the token parameter. Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Install the Iter8 controller helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the performance test using the github task with the appropriate values. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,github}\" \\ --set http.url = http://httpbin.default/get \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> Verify that the workflow has been triggered after the performance test has completed. Some variations and extensions of the github task The default github task payload sends a summary of the performance test. In your workflow, you can read from the report and use that data for control flow or use snippets of that data in different actions. For example, you can check to see if there have been any task failures and take alternative actions. You do not need to use the default github task payload . You can provide your own payload by overriding the default of the payloadTemplateURL .","title":"GitHub Actions"},{"location":"tutorials/integrations/ghactions/#trigger-a-github-actions-workflow-during-a-performance-test","text":"Iter8 provides a github task that sends a repository_dispatch which can trigger the workflows in the default branch of a GitHub repository.","title":"Trigger a GitHub Actions workflow during a performance test"},{"location":"tutorials/integrations/ghactions/#example","text":"In this example, you will run the Your first performance test but at the end of the performance test, Iter8 will trigger a workflow on GitHub. In this simple example, the workflow will simply print out a test summary that it will receive with the repository_dispatch . In a more sophisticated scenario, the workflow could, for example, read from the test summary and determine what to do next. To summarize what will happen, you will create a new GitHub repository, add a workflow that will respond to the github task, set up and run a performance test, and check if the workflow was triggered. The github task requires the name of a repository, the name of the owner, as well as an authentication token in order to send the repository_dispatch . To see a full list of the github task parameters, see here . Create a new repository on GitHub. Add the following workflow. name : iter8 `github` task test on : repository_dispatch : types : iter8 jobs : my-job : runs-on : ubuntu-latest steps : - run : 'echo \"payload: ${{ toJson(github.event.client_payload) }}\"' Note that this workflow has one job that will print out the client_payload . The default github task payload is configured with client_payload set to .Report , a summary of the performance test. Also note that the on.repository_dispatch.types is set to iter8 . The default github task payload is configured with event_type set to iter8 . This indicates that once the repository_dispatch has been sent, only workflows on the default branch with on.repository_dispatch.types set to iter8 will be triggered. Create a GitHub personal access token for the token parameter. Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Install the Iter8 controller helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the performance test using the github task with the appropriate values. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,github}\" \\ --set http.url = http://httpbin.default/get \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> Verify that the workflow has been triggered after the performance test has completed. Some variations and extensions of the github task The default github task payload sends a summary of the performance test. In your workflow, you can read from the report and use that data for control flow or use snippets of that data in different actions. For example, you can check to see if there have been any task failures and take alternative actions. You do not need to use the default github task payload . You can provide your own payload by overriding the default of the payloadTemplateURL .","title":"Example"},{"location":"tutorials/integrations/overview/","text":"Integrations \u00b6 The tutorials under the integrations section are maintained by members of the Iter8 community. They may become outdated. If you find that something is not working, please lend a helping hand and fix it in a PR. More integrations and examples are always welcome.","title":"Integrations"},{"location":"tutorials/integrations/overview/#integrations","text":"The tutorials under the integrations section are maintained by members of the Iter8 community. They may become outdated. If you find that something is not working, please lend a helping hand and fix it in a PR. More integrations and examples are always welcome.","title":"Integrations"},{"location":"tutorials/integrations/slack/","text":"Use Iter8 to send a message to a Slack channel \u00b6 Iter8 provides a slack task that sends a message to a Slack channel using a webhook . Example \u00b6 In this example, you will run the Your first performance test but at the end of the performance test, Iter8 will send a message on Slack. The message will simply contain a summary of the performance test in text form. However, you can easily construct a more sophisticated message by providing your own payload template. This task could provide important updates on a performance test over Slack, for example a summary at the end of the test. To summarize what will happen, you will create a new channel on Slack and configure a webhook, set up and run a performance test, and check if a message was sent to the channel. The slack task requires the URL of the Slack webhook. To see a full list of the github task parameters, see here . Create a new channel in your Slack organization. Create a Slack app, enable incoming webhooks, and create a new incoming webhook. See here . Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Install the Iter8 controller helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the performance test with the slack task with the appropriate values. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST Verify that the message has been sent after the performance test has completed. Some variations and extensions of the slack task The default slack task payload sends a summary of the performance test. However, you do not need to use the default payload. You can provide your own payload by overriding the default of the payloadTemplateURL . For example, you can also use Slack's Block Kit to create more sophisticated messages. You can use markdown, create different sections, or add interactivity, such as buttons.","title":"Slack"},{"location":"tutorials/integrations/slack/#use-iter8-to-send-a-message-to-a-slack-channel","text":"Iter8 provides a slack task that sends a message to a Slack channel using a webhook .","title":"Use Iter8 to send a message to a Slack channel"},{"location":"tutorials/integrations/slack/#example","text":"In this example, you will run the Your first performance test but at the end of the performance test, Iter8 will send a message on Slack. The message will simply contain a summary of the performance test in text form. However, you can easily construct a more sophisticated message by providing your own payload template. This task could provide important updates on a performance test over Slack, for example a summary at the end of the test. To summarize what will happen, you will create a new channel on Slack and configure a webhook, set up and run a performance test, and check if a message was sent to the channel. The slack task requires the URL of the Slack webhook. To see a full list of the github task parameters, see here . Create a new channel in your Slack organization. Create a Slack app, enable incoming webhooks, and create a new incoming webhook. See here . Ensure that you have a Kubernetes cluster and the kubectl CLI . You can create a local Kubernetes cluster using tools like Kind or Minikube . Install the Iter8 controller helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample HTTP service in the Kubernetes cluster. kubectl create deploy httpbin --image = kennethreitz/httpbin --port = 80 kubectl expose deploy httpbin --port = 80 Launch the performance test with the slack task with the appropriate values. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST Verify that the message has been sent after the performance test has completed. Some variations and extensions of the slack task The default slack task payload sends a summary of the performance test. However, you do not need to use the default payload. You can provide your own payload by overriding the default of the payloadTemplateURL . For example, you can also use Slack's Block Kit to create more sophisticated messages. You can use markdown, create different sections, or add interactivity, such as buttons.","title":"Example"},{"location":"tutorials/integrations/kserve/abn-grpc/","text":"A/B Testing a backend ML model \u00b6 This tutorial describes how to do A/B testing of a backend ML model hosted on KServe using the Iter8 SDK . In this tutorial, communication with the model is via gRPC calls. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample application \u00b6 A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend kubectl create deployment frontend --image = iter8/abn-sample-kserve-grpc-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call the Iter8 SDK method Lookup() before each call to the backend ML model. The frontend component uses the returned version number to route the request to the recommended version of the model. The backend application component is an ML model. Deploy the primary version of the model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"backend-0\" labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF About the primary InferenceService The base name ( backend ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies describing the application (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. Describe the application \u00b6 In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: backend action: initialize EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, a single InferenceService . Since no version-specific naming is provided, the primary version is expected to be named backend-0 and any candidate version backend-1 . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests. Generate load \u00b6 In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s -- Deploy candidate \u00b6 Deploy the candidate version of the backend model: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"backend-1\" labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Until the candidate version is ready, calls to Lookup() will return only the version number 0 ; the primary version of the model. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions. Compare versions using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.default:8080/abnDashboard Query string: namespace=default&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the ML model and deleting the candidate version. Redefine primary \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"backend-0\" labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete candidate \u00b6 Once the primary InferenceService has been redeployed, delete the candidate version: kubectl delete inferenceservice backend-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend-0 (currently serving the promoted version of the code). Cleanup \u00b6 If not already deleted, delete the candidate version of the model: kubectl delete isvc/backend-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: backend action: initialize EOF Delete the primary version of the model: kubectl delete isvc/backend-0 Delete the frontend: kubectl delete deploy/frontend svc/frontend Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"gRPC"},{"location":"tutorials/integrations/kserve/abn-grpc/#ab-testing-a-backend-ml-model","text":"This tutorial describes how to do A/B testing of a backend ML model hosted on KServe using the Iter8 SDK . In this tutorial, communication with the model is via gRPC calls. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"A/B Testing a backend ML model"},{"location":"tutorials/integrations/kserve/abn-grpc/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve/abn-grpc/#deploy-the-sample-application","text":"A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend kubectl create deployment frontend --image = iter8/abn-sample-kserve-grpc-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call the Iter8 SDK method Lookup() before each call to the backend ML model. The frontend component uses the returned version number to route the request to the recommended version of the model. The backend application component is an ML model. Deploy the primary version of the model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"backend-0\" labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF About the primary InferenceService The base name ( backend ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies describing the application (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource.","title":"Deploy the sample application"},{"location":"tutorials/integrations/kserve/abn-grpc/#describe-the-application","text":"In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: backend action: initialize EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, a single InferenceService . Since no version-specific naming is provided, the primary version is expected to be named backend-0 and any candidate version backend-1 . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests.","title":"Describe the application"},{"location":"tutorials/integrations/kserve/abn-grpc/#generate-load","text":"In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s --","title":"Generate load"},{"location":"tutorials/integrations/kserve/abn-grpc/#deploy-candidate","text":"Deploy the candidate version of the backend model: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"backend-1\" labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Until the candidate version is ready, calls to Lookup() will return only the version number 0 ; the primary version of the model. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions.","title":"Deploy candidate"},{"location":"tutorials/integrations/kserve/abn-grpc/#compare-versions-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.default:8080/abnDashboard Query string: namespace=default&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted.","title":"Compare versions using Grafana"},{"location":"tutorials/integrations/kserve/abn-grpc/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the ML model and deleting the candidate version.","title":"Promote candidate"},{"location":"tutorials/integrations/kserve/abn-grpc/#redefine-primary","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"backend-0\" labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine primary"},{"location":"tutorials/integrations/kserve/abn-grpc/#delete-candidate","text":"Once the primary InferenceService has been redeployed, delete the candidate version: kubectl delete inferenceservice backend-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend-0 (currently serving the promoted version of the code).","title":"Delete candidate"},{"location":"tutorials/integrations/kserve/abn-grpc/#cleanup","text":"If not already deleted, delete the candidate version of the model: kubectl delete isvc/backend-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: backend action: initialize EOF Delete the primary version of the model: kubectl delete isvc/backend-0 Delete the frontend: kubectl delete deploy/frontend svc/frontend Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"tutorials/integrations/kserve/abn-http/","text":"A/B Testing a backend ML model \u00b6 This tutorial describes how to do A/B testing of a backend ML model hosted on KServe using the Iter8 SDK . In this tutorial, communication with the model is via HTTP calls. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample application \u00b6 A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend kubectl create deployment frontend --image = iter8/abn-sample-kserve-http-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call the Iter8 SDK method Lookup() before each call to the backend ML model. The frontend component uses the returned version number to route the request to the recommended version of the model. The backend application component is an ML model. Deploy the primary version of the model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the primary InferenceService The base name ( backend ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies describing the application (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. Describe the application \u00b6 In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: backend action: initialize EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, a single InferenceService . Since no version-specific naming is provided, the primary version is expected to be named backend-0 and any candidate version backend-1 . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests. Generate load \u00b6 In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s -- Deploy candidate \u00b6 Deploy the candidate version of the backend model: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-1 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Until the candidate version is ready, calls to Lookup() will return only the version number 0 ; the primary version of the model. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions. Compare versions using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.default:8080/abnDashboard Query string: namespace=default&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the ML model and deleting the candidate version. Redefine primary \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete candidate \u00b6 Once the primary InferenceService has been redeployed, delete the candidate version: kubectl delete inferenceservice backend-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend-0 (currently serving the promoted version of the code). Cleanup \u00b6 If not already deleted, delete the candidate version of the model: kubectl delete isvc/backend-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: backend action: initialize EOF Delete the primary version of the model: kubectl delete isvc/backend-0 Delete the frontend: kubectl delete deploy/frontend svc/frontend Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"HTTP"},{"location":"tutorials/integrations/kserve/abn-http/#ab-testing-a-backend-ml-model","text":"This tutorial describes how to do A/B testing of a backend ML model hosted on KServe using the Iter8 SDK . In this tutorial, communication with the model is via HTTP calls. Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"A/B Testing a backend ML model"},{"location":"tutorials/integrations/kserve/abn-http/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve/abn-http/#deploy-the-sample-application","text":"A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend kubectl create deployment frontend --image = iter8/abn-sample-kserve-http-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call the Iter8 SDK method Lookup() before each call to the backend ML model. The frontend component uses the returned version number to route the request to the recommended version of the model. The backend application component is an ML model. Deploy the primary version of the model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the primary InferenceService The base name ( backend ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies describing the application (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource.","title":"Deploy the sample application"},{"location":"tutorials/integrations/kserve/abn-http/#describe-the-application","text":"In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: backend action: initialize EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, a single InferenceService . Since no version-specific naming is provided, the primary version is expected to be named backend-0 and any candidate version backend-1 . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests.","title":"Describe the application"},{"location":"tutorials/integrations/kserve/abn-http/#generate-load","text":"In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s --","title":"Generate load"},{"location":"tutorials/integrations/kserve/abn-http/#deploy-candidate","text":"Deploy the candidate version of the backend model: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-1 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Until the candidate version is ready, calls to Lookup() will return only the version number 0 ; the primary version of the model. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions.","title":"Deploy candidate"},{"location":"tutorials/integrations/kserve/abn-http/#compare-versions-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.default:8080/abnDashboard Query string: namespace=default&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted.","title":"Compare versions using Grafana"},{"location":"tutorials/integrations/kserve/abn-http/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the ML model and deleting the candidate version.","title":"Promote candidate"},{"location":"tutorials/integrations/kserve/abn-http/#redefine-primary","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine primary"},{"location":"tutorials/integrations/kserve/abn-http/#delete-candidate","text":"Once the primary InferenceService has been redeployed, delete the candidate version: kubectl delete inferenceservice backend-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend-0 (currently serving the promoted version of the code).","title":"Delete candidate"},{"location":"tutorials/integrations/kserve/abn-http/#cleanup","text":"If not already deleted, delete the candidate version of the model: kubectl delete isvc/backend-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: backend action: initialize EOF Delete the primary version of the model: kubectl delete isvc/backend-0 Delete the frontend: kubectl delete deploy/frontend svc/frontend Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"tutorials/integrations/kserve/blue-green/","text":"Blue-green rollout of a KServe ML model \u00b6 This tutorial shows how Iter8 can be used to implement a blue-green rollout of ML models hosted in a KServe environment. In a blue-green rollout, a percentage of inference requests are directed to a candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a blue-green rollout by automatically configuring routing resources to distribute inference requests. After a one-time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate model. Iter8 automatically handles all underlying routing configuration. Before you begin Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Initialize primary \u00b6 Application \u00b6 Deploy the primary version of the application. In this tutorial, the application is a KServe model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the primary The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0 Routing \u00b6 Initialize the routing resources for the application to use a blue-green rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: wisdom action: initialize strategy: blue-green EOF The initialize action (with strategy blue-green ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy splits requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart . Verify routing \u00b6 To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/knative-local-gateway 8080 :80 Download the sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/input.json Send inference requests: curl -H 'Content-Type: application/json' -H 'Host: wisdom.default' localhost:8080 -d @input.json -s -D - \\ | grep -e '^HTTP' -e app-version Sample output The primary version of the application wisdom-0 will output the following: HTTP/1.1 200 OK app-version: wisdom-0 Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only the response code and this header. Deploy candidate \u00b6 Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real world example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Verify routing changes \u00b6 The deployment of the candidate triggers an automatic routing reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Sample output You will see output from both the primary and candidate version of the application, wisdom-0 and wisdom-1 respectively. wisdom-0 output: HTTP/1.1 200 OK app-version: wisdom-0 wisdom-1 output: HTTP/1.1 200 OK app-version: wisdom-1 Modify weights (optional) \u00b6 You can modify the weight distribution of inference requests as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: wisdom action: modify-weights strategy: blue-green appVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights action overrides the default traffic split for all future candidate deployments. As above, you can verify the routing changes. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the application and deleting the candidate version. Redefine primary \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete candidate \u00b6 Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1 Verify routing changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary. Cleanup \u00b6 If not already deleted, delete the candidate: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: wisdom action: initialize strategy: blue-green EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Blue-green testing"},{"location":"tutorials/integrations/kserve/blue-green/#blue-green-rollout-of-a-kserve-ml-model","text":"This tutorial shows how Iter8 can be used to implement a blue-green rollout of ML models hosted in a KServe environment. In a blue-green rollout, a percentage of inference requests are directed to a candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a blue-green rollout by automatically configuring routing resources to distribute inference requests. After a one-time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate model. Iter8 automatically handles all underlying routing configuration. Before you begin Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash","title":"Blue-green rollout of a KServe ML model"},{"location":"tutorials/integrations/kserve/blue-green/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve/blue-green/#initialize-primary","text":"","title":"Initialize primary"},{"location":"tutorials/integrations/kserve/blue-green/#application","text":"Deploy the primary version of the application. In this tutorial, the application is a KServe model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the primary The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0","title":"Application"},{"location":"tutorials/integrations/kserve/blue-green/#routing","text":"Initialize the routing resources for the application to use a blue-green rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: wisdom action: initialize strategy: blue-green EOF The initialize action (with strategy blue-green ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy splits requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart .","title":"Routing"},{"location":"tutorials/integrations/kserve/blue-green/#verify-routing","text":"To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/knative-local-gateway 8080 :80 Download the sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/input.json Send inference requests: curl -H 'Content-Type: application/json' -H 'Host: wisdom.default' localhost:8080 -d @input.json -s -D - \\ | grep -e '^HTTP' -e app-version Sample output The primary version of the application wisdom-0 will output the following: HTTP/1.1 200 OK app-version: wisdom-0 Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only the response code and this header.","title":"Verify routing"},{"location":"tutorials/integrations/kserve/blue-green/#deploy-candidate","text":"Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real world example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions.","title":"Deploy candidate"},{"location":"tutorials/integrations/kserve/blue-green/#verify-routing-changes","text":"The deployment of the candidate triggers an automatic routing reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Sample output You will see output from both the primary and candidate version of the application, wisdom-0 and wisdom-1 respectively. wisdom-0 output: HTTP/1.1 200 OK app-version: wisdom-0 wisdom-1 output: HTTP/1.1 200 OK app-version: wisdom-1","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve/blue-green/#modify-weights-optional","text":"You can modify the weight distribution of inference requests as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: wisdom action: modify-weights strategy: blue-green appVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights action overrides the default traffic split for all future candidate deployments. As above, you can verify the routing changes.","title":"Modify weights (optional)"},{"location":"tutorials/integrations/kserve/blue-green/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the application and deleting the candidate version.","title":"Promote candidate"},{"location":"tutorials/integrations/kserve/blue-green/#redefine-primary","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine primary"},{"location":"tutorials/integrations/kserve/blue-green/#delete-candidate","text":"Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1","title":"Delete candidate"},{"location":"tutorials/integrations/kserve/blue-green/#verify-routing-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary.","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve/blue-green/#cleanup","text":"If not already deleted, delete the candidate: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: wisdom action: initialize strategy: blue-green EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"tutorials/integrations/kserve/canary/","text":"Canary rollout of a KServe ML model \u00b6 This tutorial shows how Iter8 can be used to implement a canary rollout of ML models hosted in a KServe environment. In a canary rollout, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a canary rollout by automatically configuring the routing resources to distribute inference requests. After a one-time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Iter8 automatically handles the underlying routing configuration. Before you begin Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Initialize primary \u00b6 Application \u00b6 Deploy the primary version of the application. In this tutorial, the application is a KServe model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the primary InferenceService The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0 Routing \u00b6 Initialize the routing resources for the application to use a canary rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: wisdom action: initialize strategy: canary EOF The initialize action (with strategy canary ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy sends requests with the header traffic set to the value test to the candidate version and all remaining requests to the primary version. For detailed configuration options, see the Helm chart . Verify routing \u00b6 To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh or, to send a request with header traffic: test : cat wisdom-test.sh . wisdom-test.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/knative-local-gateway 8080 :80 Download the sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/input.json Send inference requests: curl -H 'Content-Type: application/json' -H 'Host: wisdom.default' localhost:8080 -d @input.json -s -D - \\ | grep -e '^HTTP' -e app-version Or, to send a request with header traffic: test : curl -H 'Content-Type: application/json' -H 'Host: wisdom.default' localhost:8080 -d @input.json -s -D - \\ -H 'traffic: test' \\ | grep -e '^HTTP' -e app-version Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only the response code and this header. Deploy candidate \u00b6 Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real world example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Verify routing changes \u00b6 The deployment of the candidate triggers an automatic routing reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the application and deleting the candidate version. Redefine primary \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete candidate \u00b6 Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1 Verify routing changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary. Cleanup \u00b6 If not already deleted, delete the candidate model: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: wisdom action: initialize strategy: canary EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Canary testing"},{"location":"tutorials/integrations/kserve/canary/#canary-rollout-of-a-kserve-ml-model","text":"This tutorial shows how Iter8 can be used to implement a canary rollout of ML models hosted in a KServe environment. In a canary rollout, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a canary rollout by automatically configuring the routing resources to distribute inference requests. After a one-time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Iter8 automatically handles the underlying routing configuration. Before you begin Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash","title":"Canary rollout of a KServe ML model"},{"location":"tutorials/integrations/kserve/canary/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve/canary/#initialize-primary","text":"","title":"Initialize primary"},{"location":"tutorials/integrations/kserve/canary/#application","text":"Deploy the primary version of the application. In this tutorial, the application is a KServe model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the primary InferenceService The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0","title":"Application"},{"location":"tutorials/integrations/kserve/canary/#routing","text":"Initialize the routing resources for the application to use a canary rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve appName: wisdom action: initialize strategy: canary EOF The initialize action (with strategy canary ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy sends requests with the header traffic set to the value test to the candidate version and all remaining requests to the primary version. For detailed configuration options, see the Helm chart .","title":"Routing"},{"location":"tutorials/integrations/kserve/canary/#verify-routing","text":"To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh or, to send a request with header traffic: test : cat wisdom-test.sh . wisdom-test.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/knative-local-gateway 8080 :80 Download the sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/input.json Send inference requests: curl -H 'Content-Type: application/json' -H 'Host: wisdom.default' localhost:8080 -d @input.json -s -D - \\ | grep -e '^HTTP' -e app-version Or, to send a request with header traffic: test : curl -H 'Content-Type: application/json' -H 'Host: wisdom.default' localhost:8080 -d @input.json -s -D - \\ -H 'traffic: test' \\ | grep -e '^HTTP' -e app-version Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only the response code and this header.","title":"Verify routing"},{"location":"tutorials/integrations/kserve/canary/#deploy-candidate","text":"Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real world example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions.","title":"Deploy candidate"},{"location":"tutorials/integrations/kserve/canary/#verify-routing-changes","text":"The deployment of the candidate triggers an automatic routing reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary model and the secondary model: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model.","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve/canary/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the application and deleting the candidate version.","title":"Promote candidate"},{"location":"tutorials/integrations/kserve/canary/#redefine-primary","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" spec: predictor: minReplicas: 1 model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine primary"},{"location":"tutorials/integrations/kserve/canary/#delete-candidate","text":"Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1","title":"Delete candidate"},{"location":"tutorials/integrations/kserve/canary/#verify-routing-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary.","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve/canary/#cleanup","text":"If not already deleted, delete the candidate model: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve appName: wisdom action: initialize strategy: canary EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"tutorials/integrations/kserve/grpc/","text":"Load test a KServe model (via gRPC) \u00b6 This tutorial shows how easy it is to run a load test for KServe when using gRPC to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy an InferenceService \u00b6 Create an InferenceService which exposes a gRPC port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl create -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF Launch performance test \u00b6 GRPC_HOST = $( kubectl get isvc sklearn-irisv2 -o jsonpath = '{.status.components.predictor.address.url}' | sed 's#.*//##' ) GRPC_PORT = 80 helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 model-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set grpc.protoURL = https://raw.githubusercontent.com/kserve/kserve/master/docs/predict-api/v2/grpc_predict_v2.proto \\ --set grpc.host = ${ GRPC_HOST } : ${ GRPC_PORT } \\ --set grpc.call = inference.GRPCInferenceService.ModelInfer \\ --set grpc.dataURL = https://gist.githubusercontent.com/kalantar/6e9eaa03cad8f4e86b20eeb712efef45/raw/56496ed5fa9078b8c9cdad590d275ab93beaaee4/sklearn-irisv2-input-grpc.json About this performance test This performance test consists of two tasks , namely, ready and grpc . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The grpc task sends call requests to the inference.GRPCInferenceService.ModelInfer method of the cluster-local gRPC service with host address ${GRPC_HOST}:${GRPC_PORT} , and collects Iter8's built-in gRPC load test metrics. View results using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source model-test with the following parameters: URL: http://iter8.default:8080/grpcDashboard Query string: namespace=default&test=model-test Create a new dashboard by import . Paste the contents of the grpc Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following: Cleanup \u00b6 helm delete model-test kubectl delete inferenceservice sklearn-irisv2 Uninstall the Iter8 controller \u00b6 helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The grpc task can be configured with load related parameters such as the number of requests, requests per second, or number of concurrent connections.","title":"gRPC"},{"location":"tutorials/integrations/kserve/grpc/#load-test-a-kserve-model-via-grpc","text":"This tutorial shows how easy it is to run a load test for KServe when using gRPC to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"Load test a KServe model (via gRPC)"},{"location":"tutorials/integrations/kserve/grpc/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve/grpc/#deploy-an-inferenceservice","text":"Create an InferenceService which exposes a gRPC port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl create -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver protocolVersion: v2 storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" ports: - containerPort: 9000 name: h2c protocol: TCP EOF","title":"Deploy an InferenceService"},{"location":"tutorials/integrations/kserve/grpc/#launch-performance-test","text":"GRPC_HOST = $( kubectl get isvc sklearn-irisv2 -o jsonpath = '{.status.components.predictor.address.url}' | sed 's#.*//##' ) GRPC_PORT = 80 helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 model-test iter8 \\ --set \"tasks={ready,grpc}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set grpc.protoURL = https://raw.githubusercontent.com/kserve/kserve/master/docs/predict-api/v2/grpc_predict_v2.proto \\ --set grpc.host = ${ GRPC_HOST } : ${ GRPC_PORT } \\ --set grpc.call = inference.GRPCInferenceService.ModelInfer \\ --set grpc.dataURL = https://gist.githubusercontent.com/kalantar/6e9eaa03cad8f4e86b20eeb712efef45/raw/56496ed5fa9078b8c9cdad590d275ab93beaaee4/sklearn-irisv2-input-grpc.json About this performance test This performance test consists of two tasks , namely, ready and grpc . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The grpc task sends call requests to the inference.GRPCInferenceService.ModelInfer method of the cluster-local gRPC service with host address ${GRPC_HOST}:${GRPC_PORT} , and collects Iter8's built-in gRPC load test metrics.","title":"Launch performance test"},{"location":"tutorials/integrations/kserve/grpc/#view-results-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source model-test with the following parameters: URL: http://iter8.default:8080/grpcDashboard Query string: namespace=default&test=model-test Create a new dashboard by import . Paste the contents of the grpc Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following:","title":"View results using Grafana"},{"location":"tutorials/integrations/kserve/grpc/#cleanup","text":"helm delete model-test kubectl delete inferenceservice sklearn-irisv2","title":"Cleanup"},{"location":"tutorials/integrations/kserve/grpc/#uninstall-the-iter8-controller","text":"helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The grpc task can be configured with load related parameters such as the number of requests, requests per second, or number of concurrent connections.","title":"Uninstall the Iter8 controller"},{"location":"tutorials/integrations/kserve/http/","text":"Load test a KServe model (via HTTP) \u00b6 This tutorial shows how easy it is to run a load test for KServe when using HTTP to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy an InferenceService \u00b6 Create an InferenceService which exposes an HTTP port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF Launch performance test \u00b6 helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 model-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set http.url = http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer \\ --set http.payloadURL = https://gist.githubusercontent.com/kalantar/d2dd03e8ebff2c57c3cfa992b44a54ad/raw/97a0480d0dfb1deef56af73a0dd31c80dc9b71f4/sklearn-irisv2-input.json \\ --set http.contentType = \"application/json\" About this performance test This performance test consists of two tasks , namely, ready and http . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The http task sends requests to the cluster-local HTTP service whose URL exposed by the InferenceService, http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer , and collects Iter8's built-in HTTP load test metrics . View results using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source model-test with the following parameters: URL: http://iter8.default:8080/httpDashboard Query string: namespace=default&test=model-test Create a new dashboard by import . Paste the contents of the http Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following: Cleanup \u00b6 helm delete model-test kubectl delete inferenceservice sklearn-irisv2 Uninstall the Iter8 controller \u00b6 helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload.","title":"HTTP"},{"location":"tutorials/integrations/kserve/http/#load-test-a-kserve-model-via-http","text":"This tutorial shows how easy it is to run a load test for KServe when using HTTP to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. Before you begin Try Your first performance test . Understand the main concepts behind Iter8. Ensure that you have the kubectl and helm CLIs. Have access to a cluster running KServe . You can create a KServe Quickstart environment as follows: curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"Load test a KServe model (via HTTP)"},{"location":"tutorials/integrations/kserve/http/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve/http/#deploy-an-inferenceservice","text":"Create an InferenceService which exposes an HTTP port. The following serves the sklearn irisv2 model : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"sklearn-irisv2\" spec: predictor: model: modelFormat: name: sklearn runtime: kserve-mlserver storageUri: \"gs://seldon-models/sklearn/mms/lr_model\" EOF","title":"Deploy an InferenceService"},{"location":"tutorials/integrations/kserve/http/#launch-performance-test","text":"helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 model-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.isvc = sklearn-irisv2 \\ --set ready.timeout = 180s \\ --set http.url = http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer \\ --set http.payloadURL = https://gist.githubusercontent.com/kalantar/d2dd03e8ebff2c57c3cfa992b44a54ad/raw/97a0480d0dfb1deef56af73a0dd31c80dc9b71f4/sklearn-irisv2-input.json \\ --set http.contentType = \"application/json\" About this performance test This performance test consists of two tasks , namely, ready and http . The ready task checks if the sklearn-irisv2 InferenceService exists and is Ready . The http task sends requests to the cluster-local HTTP service whose URL exposed by the InferenceService, http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer , and collects Iter8's built-in HTTP load test metrics .","title":"Launch performance test"},{"location":"tutorials/integrations/kserve/http/#view-results-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana by going to http://localhost:3000 . Add a JSON API data source model-test with the following parameters: URL: http://iter8.default:8080/httpDashboard Query string: namespace=default&test=model-test Create a new dashboard by import . Paste the contents of the http Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above. The Iter8 dashboard will look like the following:","title":"View results using Grafana"},{"location":"tutorials/integrations/kserve/http/#cleanup","text":"helm delete model-test kubectl delete inferenceservice sklearn-irisv2","title":"Cleanup"},{"location":"tutorials/integrations/kserve/http/#uninstall-the-iter8-controller","text":"helm delete iter8 For additional uninstall options, see Iter8 Uninstall . Some variations and extensions of this performance test The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections. The http task can be configured to send various types of content as payload.","title":"Uninstall the Iter8 controller"},{"location":"tutorials/integrations/kserve-mm/abn/","text":"A/B Testing a backend ML model \u00b6 This tutorial describes how to do A/B testing of a backend ML model hosted on KServe ModelMesh using the Iter8 SDK . Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. If using the Quickstart environment, change your default namespace to modelmesh-serving : kubectl config set-context --current --namespace = modelmesh-serving Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000 Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Deploy the sample application \u00b6 A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend kubectl create deployment frontend --image = iter8/abn-sample-mm-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call the Iter8 SDK method Lookup() before each call to the backend ML model. The frontend component uses the returned version number to route the request to the recommended version of the model. The backend application component is an ML model. Deploy the primary version of the model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService The base name ( backend ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies describing the application (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. Describe the application \u00b6 In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: backend action: initialize EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, a single InferenceService . Since no version-specific naming is provided, the primary version is expected to be named backend-0 and any candidate version backend-1 . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests. Generate load \u00b6 In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s -- Deploy candidate \u00b6 Deploy the candidate version of the backend model: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-1 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Until the candidate version is ready, calls to Lookup() will return only the version number 0 ; the primary version of the model. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions. Compare versions using Grafana \u00b6 Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.modelmesh-serving:8080/abnDashboard Query string: namespace=modelmesh-serving&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the ML model and deleting the candidate version. Redefine primary \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete candidate \u00b6 Once the primary InferenceService has been redeployed, delete the candidate version: kubectl delete inferenceservice backend-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend-0 (currently serving the promoted version of the code). Cleanup \u00b6 If not already deleted, delete the candidate version of the model: kubectl delete isvc/backend-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve-modelmesh appName: backend action: initialize EOF Delete the primary version of the model: kubectl delete isvc/backend-0 Delete the frontend: kubectl delete deploy/frontend svc/frontend Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"A/B testing"},{"location":"tutorials/integrations/kserve-mm/abn/#ab-testing-a-backend-ml-model","text":"This tutorial describes how to do A/B testing of a backend ML model hosted on KServe ModelMesh using the Iter8 SDK . Before you begin Ensure that you have a Kubernetes cluster and the kubectl and helm CLIs. You can create a local Kubernetes cluster using tools like Kind or Minikube . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. If using the Quickstart environment, change your default namespace to modelmesh-serving : kubectl config set-context --current --namespace = modelmesh-serving Have Grafana available. For example, Grafana can be installed on your cluster as follows: kubectl create deploy grafana --image = grafana/grafana kubectl expose deploy grafana --port = 3000","title":"A/B Testing a backend ML model"},{"location":"tutorials/integrations/kserve-mm/abn/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve-mm/abn/#deploy-the-sample-application","text":"A sample application using the Iter8 SDK is provided. Deploy both the frontend and backend components of this application as described in each tab: frontend backend kubectl create deployment frontend --image = iter8/abn-sample-mm-frontend-go:0.17.3 kubectl expose deployment frontend --name = frontend --port = 8090 The frontend component is implemented to call the Iter8 SDK method Lookup() before each call to the backend ML model. The frontend component uses the returned version number to route the request to the recommended version of the model. The backend application component is an ML model. Deploy the primary version of the model using an InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService The base name ( backend ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies describing the application (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource.","title":"Deploy the sample application"},{"location":"tutorials/integrations/kserve-mm/abn/#describe-the-application","text":"In order to support Lookup() , Iter8 needs to know what the application component versions look like. A routemap is created to do this. A routemap contains a description of each version of an application and may contain routing templates . To create the routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: backend action: initialize EOF The initialize action (with strategy none ) creates a routemap that only defines the resources that make up each version of the application. In this case, a single InferenceService . Since no version-specific naming is provided, the primary version is expected to be named backend-0 and any candidate version backend-1 . Iter8 uses this information to identify when any of the versions of the application are available. It can then respond appropriately to Lookup() requests.","title":"Describe the application"},{"location":"tutorials/integrations/kserve-mm/abn/#generate-load","text":"In one shell, port-forward requests to the frontend component: kubectl port-forward service/frontend 8090 :8090 In another shell, run a script to generate load from multiple users: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/abn-sample/generate_load.sh | sh -s --","title":"Generate load"},{"location":"tutorials/integrations/kserve-mm/abn/#deploy-candidate","text":"Deploy the candidate version of the backend model: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-1 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) is the same as for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Until the candidate version is ready, calls to Lookup() will return only the version number 0 ; the primary version of the model. Once the candidate version is ready, Lookup() will return both version numbers ( 0 and 1 ) so that requests can be distributed across versions.","title":"Deploy candidate"},{"location":"tutorials/integrations/kserve-mm/abn/#compare-versions-using-grafana","text":"Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows: kubectl port-forward service/grafana 3000 :3000 Open Grafana in a browser by going to http://localhost:3000 Add a JSON API data source default/backend with the following parameters: URL: http://iter8.modelmesh-serving:8080/abnDashboard Query string: namespace=modelmesh-serving&application=backend Create a new dashboard by import . Copy and paste the contents of the abn Grafana dashboard into the text box and load it. Associate it with the JSON API data source above. The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following: Once you identify a winner, it can be promoted, and the candidate version deleted.","title":"Compare versions using Grafana"},{"location":"tutorials/integrations/kserve-mm/abn/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the ML model and deleting the candidate version.","title":"Promote candidate"},{"location":"tutorials/integrations/kserve-mm/abn/#redefine-primary","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: backend-0 labels: app.kubernetes.io/name: backend app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine primary"},{"location":"tutorials/integrations/kserve-mm/abn/#delete-candidate","text":"Once the primary InferenceService has been redeployed, delete the candidate version: kubectl delete inferenceservice backend-1 Calls to Lookup() will now recommend that all traffic be sent to the primary version backend-0 (currently serving the promoted version of the code).","title":"Delete candidate"},{"location":"tutorials/integrations/kserve-mm/abn/#cleanup","text":"If not already deleted, delete the candidate version of the model: kubectl delete isvc/backend-1 Delete the application routemap: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve-modelmesh appName: backend action: initialize EOF Delete the primary version of the model: kubectl delete isvc/backend-0 Delete the frontend: kubectl delete deploy/frontend svc/frontend Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"tutorials/integrations/kserve-mm/blue-green/","text":"Blue-green rollout of a ML model \u00b6 This tutorial shows how Iter8 can be used to implement a blue-green rollout of ML models hosted in a KServe ModelMesh Serving environment. In a blue-green rollout, a percentage of inference requests are directed to a candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a blue-green rollout by automatically configuring routing resources to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate model. Iter8 automatically handles all underlying routing configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a ModelMesh-serving Quickstart environment. If using the Quickstart environment, change your default namespace to modelmesh-serving : kubectl config set-context --current --namespace = modelmesh-serving Install Istio . You can install the demo profile . Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Initialize primary \u00b6 Application \u00b6 Deploy the primary version of the application. In this tutorial, the application is an ML model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0 Routing \u00b6 Initialize model rollout with a blue-green traffic pattern as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: blue-green EOF The initialize action (with strategy blue-green ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy splits requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart . Verify routing \u00b6 To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -vv -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer \\ | grep -e app-version Sample output The primary version of the application wisdom-0 will output the following: app-version: wisdom-0 Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only this header. Deploy candidate \u00b6 Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real world example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Verify routing changes \u00b6 The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary and candidate: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Sample output You will see output from both the primary and candidate version of the application, wisdom-0 and wisdom-1 respectively. wisdom-0 output: app-version: wisdom-0 wisdom-1 output: app-version: wisdom-1 Modify weights (optional) \u00b6 You can modify the weight distribution of inference requests as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: wisdom action: modify-weights strategy: blue-green appVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights action overrides the default traffic split for all future candidate deployments. As above, you can verify the routing changes. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the application and deleting the candidate version. Redefine primary \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete candidate \u00b6 Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1 Verify routing changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model. Cleanup \u00b6 If not already deleted, delete the candidate: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: blue-green EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Blue-green testing"},{"location":"tutorials/integrations/kserve-mm/blue-green/#blue-green-rollout-of-a-ml-model","text":"This tutorial shows how Iter8 can be used to implement a blue-green rollout of ML models hosted in a KServe ModelMesh Serving environment. In a blue-green rollout, a percentage of inference requests are directed to a candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a blue-green rollout by automatically configuring routing resources to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Optionally, the end user can modify the percentage of inference requests being sent to the candidate model. Iter8 automatically handles all underlying routing configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a ModelMesh-serving Quickstart environment. If using the Quickstart environment, change your default namespace to modelmesh-serving : kubectl config set-context --current --namespace = modelmesh-serving Install Istio . You can install the demo profile .","title":"Blue-green rollout of a ML model"},{"location":"tutorials/integrations/kserve-mm/blue-green/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve-mm/blue-green/#initialize-primary","text":"","title":"Initialize primary"},{"location":"tutorials/integrations/kserve-mm/blue-green/#application","text":"Deploy the primary version of the application. In this tutorial, the application is an ML model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0","title":"Application"},{"location":"tutorials/integrations/kserve-mm/blue-green/#routing","text":"Initialize model rollout with a blue-green traffic pattern as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: blue-green EOF The initialize action (with strategy blue-green ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy splits requests 50-50 between the primary and candidate versions. For detailed configuration options, see the Helm chart .","title":"Routing"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-routing","text":"To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -vv -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer \\ | grep -e app-version Sample output The primary version of the application wisdom-0 will output the following: app-version: wisdom-0 Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only this header.","title":"Verify routing"},{"location":"tutorials/integrations/kserve-mm/blue-green/#deploy-candidate","text":"Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real world example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions.","title":"Deploy candidate"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-routing-changes","text":"The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary and candidate: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Sample output You will see output from both the primary and candidate version of the application, wisdom-0 and wisdom-1 respectively. wisdom-0 output: app-version: wisdom-0 wisdom-1 output: app-version: wisdom-1","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve-mm/blue-green/#modify-weights-optional","text":"You can modify the weight distribution of inference requests as follows: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: wisdom action: modify-weights strategy: blue-green appVersions: - weight: 20 - weight: 80 EOF Note that using the modify-weights action overrides the default traffic split for all future candidate deployments. As above, you can verify the routing changes.","title":"Modify weights (optional)"},{"location":"tutorials/integrations/kserve-mm/blue-green/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the application and deleting the candidate version.","title":"Promote candidate"},{"location":"tutorials/integrations/kserve-mm/blue-green/#redefine-primary","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine primary"},{"location":"tutorials/integrations/kserve-mm/blue-green/#delete-candidate","text":"Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1","title":"Delete candidate"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-routing-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary model.","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve-mm/blue-green/#cleanup","text":"If not already deleted, delete the candidate: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: blue-green EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"tutorials/integrations/kserve-mm/canary/","text":"Canary rollout of a ML model \u00b6 This tutorial shows how Iter8 can be used to implement a canary rollout of ML models hosted in a KServe ModelMesh Serving environment. In a canary rollout, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a canary rollout by automatically configuring the routing resources to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Iter8 automatically handles the underlying routing configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. If using the Quickstart environment, change your default namespace to modelmesh-serving : kubectl config set-context --current --namespace = modelmesh-serving Install Istio . You can install the demo profile . Install the Iter8 controller \u00b6 helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation . Initialize primary \u00b6 Application \u00b6 Deploy the primary version of the application. In this tutorial, the application is an ML model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0 Routing \u00b6 Initialize the routing resources for the application to use a canary rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: canary EOF The initialize action (with strategy canary ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy sends requests with the header traffic set to the value test to the candidate version and all remaining requests to the primary version. For detailed configuration options, see the Helm chart . Verify routing \u00b6 To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh or, to send a request with header traffic: test : cat wisdom-test.sh . wisdom-test.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -vv -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer \\ | grep -e app-version Or, to send a request with header traffic: test : cat grpc_input.json | \\ grpcurl -vv -plaintext -proto kserve.proto -d @ \\ -H 'traffic: test' \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer \\ | grep -e app-version Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only this header. Deploy candidate \u00b6 Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions. Verify routing changes \u00b6 The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary and candidate: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model. Promote candidate \u00b6 Promoting the candidate involves redefining the primary version of the application and deleting the candidate version. Redefine primary \u00b6 cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated. Delete candidate \u00b6 Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1 Verify routing changes \u00b6 Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary. Cleanup \u00b6 If not already deleted, delete the candidate model: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: canary EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Canary testing"},{"location":"tutorials/integrations/kserve-mm/canary/#canary-rollout-of-a-ml-model","text":"This tutorial shows how Iter8 can be used to implement a canary rollout of ML models hosted in a KServe ModelMesh Serving environment. In a canary rollout, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary, or initial, version of the model. Iter8 enables a canary rollout by automatically configuring the routing resources to distribute inference requests. After a one time initialization step, the end user merely deploys candidate models, evaluates them, and either promotes or deletes them. Iter8 automatically handles the underlying routing configuration. In this tutorial, we use the Istio service mesh to distribute inference requests between different versions of a model. Before you begin Ensure that you have the kubectl CLI . Have access to a cluster running KServe ModelMesh Serving . For example, you can create a modelmesh-serving Quickstart environment. If using the Quickstart environment, change your default namespace to modelmesh-serving : kubectl config set-context --current --namespace = modelmesh-serving Install Istio . You can install the demo profile .","title":"Canary rollout of a ML model"},{"location":"tutorials/integrations/kserve-mm/canary/#install-the-iter8-controller","text":"helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true For additional install options, see Iter8 Installation .","title":"Install the Iter8 controller"},{"location":"tutorials/integrations/kserve-mm/canary/#initialize-primary","text":"","title":"Initialize primary"},{"location":"tutorials/integrations/kserve-mm/canary/#application","text":"Deploy the primary version of the application. In this tutorial, the application is an ML model. Initialize the resources for the primary version of the model ( v0 ) by deploying an InferenceService as follows: cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v0 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the primary InferenceService The base name ( wisdom ) and version ( v0 ) are identified using the labels app.kubernetes.io/name and app.kubernetes.io/version , respectively. These labels are not required. Naming the instance with the suffix -0 (and the candidate with the suffix -1 ) simplifies the routing initialization (see below). However, any name can be specified. The label iter8.tools/watch: \"true\" is required. It lets Iter8 know that it should pay attention to changes to this application resource. You can inspect the deployed InferenceService . When the READY field becomes True , the model is fully deployed. kubectl get inferenceservice wisdom-0","title":"Application"},{"location":"tutorials/integrations/kserve-mm/canary/#routing","text":"Initialize the routing resources for the application to use a canary rollout strategy: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl apply -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: canary EOF The initialize action (with strategy canary ) configures the (Istio) service mesh to route all requests to the primary version of the application ( wisdom-0 ). It further defines the routing policy that will be used when changes are observed in the application resources. By default, this routing policy sends requests with the header traffic set to the value test to the candidate version and all remaining requests to the primary version. For detailed configuration options, see the Helm chart .","title":"Routing"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-routing","text":"To verify the routing configuration, you can inspect the VirtualService : kubectl get virtualservice -o yaml wisdom To send inference requests to the model: From within the cluster From outside the cluster Create a sleep pod in the cluster from which requests can be made: curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/sleep.sh | sh - Exec into the sleep pod: kubectl exec --stdin --tty \" $( kubectl get pod --sort-by ={ metadata.creationTimestamp } -l app = sleep -o jsonpath ={ .items..metadata.name } | rev | cut -d ' ' -f 1 | rev ) \" -c sleep -- /bin/sh Make inference requests: cat wisdom.sh . wisdom.sh or, to send a request with header traffic: test : cat wisdom-test.sh . wisdom-test.sh In a separate terminal, port-forward the ingress gateway: kubectl -n istio-system port-forward svc/istio-ingressgateway 8080 :80 Download the proto file and a sample input: curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/kserve.proto curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/grpc_input.json Send inference requests: cat grpc_input.json | \\ grpcurl -vv -plaintext -proto kserve.proto -d @ \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer \\ | grep -e app-version Or, to send a request with header traffic: test : cat grpc_input.json | \\ grpcurl -vv -plaintext -proto kserve.proto -d @ \\ -H 'traffic: test' \\ -authority wisdom.modelmesh-serving \\ localhost:8080 inference.GRPCInferenceService.ModelInfer \\ | grep -e app-version Note that the model version responding to each inference request is noted in the response header app-version . In the requests above, we display only this header.","title":"Verify routing"},{"location":"tutorials/integrations/kserve-mm/canary/#deploy-candidate","text":"Deploy a candidate model using a second InferenceService : cat <<EOF | kubectl apply -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-1 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF About the candidate In this tutorial, the model source (field spec.predictor.model.storageUri ) for the candidate is the same as the one for the primary version of the model. In a real example, this would be different. The version label ( app.kubernetes.io/version ) can be used to distinguish between versions.","title":"Deploy candidate"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-routing-changes","text":"The deployment of the candidate model triggers an automatic reconfiguration by Iter8. Inspect the VirtualService to see that the routing has been changed. Requests are now distributed between the primary and candidate: kubectl get virtualservice wisdom -o yaml You can send additional inference requests as described above. They will be handled by both versions of the model.","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve-mm/canary/#promote-candidate","text":"Promoting the candidate involves redefining the primary version of the application and deleting the candidate version.","title":"Promote candidate"},{"location":"tutorials/integrations/kserve-mm/canary/#redefine-primary","text":"cat <<EOF | kubectl replace -f - apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: wisdom-0 labels: app.kubernetes.io/name: wisdom app.kubernetes.io/version: v1 iter8.tools/watch: \"true\" annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF What is different? The version label ( app.kubernetes.io/version ) was updated. In a real world example, spec.predictor.model.storageUri would also be updated.","title":"Redefine primary"},{"location":"tutorials/integrations/kserve-mm/canary/#delete-candidate","text":"Once the primary InferenceService has been redeployed, delete the candidate: kubectl delete inferenceservice wisdom-1","title":"Delete candidate"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-routing-changes_1","text":"Inspect the VirtualService to see that the it has been automatically reconfigured to send requests only to the primary.","title":"Verify routing changes"},{"location":"tutorials/integrations/kserve-mm/canary/#cleanup","text":"If not already deleted, delete the candidate model: kubectl delete isvc/wisdom-1 Delete routing: cat <<EOF | helm template routing --repo https://iter8-tools.github.io/iter8 routing-actions --version 0.18 -f - | kubectl delete -f - appType: kserve-modelmesh appName: wisdom action: initialize strategy: canary EOF Delete primary: kubectl delete isvc/wisdom-0 Uninstall Iter8 controller: helm delete iter8 For additional uninstall options, see Iter8 Uninstall .","title":"Cleanup"},{"location":"user-guide/tasks/github/","text":"github \u00b6 Trigger GitHub workflows via a repository_dispatch . A repository_dispatch will trigger workflows in the default branch of the GitHub repository. By default, an summary of the performance test will also be sent. Usage Example \u00b6 helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,github}\" \\ --set http.url = http://httpbin.default/get \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> See here for a more in-depth tutorial. Parameters \u00b6 Name Type Required Default value Description owner string Yes N/A Owner of the GitHub repository repo string Yes N/A GitHub repository token string Yes N/A Authorization token payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/iter8/v0.18.3/templates/notify/_payload-github.tpl URL to a payload template softFailure bool No true Indicates the performance test should not fail if the task cannot successfully send the request Default payload \u00b6 A repository_dispatch requires a payload that contains the type of the event. The default payload template will set the event_type to iter8 . In addition, it will also provide a performance test summary in the client_payload , which means that this data will be accessible in the GitHub workflow via ${{ toJson(github.event.client_payload) }} . However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default.","title":"github"},{"location":"user-guide/tasks/github/#github","text":"Trigger GitHub workflows via a repository_dispatch . A repository_dispatch will trigger workflows in the default branch of the GitHub repository. By default, an summary of the performance test will also be sent.","title":"github"},{"location":"user-guide/tasks/github/#usage-example","text":"helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,github}\" \\ --set http.url = http://httpbin.default/get \\ --set github.owner = <GitHub owner> \\ --set github.repo = <GitHub repository> \\ --set github.token = <GitHub token> See here for a more in-depth tutorial.","title":"Usage Example"},{"location":"user-guide/tasks/github/#parameters","text":"Name Type Required Default value Description owner string Yes N/A Owner of the GitHub repository repo string Yes N/A GitHub repository token string Yes N/A Authorization token payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/iter8/v0.18.3/templates/notify/_payload-github.tpl URL to a payload template softFailure bool No true Indicates the performance test should not fail if the task cannot successfully send the request","title":"Parameters"},{"location":"user-guide/tasks/github/#default-payload","text":"A repository_dispatch requires a payload that contains the type of the event. The default payload template will set the event_type to iter8 . In addition, it will also provide a performance test summary in the client_payload , which means that this data will be accessible in the GitHub workflow via ${{ toJson(github.event.client_payload) }} . However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default.","title":"Default payload"},{"location":"user-guide/tasks/grpc/","text":"grpc \u00b6 Generate requests for a gRPC service and and collect latency and error-related metrics. Usage example \u00b6 In this performance test, the grpc task generates call requests for a gRPC service hosted at hello.default:50051 , defined in the protobuf file located at grpc.protoURL , with a gRPC method named helloworld.Greeter.SayHello . Metrics collected by this task are viewable with a Grafana dashboard. Single method: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json Multiple methods: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json \\ --set grpc.endpoints.routeChat.call = routeguide.RouteGuide.RouteChat \\ --set grpc.endpoints.routeChat.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/bidirectional.json Parameters \u00b6 Any field in the Config struct of the ghz runner package can be used as a parameter in this task. The JSON tags of the struct fields directly correspond to the names of the parameters of this task. In the usage example , the parameters host and call correspond to the Host and Call fields respectively in the Config struct. In addition, the following fields are defined by this task. Name Type Description protoURL string (URL) URL where the protobuf file that defines the gRPC service is located. dataURL string (URL) URL where JSON data to be used in call requests is located. binaryDataURL string (URL) URL where binary data to be used in call requests is located. metadataURL string (URL) URL where the JSON metadata data to be used in call requests is located. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above as well as those from the Config struct. Load testing and metric collection will be conducted separately for each endpoint. Precedence \u00b6 Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default timeout of 20s (from Config struct). helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In the following example, the getFeature and listFeatures endpoints will use the default timeout of 20s and the recordRoute endpoint will use a timeout of 30s . helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In the following example, all three endpoints will use a qps of 40s . helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In the following example, the getFeature and listFeatures endpoints will use a timeout of 40s and the listFeatures endpoint will use a timeout of 30s . helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json \\ --set grpc.endpoints.recordRoute.timeout = 30s Further more, set parameters will trickle down to the endpoints. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.skipFirst = 5 \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In this example, all three endpoints will have a skipFirst of 5. Grafana Dashboard \u00b6 The results of the grpc task is visualized using the grpc Iter8 Grafana dashboard. The dashboard can be found here . Assuming the URL to the Grafana service is $GRAFANA_URL , you can install the dashboard as follows: Open Grafana in a browser. Add a new data JSON API data source with the following parameters URL: $GRAFANA_URL/grpcDashboard Query string: namespace=<namespace>&test=<test name> Import the grpc Iter8 Grafana dashboard Copy and paste the contents of this link into the text box You will see a visualization of the performance test like the following: For multiple endpoints, the visualization will look like the following:","title":"grpc"},{"location":"user-guide/tasks/grpc/#grpc","text":"Generate requests for a gRPC service and and collect latency and error-related metrics.","title":"grpc"},{"location":"user-guide/tasks/grpc/#usage-example","text":"In this performance test, the grpc task generates call requests for a gRPC service hosted at hello.default:50051 , defined in the protobuf file located at grpc.protoURL , with a gRPC method named helloworld.Greeter.SayHello . Metrics collected by this task are viewable with a Grafana dashboard. Single method: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.call = routeguide.RouteGuide.GetFeature \\ --set grpc.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json Multiple methods: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json \\ --set grpc.endpoints.routeChat.call = routeguide.RouteGuide.RouteChat \\ --set grpc.endpoints.routeChat.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/bidirectional.json","title":"Usage example"},{"location":"user-guide/tasks/grpc/#parameters","text":"Any field in the Config struct of the ghz runner package can be used as a parameter in this task. The JSON tags of the struct fields directly correspond to the names of the parameters of this task. In the usage example , the parameters host and call correspond to the Host and Call fields respectively in the Config struct. In addition, the following fields are defined by this task. Name Type Description protoURL string (URL) URL where the protobuf file that defines the gRPC service is located. dataURL string (URL) URL where JSON data to be used in call requests is located. binaryDataURL string (URL) URL where binary data to be used in call requests is located. metadataURL string (URL) URL where the JSON metadata data to be used in call requests is located. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above as well as those from the Config struct. Load testing and metric collection will be conducted separately for each endpoint.","title":"Parameters"},{"location":"user-guide/tasks/grpc/#precedence","text":"Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default timeout of 20s (from Config struct). helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In the following example, the getFeature and listFeatures endpoints will use the default timeout of 20s and the recordRoute endpoint will use a timeout of 30s . helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In the following example, all three endpoints will use a qps of 40s . helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In the following example, the getFeature and listFeatures endpoints will use a timeout of 40s and the listFeatures endpoint will use a timeout of 30s . helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.timeout = 40s \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json \\ --set grpc.endpoints.recordRoute.timeout = 30s Further more, set parameters will trickle down to the endpoints. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 routeguide-test iter8 \\ --set \"tasks={grpc}\" \\ --set grpc.host = routeguide.default:50051 \\ --set grpc.protoURL = https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\ --set grpc.skipFirst = 5 \\ --set grpc.endpoints.getFeature.call = routeguide.RouteGuide.GetFeature \\ --set grpc.endpoints.getFeature.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\ --set grpc.endpoints.listFeatures.call = routeguide.RouteGuide.ListFeatures \\ --set grpc.endpoints.listFeatures.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\ --set grpc.endpoints.listFeatures.timeout = 30s \\ --set grpc.endpoints.recordRoute.call = routeguide.RouteGuide.RecordRoute \\ --set grpc.endpoints.recordRoute.dataURL = https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json In this example, all three endpoints will have a skipFirst of 5.","title":"Precedence"},{"location":"user-guide/tasks/grpc/#grafana-dashboard","text":"The results of the grpc task is visualized using the grpc Iter8 Grafana dashboard. The dashboard can be found here . Assuming the URL to the Grafana service is $GRAFANA_URL , you can install the dashboard as follows: Open Grafana in a browser. Add a new data JSON API data source with the following parameters URL: $GRAFANA_URL/grpcDashboard Query string: namespace=<namespace>&test=<test name> Import the grpc Iter8 Grafana dashboard Copy and paste the contents of this link into the text box You will see a visualization of the performance test like the following: For multiple endpoints, the visualization will look like the following:","title":"Grafana Dashboard"},{"location":"user-guide/tasks/http/","text":"http \u00b6 Generate requests for an HTTP service and and collect latency and error-related metrics. Usage example \u00b6 In this performance test, the http task generates requests for https://httpbin.org/get , and collects latency and error-related metrics. Metrics collected by this task are viewable with a Grafana dashboard. Single endpoint: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.url = https://httpbin.org/get Multiple endpoints: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello Parameters \u00b6 Name Type Description url string (URL) URL where requests are sent. headers map[string]string HTTP headers to use in the requests. numRequests int Number of requests to be sent to the app. Default value is 100. duration string Duration of this task. Specified in the Go duration string format (example, 5s ). If both duration and numRequests are specified, then duration is ignored. qps float qps stands for queries-per-second. Number of requests per second sent to the app. Default value is 8.0. connections int Number of parallel connections used to send requests. Default value is 4. payloadURL string (URL) URL from which to download the content that will be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this content as the payload. payloadStr string String data to be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this string as the payload. contentType string Content type of the payload. This is intended to be used in conjunction with one of the payload* fields. If this field is specified, Iter8 will send HTTP POST requests to the app using this as the Content-Type header value. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above. Load testing and metric collection will be conducted separately for each endpoint. Precedence \u00b6 Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default qps (queries-per-second) of 8. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello In the following example, the get and getAnything endpoints will use the default qps of 8 and the post endpoint will use a qps of 15. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 In the following example, all three endpoints will use a qps (queries-per-second) of 10. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello In the following example, the get and getAnything endpoints will use a qps of 10 and the post endpoint will use a qps of 15. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 Further more, set parameters will trickle down to the endpoints. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.numRequests = 50 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello In this example, all three endpoints will have a numRequests of 50. Grafana Dashboard \u00b6 The results of the http task is visualized using the http Iter8 Grafana dashboard. The dashboard can be found here . Assuming the URL to the Grafana service is $GRAFANA_URL , you can install the dashboard as follows: Open Grafana in a browser. Add a new data JSON API data source with the following parameters URL: $GRAFANA_URL/httpDashboard Query string: namespace=<namespace>&test=<test name> Import the http Iter8 Grafana dashboard Copy and paste the contents of this link into the text box You will see a visualization of the performance test like the following: For multiple endpoints, the visualization will look like the following:","title":"http"},{"location":"user-guide/tasks/http/#http","text":"Generate requests for an HTTP service and and collect latency and error-related metrics.","title":"http"},{"location":"user-guide/tasks/http/#usage-example","text":"In this performance test, the http task generates requests for https://httpbin.org/get , and collects latency and error-related metrics. Metrics collected by this task are viewable with a Grafana dashboard. Single endpoint: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.url = https://httpbin.org/get Multiple endpoints: helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello","title":"Usage example"},{"location":"user-guide/tasks/http/#parameters","text":"Name Type Description url string (URL) URL where requests are sent. headers map[string]string HTTP headers to use in the requests. numRequests int Number of requests to be sent to the app. Default value is 100. duration string Duration of this task. Specified in the Go duration string format (example, 5s ). If both duration and numRequests are specified, then duration is ignored. qps float qps stands for queries-per-second. Number of requests per second sent to the app. Default value is 8.0. connections int Number of parallel connections used to send requests. Default value is 4. payloadURL string (URL) URL from which to download the content that will be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this content as the payload. payloadStr string String data to be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this string as the payload. contentType string Content type of the payload. This is intended to be used in conjunction with one of the payload* fields. If this field is specified, Iter8 will send HTTP POST requests to the app using this as the Content-Type header value. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The string is the name of the endpoint and the EndPoint struct includes all the parameters described above. Load testing and metric collection will be conducted separately for each endpoint.","title":"Parameters"},{"location":"user-guide/tasks/http/#precedence","text":"Some parameters have a default value, which can be overwritten. In addition, with the endpoints parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value. In the following example, all three endpoints will use the default qps (queries-per-second) of 8. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello In the following example, the get and getAnything endpoints will use the default qps of 8 and the post endpoint will use a qps of 15. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 In the following example, all three endpoints will use a qps (queries-per-second) of 10. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello In the following example, the get and getAnything endpoints will use a qps of 10 and the post endpoint will use a qps of 15. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.qps = 10 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello \\ --set http.endpoints.post.qps = 15 Further more, set parameters will trickle down to the endpoints. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.numRequests = 50 \\ --set http.endpoints.get.url = http://httpbin.default/get \\ --set http.endpoints.getAnything.url = http://httpbin.default/anything \\ --set http.endpoints.post.url = http://httpbin.default/post \\ --set http.endpoints.post.payloadStr = hello In this example, all three endpoints will have a numRequests of 50.","title":"Precedence"},{"location":"user-guide/tasks/http/#grafana-dashboard","text":"The results of the http task is visualized using the http Iter8 Grafana dashboard. The dashboard can be found here . Assuming the URL to the Grafana service is $GRAFANA_URL , you can install the dashboard as follows: Open Grafana in a browser. Add a new data JSON API data source with the following parameters URL: $GRAFANA_URL/httpDashboard Query string: namespace=<namespace>&test=<test name> Import the http Iter8 Grafana dashboard Copy and paste the contents of this link into the text box You will see a visualization of the performance test like the following: For multiple endpoints, the visualization will look like the following:","title":"Grafana Dashboard"},{"location":"user-guide/tasks/ready/","text":"ready \u00b6 Check if a Kubernetes object exists and is ready. Usage example \u00b6 In the following example, the ready task checks if a deployment named httpbin-prod exists and its availability condition is set to true, and a service named httpbin exists. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin-prod \\ --set ready.service = httpbin \\ --set http.url = http://httpbin.default/get Parameters \u00b6 Name Type Description deploy string Name of a Kubernetes deployment. The task checks if the deployment exists and its Available condition is set to true. service string Name of a Kubernetes service. The task checks if the service exists. ksvc string Name of a Knative service. The task checks if the service exists and its Ready condition is set to true. timeout string Timeout for readiness check to succeed. Default value is 60s . namespace string The namespace under which to look for the Kubernetes objects. Extensions \u00b6 Iter8 can be easily extended to support readiness checks for any type of Kubernetes object (including objects with custom resource types). To do so, add the new resource type to the list of known types defined in the default values.yaml file for the chart. Example \u00b6 To include a Knative service as part of a version definition, add the following to the map of resourceTypes in the values.yaml file used to configure the controller. The addition identifies the Kubernetes group, version, and resource (GVR) and the status condition that should be checked for readiness. ksvc : Group : serving.knative.dev Version : v1 Resource : services conditions : - Ready","title":"ready"},{"location":"user-guide/tasks/ready/#ready","text":"Check if a Kubernetes object exists and is ready.","title":"ready"},{"location":"user-guide/tasks/ready/#usage-example","text":"In the following example, the ready task checks if a deployment named httpbin-prod exists and its availability condition is set to true, and a service named httpbin exists. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={ready,http}\" \\ --set ready.deploy = httpbin-prod \\ --set ready.service = httpbin \\ --set http.url = http://httpbin.default/get","title":"Usage example"},{"location":"user-guide/tasks/ready/#parameters","text":"Name Type Description deploy string Name of a Kubernetes deployment. The task checks if the deployment exists and its Available condition is set to true. service string Name of a Kubernetes service. The task checks if the service exists. ksvc string Name of a Knative service. The task checks if the service exists and its Ready condition is set to true. timeout string Timeout for readiness check to succeed. Default value is 60s . namespace string The namespace under which to look for the Kubernetes objects.","title":"Parameters"},{"location":"user-guide/tasks/ready/#extensions","text":"Iter8 can be easily extended to support readiness checks for any type of Kubernetes object (including objects with custom resource types). To do so, add the new resource type to the list of known types defined in the default values.yaml file for the chart.","title":"Extensions"},{"location":"user-guide/tasks/ready/#example","text":"To include a Knative service as part of a version definition, add the following to the map of resourceTypes in the values.yaml file used to configure the controller. The addition identifies the Kubernetes group, version, and resource (GVR) and the status condition that should be checked for readiness. ksvc : Group : serving.knative.dev Version : v1 Resource : services conditions : - Ready","title":"Example"},{"location":"user-guide/tasks/slack/","text":"slack \u00b6 Send an performance test summary in a message to a Slack channel using a incoming webhook . Usage Example \u00b6 helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST See here for a more in-depth tutorial. Parameters \u00b6 Name Type Required Default value Description url string Yes N/A URL to the Slack webhook payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/iter8/v0.18.3/templates/notify/_payload-slack.tpl URL to a payload template softFailure bool No true Indicates the performance test should not fail if the task cannot successfully send the request Default payload \u00b6 The payload will determine what will be contained in the Slack message. The default payload template of the slack task is to send a performance test summary in text form. However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default.","title":"slack"},{"location":"user-guide/tasks/slack/#slack","text":"Send an performance test summary in a message to a Slack channel using a incoming webhook .","title":"slack"},{"location":"user-guide/tasks/slack/#usage-example","text":"helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http,slack}\" \\ --set http.url = http://httpbin.default/get \\ --set slack.url = <Slack webhook> \\ --set slack.method = POST See here for a more in-depth tutorial.","title":"Usage Example"},{"location":"user-guide/tasks/slack/#parameters","text":"Name Type Required Default value Description url string Yes N/A URL to the Slack webhook payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/iter8/v0.18.3/templates/notify/_payload-slack.tpl URL to a payload template softFailure bool No true Indicates the performance test should not fail if the task cannot successfully send the request","title":"Parameters"},{"location":"user-guide/tasks/slack/#default-payload","text":"The payload will determine what will be contained in the Slack message. The default payload template of the slack task is to send a performance test summary in text form. However, if you would like to use a different payload template, simply set a payloadTemplateURL and Iter8 will not use the default.","title":"Default payload"},{"location":"user-guide/topics/ab_testing/","text":"A/B/n Testing \u00b6 A/B/n testing relies on business metrics typically computed by a frontend, user-facing, application component. Metric values often depend on one or more interactions with backend (not user-facing) application components. To run an A/B/n test on a backend component, it is necessary to be able to associate a metric value (computed by the frontend component) to the version of the backend component that contributed to its computation. The challenge is that the frontend component often does not know which version of the backend component processed a given request. To address this challenge, Iter8 introduces an A/B/n SDK. The Iter8 SDK uses a fixed set of versions numbers ( 0 , 1 , etc.) as a way to refer to the current set of versions of a Kubernetes application or ML model. The version of the application associated with a given version number changes over time as new versions are developed, deployed for testing, and either promoted or deleted. Since the set of version numbers is fixed, they can be used to configure routing to the application. The Iter8 SDK provides two APIs to frontend application components: a. Lookup() - Given an application and user session, returns a version number to be used as an index to a table of routes. So long as there are no changes in configuration, the version number (and hence the route) will be same for the same user session, guaranteeing session stickiness. b. WriteMetric() - Given an application, a user session, a metric name its value, WriteMetric() associates the metric value with the appropriate version of the application. Configuring the Iter8 controller \u00b6 The Iter8 controller is implemented using gRPC. The service is configured to watch a given set of Kubernetes resource types. The default set of types Iter8 can watch are identified in the default values.yaml file . Other configuration options are described in the same file. To configure the specific resources to watch for a given application, a Kubernetes ConfigMap is created. It identifies the specific resources that comprise each version. For example, consider the ConfigMap : apiVersion : v1 kind : ConfigMap metadata : name : backend labels : app.kubernetes.io/managed-by : iter8 iter8.tools/kind : routemap iter8.tools/version : \"v0.18\" immutable : true data : strSpec : | versions: - resources: - gvrShort: svc name: backend namespace: default - gvrShort: deploy name: backend namespace: default - resources: - gvrShort: svc name: backend-candidate-1 namespace: default - gvrShort: deploy name: backend-candidate-1 namespace: default This ConfigMap describes an application backend . It identifies two versions of the application. The first is comprised of a Kubernetes Deployment and a Service object both named backend in the default namespace. The second is comprised of the same resource types named backend-candidate-1 in the same namespace. Deployment time configuration of backend components \u00b6 As versions of a watched application are deployed or deleted, the Iter8 controller keeps track of which versions are available enabling it to respond appropriately to Lookup() and WriteMetric() requests. Developing frontend components: Using the SDK \u00b6 The basic steps to author a frontend application component using the Iter8 SDK are outlined below for Node.js and Go . Similar steps would be required for any gRPC supported language. Use/import language specific libraries \u00b6 The gRPC protocol buffer definition is used to generate language specific implementation. These files can be used directly or packaged and imported as a library. As examples, the Node.js sample uses manually generated files directly. On the other hand, the Go sample imports the library provided by the core Iter8 service implementation. In addition to the API specific methods, some general gRPC libraries are required. Node.js Go The manually generated node files abn_pd.js and abn_grpc_pb.js used in the sample application can be copied and used without modification. var grpc = require ( '@grpc/grpc-js' ); var messages = require ( './abn_pb.js' ); var services = require ( './abn_grpc_pb.js' ); import ( \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials/insecure\" pb \"github.com/iter8-tools/iter8/abn/grpc\" ) Instantiate a gRPC client \u00b6 Instantiate a client to the Iter8 controller: Node.js Go var client = new services . ABNClient ( abnEndpoint , grpc . credentials . createInsecure ()); opts := [] grpc . DialOption { grpc . WithTransportCredentials ( insecure . NewCredentials ())} conn , err := grpc . Dial ( fmt . Sprintf ( \"%s:%s\" , getAbnService (), getAbnServicePort ()), opts ... ) if err != nil { panic ( \"Cannot establish connection with abn service\" ) } c := pb . NewABNClient ( conn ) client = & c Define routing \u00b6 Track identifiers are mapped to a static set of endpoints. One approach is to maintain a map from track identifier to endpoint: Node.js Go const versionNumberToRoute = [ \"http://backend.default.svc.cluster.local:8091\" , \"http://backend-candidate-1.default.svc.cluster.local:8091\" , ] versionNumberToRoute = [] string { \"http://backend.default.svc.cluster.local:8091\" , \"http://backend-candidate-1.default.svc.cluster.local:8091\" , } Using Lookup() \u00b6 Given a user session identifier, Lookup() returns a version number that can be used to route requests. In code sample below, the user session identifier is assumed to be passed in the X-User header of user requests. The version number is used as an index to the versionNumberToRoute map defined above. A default is used if the call to Lookup() fails for any reason. Node.js Go var application = new messages . Application (); application . setName ( 'default/backend' ); application . setUser ( req . header ( 'X-User' )); client . lookup ( application , function ( err , versionRecommendation ) { if ( err ) { // use default route (see above) console . warn ( \"error calling Lookup()\" ) } else { // use route determined by recommended track console . info ( 'lookup suggested track %d' , versionRecommendation . getVersionnumber ()) versionNumber = versionRecommendation . getVersionnumber () if ( versionNumber != NaN && 0 <= versionNumber && versionNumber < versionNumberToRoute . length ) { route = versionNumberToRoute [ versionNumber ] } } // call backend service using route ... }); route := versionNumberToRoute [ 0 ] user := req . Header [ \"X-User\" ][ 0 ] s , err := ( * client ). Lookup ( ctx , & pb . Application { Name : \"default/backend\" , User : user , }, ) if err == nil && s != nil { versionNumber := int ( s . GetVersionNumber ()) if err == nil && 0 <= versionNumber && versionNumber < len ( versionNumberToRoute ) { route = versionNumberToRoute [ versionNumber ] } // else use default value for route } // call backend service using route ... Using WriteMetric() \u00b6 As an example, a single metric named sample_metric is assigned a random value between 0 and 100 and written. Node.js Go var mv = new messages . MetricValue (); mv . setName ( 'sample_metric' ); mv . setValue ( random ({ min : 0 , max : 100 , integer : true }). toString ()); mv . setApplication ( 'default/backend' ); mv . setUser ( user ); client . writeMetric ( mv , function ( err , session ) {}); _ , _ = ( * client ). WriteMetric ( ctx , & pb . MetricValue { Name : \"sample_metric\" , Value : fmt . Sprintf ( \"%f\" , rand . Float64 () * 100.0 ), Application : \"default/backend\" , User : user , }, )","title":"A/B/n testing"},{"location":"user-guide/topics/ab_testing/#abn-testing","text":"A/B/n testing relies on business metrics typically computed by a frontend, user-facing, application component. Metric values often depend on one or more interactions with backend (not user-facing) application components. To run an A/B/n test on a backend component, it is necessary to be able to associate a metric value (computed by the frontend component) to the version of the backend component that contributed to its computation. The challenge is that the frontend component often does not know which version of the backend component processed a given request. To address this challenge, Iter8 introduces an A/B/n SDK. The Iter8 SDK uses a fixed set of versions numbers ( 0 , 1 , etc.) as a way to refer to the current set of versions of a Kubernetes application or ML model. The version of the application associated with a given version number changes over time as new versions are developed, deployed for testing, and either promoted or deleted. Since the set of version numbers is fixed, they can be used to configure routing to the application. The Iter8 SDK provides two APIs to frontend application components: a. Lookup() - Given an application and user session, returns a version number to be used as an index to a table of routes. So long as there are no changes in configuration, the version number (and hence the route) will be same for the same user session, guaranteeing session stickiness. b. WriteMetric() - Given an application, a user session, a metric name its value, WriteMetric() associates the metric value with the appropriate version of the application.","title":"A/B/n Testing"},{"location":"user-guide/topics/ab_testing/#configuring-the-iter8-controller","text":"The Iter8 controller is implemented using gRPC. The service is configured to watch a given set of Kubernetes resource types. The default set of types Iter8 can watch are identified in the default values.yaml file . Other configuration options are described in the same file. To configure the specific resources to watch for a given application, a Kubernetes ConfigMap is created. It identifies the specific resources that comprise each version. For example, consider the ConfigMap : apiVersion : v1 kind : ConfigMap metadata : name : backend labels : app.kubernetes.io/managed-by : iter8 iter8.tools/kind : routemap iter8.tools/version : \"v0.18\" immutable : true data : strSpec : | versions: - resources: - gvrShort: svc name: backend namespace: default - gvrShort: deploy name: backend namespace: default - resources: - gvrShort: svc name: backend-candidate-1 namespace: default - gvrShort: deploy name: backend-candidate-1 namespace: default This ConfigMap describes an application backend . It identifies two versions of the application. The first is comprised of a Kubernetes Deployment and a Service object both named backend in the default namespace. The second is comprised of the same resource types named backend-candidate-1 in the same namespace.","title":"Configuring the Iter8 controller"},{"location":"user-guide/topics/ab_testing/#deployment-time-configuration-of-backend-components","text":"As versions of a watched application are deployed or deleted, the Iter8 controller keeps track of which versions are available enabling it to respond appropriately to Lookup() and WriteMetric() requests.","title":"Deployment time configuration of backend components"},{"location":"user-guide/topics/ab_testing/#developing-frontend-components-using-the-sdk","text":"The basic steps to author a frontend application component using the Iter8 SDK are outlined below for Node.js and Go . Similar steps would be required for any gRPC supported language.","title":"Developing frontend components: Using the SDK"},{"location":"user-guide/topics/ab_testing/#useimport-language-specific-libraries","text":"The gRPC protocol buffer definition is used to generate language specific implementation. These files can be used directly or packaged and imported as a library. As examples, the Node.js sample uses manually generated files directly. On the other hand, the Go sample imports the library provided by the core Iter8 service implementation. In addition to the API specific methods, some general gRPC libraries are required. Node.js Go The manually generated node files abn_pd.js and abn_grpc_pb.js used in the sample application can be copied and used without modification. var grpc = require ( '@grpc/grpc-js' ); var messages = require ( './abn_pb.js' ); var services = require ( './abn_grpc_pb.js' ); import ( \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials/insecure\" pb \"github.com/iter8-tools/iter8/abn/grpc\" )","title":"Use/import language specific libraries"},{"location":"user-guide/topics/ab_testing/#instantiate-a-grpc-client","text":"Instantiate a client to the Iter8 controller: Node.js Go var client = new services . ABNClient ( abnEndpoint , grpc . credentials . createInsecure ()); opts := [] grpc . DialOption { grpc . WithTransportCredentials ( insecure . NewCredentials ())} conn , err := grpc . Dial ( fmt . Sprintf ( \"%s:%s\" , getAbnService (), getAbnServicePort ()), opts ... ) if err != nil { panic ( \"Cannot establish connection with abn service\" ) } c := pb . NewABNClient ( conn ) client = & c","title":"Instantiate a gRPC client"},{"location":"user-guide/topics/ab_testing/#define-routing","text":"Track identifiers are mapped to a static set of endpoints. One approach is to maintain a map from track identifier to endpoint: Node.js Go const versionNumberToRoute = [ \"http://backend.default.svc.cluster.local:8091\" , \"http://backend-candidate-1.default.svc.cluster.local:8091\" , ] versionNumberToRoute = [] string { \"http://backend.default.svc.cluster.local:8091\" , \"http://backend-candidate-1.default.svc.cluster.local:8091\" , }","title":"Define routing"},{"location":"user-guide/topics/ab_testing/#using-lookup","text":"Given a user session identifier, Lookup() returns a version number that can be used to route requests. In code sample below, the user session identifier is assumed to be passed in the X-User header of user requests. The version number is used as an index to the versionNumberToRoute map defined above. A default is used if the call to Lookup() fails for any reason. Node.js Go var application = new messages . Application (); application . setName ( 'default/backend' ); application . setUser ( req . header ( 'X-User' )); client . lookup ( application , function ( err , versionRecommendation ) { if ( err ) { // use default route (see above) console . warn ( \"error calling Lookup()\" ) } else { // use route determined by recommended track console . info ( 'lookup suggested track %d' , versionRecommendation . getVersionnumber ()) versionNumber = versionRecommendation . getVersionnumber () if ( versionNumber != NaN && 0 <= versionNumber && versionNumber < versionNumberToRoute . length ) { route = versionNumberToRoute [ versionNumber ] } } // call backend service using route ... }); route := versionNumberToRoute [ 0 ] user := req . Header [ \"X-User\" ][ 0 ] s , err := ( * client ). Lookup ( ctx , & pb . Application { Name : \"default/backend\" , User : user , }, ) if err == nil && s != nil { versionNumber := int ( s . GetVersionNumber ()) if err == nil && 0 <= versionNumber && versionNumber < len ( versionNumberToRoute ) { route = versionNumberToRoute [ versionNumber ] } // else use default value for route } // call backend service using route ...","title":"Using Lookup()"},{"location":"user-guide/topics/ab_testing/#using-writemetric","text":"As an example, a single metric named sample_metric is assigned a random value between 0 and 100 and written. Node.js Go var mv = new messages . MetricValue (); mv . setName ( 'sample_metric' ); mv . setValue ( random ({ min : 0 , max : 100 , integer : true }). toString ()); mv . setApplication ( 'default/backend' ); mv . setUser ( user ); client . writeMetric ( mv , function ( err , session ) {}); _ , _ = ( * client ). WriteMetric ( ctx , & pb . MetricValue { Name : \"sample_metric\" , Value : fmt . Sprintf ( \"%f\" , rand . Float64 () * 100.0 ), Application : \"default/backend\" , User : user , }, )","title":"Using WriteMetric()"},{"location":"user-guide/topics/extensions/","text":"Iter8 controller extensions \u00b6 Iter8 can be easily extended to watch any type of Kubernetes object, including objects with custom resource definitions (CRDs) as part of a application version. For example, to include a Knative service as part of a version definition, add the following to the map of resourceTypes in the values.yaml file used to configure the controller. The addition identifies the Kubernetes group, version, and resource (GVR) and the status condition that should be checked for readiness. ksvc : Group : serving.knative.dev Version : v1 Resource : services conditions : - Ready If you are using kustomize instead of helm, update the ConfigMap in a similar way.","title":"Controller extensions"},{"location":"user-guide/topics/extensions/#iter8-controller-extensions","text":"Iter8 can be easily extended to watch any type of Kubernetes object, including objects with custom resource definitions (CRDs) as part of a application version. For example, to include a Knative service as part of a version definition, add the following to the map of resourceTypes in the values.yaml file used to configure the controller. The addition identifies the Kubernetes group, version, and resource (GVR) and the status condition that should be checked for readiness. ksvc : Group : serving.knative.dev Version : v1 Resource : services conditions : - Ready If you are using kustomize instead of helm, update the ConfigMap in a similar way.","title":"Iter8 controller extensions"},{"location":"user-guide/topics/install/","text":"Iter8 can be installed and configured to watch resources either in a single namespace (namespace scoped) or in the whole cluster (cluster scoped). Install with helm \u00b6 Namespace scoped Cluster scoped helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true To install Iter8 in a non-default namespace, use the -n option. Install with kustomize \u00b6 Namespace scoped Cluster scoped kubectl apply -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/namespaceScoped?ref=v0.18.3' kubectl apply -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/clusterScoped?ref=v0.18.3' To install Iter8 in a non-default namespace, download the kustomize folder and modify the namespace field in the kustomization.yaml file. Install on OpenDataHub \u00b6 See https://github.com/opendatahub-io-contrib/odh-contrib-manifests/tree/main/iter8","title":"Install options"},{"location":"user-guide/topics/install/#install-with-helm","text":"Namespace scoped Cluster scoped helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller helm install --repo https://iter8-tools.github.io/iter8 --version 0 .18 iter8 controller \\ --set clusterScoped = true To install Iter8 in a non-default namespace, use the -n option.","title":"Install with helm"},{"location":"user-guide/topics/install/#install-with-kustomize","text":"Namespace scoped Cluster scoped kubectl apply -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/namespaceScoped?ref=v0.18.3' kubectl apply -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/clusterScoped?ref=v0.18.3' To install Iter8 in a non-default namespace, download the kustomize folder and modify the namespace field in the kustomization.yaml file.","title":"Install with kustomize"},{"location":"user-guide/topics/install/#install-on-opendatahub","text":"See https://github.com/opendatahub-io-contrib/odh-contrib-manifests/tree/main/iter8","title":"Install on OpenDataHub"},{"location":"user-guide/topics/parameters/","text":"Performance test parameters \u00b6 Iter8 is built on Helm . Performance tests can be configured with parameters using the same mechanisms provided by Helm for setting chart values . The set of configurable parameters for a performance test includes the parameters of the tasks involved in the test. Iter8 uses the convention that the parameters of a task are nested under the name of that task. In the following example, the url parameter of the http task is nested under the http object. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.url = https://httpbin.org/get All the parameters of the performance test (including of all included tasks) are optional unless otherwise documented. Parameters \u00b6 Global performance test parameters are described here. Task specific parameters are documented in each task description. Name Type Description serviceAccountName string Optional name of a service account to use. If specified, it is assumed the service account has the necessary permissions to run a performance test. If not specified, Iter8 will create a service account. logLevel string Log level. Must be one of trace , debug , info (default), warning , or error .","title":"Test parameters"},{"location":"user-guide/topics/parameters/#performance-test-parameters","text":"Iter8 is built on Helm . Performance tests can be configured with parameters using the same mechanisms provided by Helm for setting chart values . The set of configurable parameters for a performance test includes the parameters of the tasks involved in the test. Iter8 uses the convention that the parameters of a task are nested under the name of that task. In the following example, the url parameter of the http task is nested under the http object. helm upgrade --install \\ --repo https://iter8-tools.github.io/iter8 --version 0 .18 httpbin-test iter8 \\ --set \"tasks={http}\" \\ --set http.url = https://httpbin.org/get All the parameters of the performance test (including of all included tasks) are optional unless otherwise documented.","title":"Performance test parameters"},{"location":"user-guide/topics/parameters/#parameters","text":"Global performance test parameters are described here. Task specific parameters are documented in each task description. Name Type Description serviceAccountName string Optional name of a service account to use. If specified, it is assumed the service account has the necessary permissions to run a performance test. If not specified, Iter8 will create a service account. logLevel string Log level. Must be one of trace , debug , info (default), warning , or error .","title":"Parameters"},{"location":"user-guide/topics/routemap/","text":"Routemaps \u00b6 A routemap contains a description of each version of an application and may contain one or more routing templates. The description of versions is used by Iter8 to identify which versions of the application are available at any moment. Whenever versions become available or disappear, any defined routing templates will be applied. This results in the automatic reconfiguration of the routing. Version list \u00b6 A version is a list of resources that, when available and ready, indicate that the version is available. For example, the following describes a application with two versions. In this case, each version has a Service and a Deployment . In one version, they are named httpbin-0 , and in the other, httpbin-1 : versions : - resources : - gvrShort : svc name : httpbin-0 namespace : default - gvrShort : deploy name : httpbin-0 namespace : default - resources : - gvrShort : svc name : httpbin-1 namespace : default - gvrShort : deploy name : httpbin-1 namespace : default Note that the resources types are specified using a short name. In this example, svc and deploy . A short name is used to simplify the specification of the resources in a version. A mapping of short name to Kubernetes group, version, resource is captured in the configuration of the Iter8 controller. This set can be extended to include any types including custom resources; that is, those defined by a CRD. A version may optionally specify an integer weight indicating the proportion of traffic that should be sent to it relative to other versions. Routing templates \u00b6 A routing template is a Go template that is applied each time a new version becomes available or an old one goes away. Multiple templates can be defined/applied. THe application of the templates allows Iter8 to automatically reconfigure the routing when versions come and go. For example, the template created by the initialize action in the automated blue-green rollout tutorial is for an Istio VirtualService . Applying the template to the available versions yields the necessary VirtualSerivce definition. The template definition is as follows: blue-green : gvrShort : vs template : | apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin spec: gateways: - mm-gateway - mesh hosts: - httpbin.default - httpbin.default.svc - httpbin.default.svc.cluster.local http: - route: # primary version - destination: host: httpbin-0.default.svc.cluster.local {{- if gt (index .Weights 1) 0 }} weight: {{ index .Weights 0 }} {{- end }} headers: response: add: app-version: httpbin-0 # other versions {{- if gt (index .Weights 1) 0 }} - destination: host: httpbin-1.default.svc.cluster.local weight: {{ index .Weights 1 }} headers: response: add: app-version: httpbin-1 {{- end }} Implementation \u00b6 A routemap is implemented as an immutable ConfigMap with the following labels: - iter8.tools/kind with value routemap - iter8.tools/version with value corresponding the version of the controller being used The list of versions and routing templates are stored as stringified YAML.","title":"Routemaps"},{"location":"user-guide/topics/routemap/#routemaps","text":"A routemap contains a description of each version of an application and may contain one or more routing templates. The description of versions is used by Iter8 to identify which versions of the application are available at any moment. Whenever versions become available or disappear, any defined routing templates will be applied. This results in the automatic reconfiguration of the routing.","title":"Routemaps"},{"location":"user-guide/topics/routemap/#version-list","text":"A version is a list of resources that, when available and ready, indicate that the version is available. For example, the following describes a application with two versions. In this case, each version has a Service and a Deployment . In one version, they are named httpbin-0 , and in the other, httpbin-1 : versions : - resources : - gvrShort : svc name : httpbin-0 namespace : default - gvrShort : deploy name : httpbin-0 namespace : default - resources : - gvrShort : svc name : httpbin-1 namespace : default - gvrShort : deploy name : httpbin-1 namespace : default Note that the resources types are specified using a short name. In this example, svc and deploy . A short name is used to simplify the specification of the resources in a version. A mapping of short name to Kubernetes group, version, resource is captured in the configuration of the Iter8 controller. This set can be extended to include any types including custom resources; that is, those defined by a CRD. A version may optionally specify an integer weight indicating the proportion of traffic that should be sent to it relative to other versions.","title":"Version list"},{"location":"user-guide/topics/routemap/#routing-templates","text":"A routing template is a Go template that is applied each time a new version becomes available or an old one goes away. Multiple templates can be defined/applied. THe application of the templates allows Iter8 to automatically reconfigure the routing when versions come and go. For example, the template created by the initialize action in the automated blue-green rollout tutorial is for an Istio VirtualService . Applying the template to the available versions yields the necessary VirtualSerivce definition. The template definition is as follows: blue-green : gvrShort : vs template : | apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin spec: gateways: - mm-gateway - mesh hosts: - httpbin.default - httpbin.default.svc - httpbin.default.svc.cluster.local http: - route: # primary version - destination: host: httpbin-0.default.svc.cluster.local {{- if gt (index .Weights 1) 0 }} weight: {{ index .Weights 0 }} {{- end }} headers: response: add: app-version: httpbin-0 # other versions {{- if gt (index .Weights 1) 0 }} - destination: host: httpbin-1.default.svc.cluster.local weight: {{ index .Weights 1 }} headers: response: add: app-version: httpbin-1 {{- end }}","title":"Routing templates"},{"location":"user-guide/topics/routemap/#implementation","text":"A routemap is implemented as an immutable ConfigMap with the following labels: - iter8.tools/kind with value routemap - iter8.tools/version with value corresponding the version of the controller being used The list of versions and routing templates are stored as stringified YAML.","title":"Implementation"},{"location":"user-guide/topics/uninstall/","text":"Uninstall with helm \u00b6 If installed with helm , uninstall with: helm delete iter8 Uninstall with kustomize \u00b6 If installed with kustomize , uninstall with one of the following: Namespace scoped Cluster scoped kubectl delete -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/namespaceScoped?ref=v0.18.3' kubectl delete -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/clusterScoped?ref=v0.18.3'","title":"Uninstall options"},{"location":"user-guide/topics/uninstall/#uninstall-with-helm","text":"If installed with helm , uninstall with: helm delete iter8","title":"Uninstall with helm"},{"location":"user-guide/topics/uninstall/#uninstall-with-kustomize","text":"If installed with kustomize , uninstall with one of the following: Namespace scoped Cluster scoped kubectl delete -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/namespaceScoped?ref=v0.18.3' kubectl delete -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/clusterScoped?ref=v0.18.3'","title":"Uninstall with kustomize"}]}