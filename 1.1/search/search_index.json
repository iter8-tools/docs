{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Iter8","text":""},{"location":"contributing/","title":"Overview","text":"<p>Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96</p> <p>As you get started, you are in the best position to give us feedback on key areas including:</p> <ul> <li>Problems found during setup of Iter8</li> <li>Gaps in our getting started tutorial and other documentation</li> <li>Bugs in our test and automation scripts</li> </ul> <p>If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to contribute","text":"<p>We welcome many types of contributions including:</p> <ul> <li>Iter8 controller and test charts</li> <li>Docs</li> <li>CI, builds, and tests</li> <li>Reviewing pull requests</li> </ul>"},{"location":"contributing/#ask-for-help","title":"Ask for help","text":"<p>The best ways to reach us with a question is to ask...</p> <ul> <li>On the original GitHub issue</li> <li>In the <code>#development</code> channel in the Iter8 Slack workspace</li> </ul>"},{"location":"contributing/#find-or-file-an-issue","title":"Find or file an issue","text":"<p>Iter8 issues are tracked here.</p>"},{"location":"contributing/#pull-request-lifecycle","title":"Pull request lifecycle","text":"<ul> <li>Your PR is associated with one (and infrequently, with more than one) GitHub issue. You can start the submission of your PR as soon as this issue has been created.</li> <li>Follow the standard GitHub fork and pull request process when creating and submitting your PR.</li> <li>The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft, to signal that it is not yet ready for review.</li> <li>Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the <code>#development</code> Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review.</li> <li>You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days.</li> <li>Use the <code>#development</code> Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled.</li> <li>Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools.</li> </ul>"},{"location":"contributing/#sign-your-commits","title":"Sign your commits","text":"<p>Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project.</p> <p>Read GitHub's documentation on signing your commits.</p> <p>You sign-off by adding the following to your commit messages. Your sign-off must match the Git user and email associated with the commit.</p> <pre><code>This is my commit message\n\nSigned-off-by: Your Name &lt;your.name@example.com&gt;\n</code></pre> <p>Git has a <code>-s</code> command line option to do this automatically:</p> <pre><code>git commit -s -m 'This is my commit message'\n</code></pre> <p>If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running:</p> <pre><code>git commit --amend -s\n</code></pre>"},{"location":"contributing/#development-environment-setup","title":"Development environment setup","text":"<p>The Iter8 project consists of the following repos.</p> <ol> <li>iter8-tools/iter8: source for the Iter8 CLI, performance test, and controller charts</li> <li>iter8-tools/docs: source for Iter8 docs</li> </ol>"},{"location":"contributing/#iter8-toolsiter8","title":"iter8-tools/iter8","text":"<p>This is the source repo for Iter8 CLI.</p>"},{"location":"contributing/#clone-iter8","title":"Clone <code>iter8</code>","text":"<pre><code>git clone https://github.com/iter8-tools/iter8.git\n</code></pre>"},{"location":"contributing/#run-unit-tests-and-see-coverage-information","title":"Run unit tests and see coverage information","text":"<pre><code>make tests\nmake coverage\nmake htmlcov\n</code></pre>"},{"location":"contributing/#lint-iter8","title":"Lint Iter8","text":"<pre><code>make lint\n</code></pre>"},{"location":"contributing/#build-and-push-iter8-image","title":"Build and push Iter8 image","text":"<p>Define a name for your Docker image</p> <pre><code>IMG=[Docker image name]\n</code></pre> <p>Build and push Iter8 image to Docker</p> <pre><code>docker build -f docker/Dockerfile -t $IMG . \ndocker push $IMG\n</code></pre>"},{"location":"contributing/#install-controller","title":"Install controller","text":"<p>To install a development build of the controller, set the <code>image</code> property to the name of the Docker image $IMG:</p> <p><code>helm upgrade --install --repo https://iter8-tools.github.io/iter8 --version 1.1 contoller \\ --set image=$IMG</code></p>"},{"location":"contributing/#iter8-toolsdocs","title":"iter8-tools/docs","text":"<p>This is the source repo for Iter8 documentation.</p>"},{"location":"contributing/#clone-docs","title":"Clone <code>docs</code>","text":"<pre><code>git clone https://github.com/iter8-tools/docs.git\n</code></pre>"},{"location":"contributing/#locally-serve-docs","title":"Locally serve docs","text":"<p>From the root of this repo:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nmkdocs serve -s\n</code></pre> <p>You can now see your local docs at http://localhost:8000. You will also see live updates to http://localhost:8000 as you update the contents of the <code>docs</code> folder.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<ol> <li>Support multi-cluster installs</li> <li>Open Data Hub tier 1 project</li> <li>Metrics &amp; evaluation for foundation model/LLM-based apps</li> <li>Hyperparameter tuning for foundation model/LLM-based inference pipelines</li> <li>Data/concept drift detection for ML models</li> </ol>"},{"location":"community/community/","title":"Community","text":""},{"location":"community/community/#slack","title":"Slack","text":"<p>Join the Iter8 Slack for usage and development related discussions.</p>"},{"location":"community/community/#github-issues","title":"GitHub issues","text":"<p>GitHub issues for all Iter8 repositories are managed here.</p>"},{"location":"community/news/","title":"News and announcements","text":"<ul> <li> <p>September 2023: Iter8 has been accepted as a tier 2 component for Open Data Hub.</p> </li> <li> <p>June 2023: Our proposal to integrate with Open Data Hub has been accepted!</p> </li> <li> <p>March 2023: New Stack blog article by Michael Kalantar. Iter8: Simple A/B/n Testing of Kubernetes Apps, ML Models</p> </li> <li> <p>February 2023: DZone article by Alan Cha. Automated Performance Testing With ArgoCD and Iter8</p> </li> <li> <p>December 2022: DZone article by Michael Kalantar. Simplifying A/B/n Testing of Backend Services</p> </li> <li> <p>October 2022: Iter8 at KubeCon. Conference details</p> <ul> <li>Presentation by Srinivasan Parthasarathy. Video coming soon</li> <li>Lightning talk by Alan Cha. Video link here</li> </ul> </li> <li> <p>August 2022: ITNEXT article by Alan Cha. Performance testing with Iter8, now with custom metrics!</p> </li> <li> <p>August 2022: Knative blog article by Srinivasan Parthasarathy. Simple Performance Testing with SLOs</p> </li> <li> <p>June 2022: Iter8 at Open Source Summit. Video coming soon. Conference details</p> </li> <li> <p>May 2022: IBM Developer blog article by Srinivasan Parthasarathy. Dead simple benchmarking and SLO validation for Kubernetes services</p> </li> <li> <p>May 2022: New Stack blog article by Srinivasan Parthasarathy. Iter8 Unifies Performance Validation for gRPC and HTTP</p> </li> <li> <p>March 2022: New Stack blog article by Michael Kalantar. Simple Load Testing with GitHub Actions</p> </li> <li> <p>Feb 2022: New Stack blog article on Simple HTTP Load Testing with SLOs</p> </li> <li> <p>Nov 2021: Iter8 at ACM Symposium on Cloud Computing. Full paper here</p> <p></p> </li> </ul> Iter8 v0.7 and older <ul> <li>Oct 2021: New Stack blog article by Hai Huang: Progressive Delivery on OpenShift</li> </ul> <ul> <li>Oct 2021: Iter8 at PREVAIL conference. Video coming soon. Conference details</li> </ul> <ul> <li>Oct 2021: New Stack blog article by Srinivasan Parthasarathy: Validate Service-Level Objectives of REST APIs Using Iter8</li> </ul> <ul> <li>Jul 2021: Blog article by Clive Cox: ML\u200c \u200cProgressive\u200c \u200cRollouts\u200c \u200cwith\u200c \u200cSeldon\u200c \u200cand\u200c \u200cIter8\u200c</li> </ul> <ul> <li> <p>Jul 2021: Iter8 at Knative meetup</p> <p></p> </li> </ul> <ul> <li> <p>May 2021: Iter8 at KubeCon + CloudNativeCon Europe</p> <p></p> </li> </ul> <ul> <li> <p>Mar 2021: Iter8 at Knative meetup</p> <p></p> </li> </ul> <ul> <li>Mar 2021: Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing</li> </ul> <ul> <li>Oct 2020: Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8</li> </ul> <ul> <li>Oct 2020: Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application</li> </ul> <ul> <li>Oct 2020: Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes</li> </ul> <ul> <li>Oct 2020: Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood</li> </ul> <ul> <li>Aug 2020: Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control</li> </ul>"},{"location":"getting-started/advantages/","title":"Advantages","text":""},{"location":"getting-started/advantages/#use-any-resources","title":"Use any resources","text":"<p>Iter8 allows an application to be composed of resources of any type, including custom resources defined by custom resource definitions (CRDs). In Iter8, the set of resources that make up an application version is declarative making it easy to extend for new resource types. The same extension mechanism also allows Iter8 to be used with any service mesh or ingress.</p>"},{"location":"getting-started/advantages/#abn-testing-of-backend-components","title":"A/B/n testing of backend components","text":"<p>Using the Iter8 SDK enables frontend application components to reliably associate business metrics with the contributing versions of the backend. This addresses a key challenge of A/B/n testing of backend application components/ML models.</p>"},{"location":"getting-started/advantages/#simplified-performance-testing","title":"Simplified performance testing","text":"<p>Iter8 makes it easy to start running performance tests. Tests are composed with easily configurable tasks. Furthermore, there is no need to setup and configure an external metrics database -- Iter8 captures the metrics data and provides a REST API to access it, allowing it to be visualized and evaluated in Grafana.</p>"},{"location":"getting-started/advantages/#comparison-to-other-tools","title":"Comparison to other tools","text":"<p>Flagger and Argo Rollouts share similarities with Iter8. Both provide support for advanced application rollout on Kubernetes with blue-green and canary analysis. They work with many service meshes and ingress products to provide this support. Users specify the desired rollout using a Kubernetes custom resource.</p> <p>Iter8 is inspired by both projects. However, Iter8 differs in several regards. For example, with Iter8:</p> <ul> <li> <p>Applications can be composed of any resource type. For example, it works with machine learning applications built using KServe <code>InferenceService</code> resources out of the box. To do so, Iter8 allows the user to specify the resources being deployed as part of the specification of the rollout instead of assuming a particular pattern.</p> </li> <li> <p>Users can A/B/n test application backend components. Beyond providing HTTP header and cookie-based routing, Iter8 provides a client SDK with a simple API that allows users to write frontend components designed to focus A/B/n testing on the backend components.</p> </li> <li> <p>No custom resource is required to specify rollouts. Both Flagger and Argo Rollouts, requires the user to install and use a custom resource type to define rollouts. In Iter8, users specify rollouts using Helm configuration files.</p> </li> </ul>"},{"location":"getting-started/concepts/","title":"Iter8","text":"<p>Iter8 is the Kubernetes release optimizer built for DevOps, MLOps, SRE and data science teams. Iter8 automates traffic control for new versions of apps/ML models in the cluster and visualizes their performance metrics.</p>"},{"location":"getting-started/concepts/#use-cases","title":"Use cases","text":"<p>Iter8 simplifies a variety of traffic engineering and metrics-driven validation use cases. To support such use cases, Iter8 provides support for the key challenges enabling simpler implementation and quicker adoption of testing in the CD process.</p> <p>Progressive release with automated traffic management Iter8 supports blue-green, canary and mirrored releases of new applications and ML models. When new models are deployed, Iter8 automatically reconfigures the routing to desired traffic pattern. Deployment of new versions and their promotion is done by describing the desired state.</p> <p>A/B/n testing with client SDK and business metrics Iter8 addresses the challenge of doing A/B/n testing of backend application components/ML models. It provides a simple client SDK allowing a user-facing component to easily and reliably associate business metrics with the backend components that were used. This SDK provides sticky lookup based on user request headers.</p> <p>Performance testing for HTTP and gRPC endpoints To enable rapid testing, Iter8 provides synthetic load generation and notification support. A set of reusable tasks can be used to implement the desired test and notification behavior.</p>"},{"location":"getting-started/concepts/#design-principles","title":"Design Principles","text":"<p>Support all applications Iter8 does not limit what types of resources an application is composed of. It supports applications that are composed of any Kubernetes resources including those defined by custom resource definitions (CRDs). Adding support for a new resource type is both straightforward and declarative.</p> <p>Support any routing technology Progress release use cases are supported using an service mesh or ingress. Iter8 natively supports the Kubernetes Gateway API allowing easy adoption of many of these technologies. However, native interfaces can also be supported declaratively.</p> <p>Simplify user interaction Iter8 leverages Helm to allow users to declaratively specify deployment patterns and to describe test scenarios. The Helm charts provided by Iter8 minimize the barrier to entry by providing common examples. Extension is often possible just be modifying the input to the charts. However, more complicated use cases can also be supported by (user) modification of the Helm charts as well.</p> <p>Minimize Access Progressive release and A/B/n use cases require the user to install a Kubernetes controller. However, Iter8 allows for users with only namespace level access to install and use Iter8. Iter8 can also be installed and run with cluster level access.</p>"},{"location":"getting-started/concepts/#implementation-choices","title":"Implementation Choices","text":""},{"location":"getting-started/concepts/#iter8-controller","title":"Iter8 controller","text":"<p>Iter8 is a controller based architecture. It monitors the Kubernetes objects that make up versions of an application/ML model and automatically reconfigures the routing resources based on their state. In this way, a user just deploys and evaluates versions; she does not con</p> <p>The following picture illustrates a blue-green rollout scenario that is orchestrated by this controller.</p> <p></p> <p>As part of the dynamic reconfiguration of route resources, the Iter8 controller also checks for readiness (for e.g., in KServe ModelMesh), availability (for e.g., in Kubernetes deployments) and other relevant status conditions before configuring traffic splits to candidate versions. Similarly, before candidate versions are deleted, the Iter8 controller uses finalizers to first ensure that all traffic flows to the primary version of the ML model. This makes for a very high-degree of reliability and zero-downtime/loss-less rollouts of new app/ML model versions. Users do not get this level of reliability out-of-the-box with a vanilla service mesh.</p> <p>With Iter8, the barrier to entry for end-users is significantly reduced. In particular, by just providing names of their ML serving resources, and (optional) traffic weights/labels, end users can get started with their release optimization use cases rapidly. Further, Iter8 does not limit the capabilities of the underlying service mesh in any way. This means more advanced teams still get to use all the power of the service-mesh alongside the reliability and ease-of-use that Iter8 brings.</p>"},{"location":"getting-started/concepts/#client-sdk","title":"Client SDK","text":"<p>Iter8 provides a client-side SDK to facilitate routing as well as a metrics collection task associated with distributed (i.e., client-server architecture-based) A/B/n testing in Kubernetes. </p> <p>The following picture illustrates the use of the SDK for A/B testing.</p> <p></p> <p>Iter8's SDK is designed to handle user stickiness, collection of business metrics, and decoupling of front-end and back-end releases processes during A/B/n testing.</p> <p>In addition, Iter8 provides a simple metrics store, eliminating the need for an external database.</p>"},{"location":"getting-started/concepts/#performance-testing-tasks","title":"Performance testing tasks","text":"<p>Iter8 introduces a set of tasks which can be composed in order to conduct a variety of performance tests.</p> <p>The following picture illustrates a performance test for an HTTP application, and this test consists of two tasks. One verifies the application is ready and the second generates synthetic HTTP load against the application and captures the resulting performance metrics.</p> <p></p> <p>In addition to load testing HTTP and gRPC services, Iter8 tasks can perform other actions such as sending notifications to Slack or GitHub.</p>"},{"location":"getting-started/concepts/#built-using-open-source","title":"Built using open source","text":"<p>Iter8 is written in <code>go</code> and builds on a few awesome open source projects including:</p> <ul> <li>Helm</li> <li>Istio</li> <li>Kubernetes Gateway API</li> <li>Fortio</li> <li>ghz</li> <li>Grafana</li> </ul>"},{"location":"getting-started/first-abn/","title":"A/B Testing with the Iter8 SDK","text":"<p>This tutorial describes how to do A/B testing of a backend component using the Iter8 SDK. </p> <p></p> Before you begin <ol> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> and <code>helm</code> CLIs. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"getting-started/first-abn/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"getting-started/first-abn/#deploy-the-sample-application","title":"Deploy the sample application","text":"<p>A simple sample two-tier application using the Iter8 SDK is provided. Note that only the frontend component uses the Iter8 SDK. Deploy both the frontend and backend components of this application as described in each tab:</p> FrontendBackend <p>Install the frontend component using an implementation in the language of your choice:</p> NodeGo <pre><code>kubectl create deployment frontend --image=iter8/abn-sample-frontend-node:0.17.3\nkubectl expose deployment frontend --name=frontend --port=8090\n</code></pre> <pre><code>kubectl create deployment frontend --image=iter8/abn-sample-frontend-go:0.17.3\nkubectl expose deployment frontend --name=frontend --port=8090\n</code></pre> <p>The frontend component is implemented to call <code>Lookup()</code> before each call to the backend component. The frontend component uses the returned version number to route the request to the recommended version of the backend component.</p> <p>Release an initial version of the backend named <code>backend</code>:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment\napplication: \n  port: 8091\n  versions:\n  - metadata:\n      name: backend\n    image: iter8/abn-sample-backend:0.17-v1\nEOF\n</code></pre>"},{"location":"getting-started/first-abn/#generate-load","title":"Generate load","text":"<p>In one shell, port-forward requests to the frontend component:     <pre><code>kubectl port-forward service/frontend 8090:8090\n</code></pre> In another shell, run a script to generate load from multiple users:     <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.3/samples/abn-sample/generate_load.sh | sh -s --\n</code></pre></p> <p>The load generator and sample frontend application outputs the backend that handled each recommendation. With just one version is deployed, all requests are handled by <code>backend-0</code>. In the output you will see something like:</p> <pre><code>Recommendation: {\"Id\":19,\"Name\":\"sample\",\"Source\":\"backend-74ff88c76d-nb87j\"}\n</code></pre>"},{"location":"getting-started/first-abn/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the backend component can be deployed simply by adding a second version to the list of versions:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment\napplication: \n  port: 8091\n  versions:\n  - metadata:\n      name: backend\n    image: iter8/abn-sample-backend:0.17-v1\n  - metadata:\n      name: backend-candidate-1\n    image: iter8/abn-sample-backend:0.17-v2\nEOF\n</code></pre> <p>While the candidate version is deploying, <code>Lookup()</code> will return only the version index number <code>0</code>; that is, the first, or primary, version of the model. Once the candidate version is ready, <code>Lookup()</code> will return both <code>0</code> and <code>1</code>, the indices of both versions, so that requests can be distributed across both versions.</p> <p>Once both backend versions are responding to requests, the output of the load generator will include recommendations from the candidate version. In this example, you should see something like:</p> <pre><code>Recommendation: {\"Id\":19,\"Name\":\"sample\",\"Source\":\"backend-candidate-1-56cb7cd5cf-bkrjv\"}\n</code></pre>"},{"location":"getting-started/first-abn/#compare-versions-using-grafana","title":"Compare versions using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>default/backend</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/abnDashboard</code></li> <li>Query string: <code>namespace=default&amp;application=backend</code></li> </ul> <p>Create a new dashboard by import. Copy and paste the contents of the <code>abn</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source above.</p> <p>The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following:</p> <p></p> <p>Once you identify a winner, it can be promoted, and the candidate version deleted.</p>"},{"location":"getting-started/first-abn/#promote-candidate","title":"Promote candidate","text":"<p>To promote the candidate version (<code>backend-candidate-1</code>), re-release the application, updating the image of the primary (the first) version to use the image of the candidate version and remove the candidate version:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment\napplication: \n  port: 8091\n  versions:\n  - metadata:\n      name: backend\n    image: iter8/abn-sample-backend:0.17-v2\nEOF\n</code></pre> <p>Calls to <code>Lookup()</code> will now recommend that all traffic be sent to the new primary version <code>backend</code> (currently serving the promoted version of the code).</p> <p>The output of the load generator will again show just <code>backend_0</code>:</p> <pre><code>Recommendation: {\"Id\":19,\"Name\":\"sample\",\"Source\":\"backend-74ff88c76d-nb87j\"}\n</code></pre>"},{"location":"getting-started/first-abn/#cleanup","title":"Cleanup","text":"<p>Delete the sample application:</p> <pre><code>kubectl delete svc/frontend deploy/frontend\nhelm delete backend\n</code></pre> <p>Uninstall the Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre> <p>Congratulations!  You completed your first A/B test with Iter8.</p>"},{"location":"getting-started/first-performance/","title":"Load test HTTP endpoint","text":"<p>Run your first performance test by load testing a Kubernetes HTTP service and visualizing the performance with an Iter8 Grafana dashboard.</p> <p></p> Before you begin <ol> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> and <code>helm</code> CLIs. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Deploy the sample HTTP service in the Kubernetes cluster. <pre><code>kubectl create deploy httpbin --image=kennethreitz/httpbin --port=80\nkubectl expose deploy httpbin --port=80\n</code></pre></li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"getting-started/first-performance/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"getting-started/first-performance/#launch-performance-test","title":"Launch performance test","text":"GET examplePOST example <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={ready,http}\" \\\n--set ready.deploy=httpbin \\\n--set ready.service=httpbin \\\n--set ready.timeout=60s \\\n--set http.url=http://httpbin.default/get\n</code></pre> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={ready,http}\" \\\n--set ready.deploy=httpbin \\\n--set ready.service=httpbin \\\n--set ready.timeout=60s \\\n--set http.url=http://httpbin.default/post \\\n--set http.payloadStr=hello\n</code></pre> About this performance test <p>This performance test consists of two tasks, namely, ready and http. </p> <p>The ready task checks if the <code>httpbin</code> deployment exists and is available, and the <code>httpbin</code> service exists. </p> <p>The http task sends requests to the cluster-local HTTP service using the specified <code>url</code>, and collects Iter8's built-in HTTP load test metrics. This tasks supports both GET and POST requests, and for POST requests, a payload can be provided by using either <code>payloadStr</code> or <code>payloadURL</code>.</p>"},{"location":"getting-started/first-performance/#view-results-using-grafana","title":"View results using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>httpbin-test</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/httpDashboard</code> </li> <li>Query string: <code>namespace=default&amp;test=httpbin-test</code></li> </ul> <p>Create a new dashboard by import. Paste the contents of the <code>http</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above.</p> <p>The Iter8 dashboard will look like the following:</p> <p></p>"},{"location":"getting-started/first-performance/#view-logs","title":"View logs","text":"<p>Logs are useful for debugging. To see the test logs:</p> <pre><code>kubectl logs -l iter8.tools/test=httpbin-test\n</code></pre> Sample performance test logs <pre><code>  time=2023-09-01 19:31:40 level=info msg=task 2: ready: started\n  time=2023-09-01 19:31:40 level=info msg=task 2: ready: completed\n  time=2023-09-01 19:31:40 level=info msg=task 3: http: started\n  {\"ts\":1693596700.013507,\"level\":\"info\",\"file\":\"httprunner.go\",\"line\":100,\"msg\":\"Starting http test\",\"run\":\"0\",\"url\":\"http://httpbin.default/get\",\"threads\":\"4\",\"qps\":\"8.0\",\"warmup\":\"parallel\",\"conn-reuse\":\"\"}\n  {\"ts\":1693596712.606946,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T001 ended after 12.534760214s : 25 calls. qps=1.9944537887591696\"}\n  {\"ts\":1693596712.616122,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T002 ended after 12.544591006s : 25 calls. qps=1.9928907995519867\"}\n  {\"ts\":1693596712.623089,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T003 ended after 12.551572714s : 25 calls. qps=1.9917822706086104\"}\n  {\"ts\":1693596712.628555,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T000 ended after 12.557040548s : 25 calls. qps=1.9909149695293316\"}\n  {\"ts\":1693596712.629567,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":564,\"msg\":\"Run ended\",\"run\":\"0\",\"elapsed\":\"12.557657464s\",\"calls\":\"100\",\"qps\":\"7.963268649959411\"}\n  time=2023-09-01 19:31:52 level=info msg=task 3: http: completed\n</code></pre>"},{"location":"getting-started/first-performance/#cleanup","title":"Cleanup","text":"<p>Remove the performance test and the sample app from the Kubernetes cluster. <pre><code>helm delete httpbin-test\nkubectl delete svc/httpbin\nkubectl delete deploy/httpbin\n</code></pre></p>"},{"location":"getting-started/first-performance/#uninstall-the-iter8-controller","title":"Uninstall the Iter8 controller","text":"<pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre> <p>Congratulations!  You completed your first performance test with Iter8.</p> Some variations and extensions of this performance test <ol> <li>The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections.</li> <li>The http task can be configured to send various types of content as payload.</li> </ol>"},{"location":"getting-started/first-release/","title":"Your first progressive release","text":"<p>This tutorial shows how Iter8 can be used to release a basic Kubernetes application using a blue-green release strategy.  In a blue-green release, a percentage of requests are directed to a candidate version of the model.  This percentage can be changed over time.  The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the blue-green release strategy.</p> <p></p> Before you begin <ol> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> and <code>helm</code> CLIs. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Install Istio. It suffices to install the demo profile, for example by using:  <pre><code>istioctl install --set profile=demo -y\n</code></pre></li> </ol>"},{"location":"getting-started/first-release/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"getting-started/first-release/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the application using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  strategy: blue-green\nEOF\n</code></pre> What happens? <p>Because <code>environment</code> is set to <code>deployment-istio</code>, a <code>Deployment</code> and a <code>Service</code> object are created.     - The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.     - The name <code>httpbin-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.     - Alternatively, a <code>deploymentSpecification</code> and/or a <code>serviceSpecification</code> could have been specified.</p> <p>To support routing, a <code>Service</code> (of type <code>ExternalName</code>) named <code>default/httpbin</code> pointing at the Istio gateway, <code>istio-ingressgateway.istio-system</code>, is deployed. The name is the Helm release name since it not specified in <code>application.metadata</code>. Further, an Iter8 routemap is created. Finally, to support the blue-green release, a <code>ConfigMap</code> (<code>httpbin-0-weight-config</code>) is created to be used to manage the proportion of traffic sent to this version.</p> <p>Once the application components are ready, the Iter8 controller automatically configures the routing by creating an Istio <code>VirtualService</code>. It is configured to route all traffic to the only deployed version, <code>httpbin-0</code>.</p>"},{"location":"getting-started/first-release/#verify-routing","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code>:</p> <pre><code>kubectl get virtualservice httpbin -o yaml\n</code></pre> <p>You can also send requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.4/samples/kserve-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>Send requests: <pre><code>curl httpbin.default -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the success of the request (the HTTP return code) and the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre> To send requests from outside the cluster <p>To configure the release for traffic from outside the cluster, a suitable Istio <code>Gateway</code> is required (for example). When using the Iter8 <code>release</code> chart, set the <code>gateway</code> field to the name of your <code>Gateway</code>. Finally, to send traffic:</p> <p>(a) In a separate terminal, port-forward the ingress gateway: <pre><code>kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80\n</code></pre> (b) Send requests using the <code>Host</code> header: <pre><code>curl -H 'Host: httpbin.default' localhost:8080 -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p>"},{"location":"getting-started/first-release/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate can deployed by simply adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: blue-green\nEOF\n</code></pre> About the candidate version <p>In this tutorial, the candidate image is the same as the one for the primary version. In a real world example, it would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the second version is deployed and ready, the Iter8 controller automatically reconfigures the routing; the <code>VirtualService</code> is updated to distribute traffic between versions based on the weights.</p>"},{"location":"getting-started/first-release/#verify-routing_1","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. Requests will now be handled equally by both versions. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n...\nHTTP/1.1 200 OK\napp-version: httpbin-1\n</code></pre>"},{"location":"getting-started/first-release/#modify-weights-optional","title":"Modify weights (optional)","text":"<p>To modify the request distribution between the versions, add a <code>weight</code> to each version. The weights are relative to each other.</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n    weight: 30\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n    weight: 70\n  strategy: blue-green\nEOF\n</code></pre> <p>Iter8 automatically reconfigures the routing to distribute traffic between the versions based on the new weights.</p>"},{"location":"getting-started/first-release/#verify-routing_2","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. 70 percent of requests will now be handled by the candidate version; the remaining 30 percent by the primary version.</p>"},{"location":"getting-started/first-release/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: blue-green\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, the image would also have been updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary version ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"getting-started/first-release/#verify-routing_3","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"getting-started/first-release/#cleanup","title":"Cleanup","text":"<p>Delete the application and its routing configuration:</p> <pre><code>helm delete httpbin\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>Congratulations!  You completed your first blue-green release with Iter8.</p>"},{"location":"getting-started/help/","title":"Get Help","text":"<ol> <li>Read Iter8 docs.</li> <li>Join the Iter8 Slack workspace.</li> <li>File an issue or start a discussion on the Iter8 GitHub repo.</li> </ol>"},{"location":"getting-started/install/","title":"Installation","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"getting-started/logs/","title":"Logs","text":"Sample performance test logs <pre><code>  time=2023-09-01 19:31:40 level=info msg=task 2: ready: started\n  time=2023-09-01 19:31:40 level=info msg=task 2: ready: completed\n  time=2023-09-01 19:31:40 level=info msg=task 3: http: started\n  {\"ts\":1693596700.013507,\"level\":\"info\",\"file\":\"httprunner.go\",\"line\":100,\"msg\":\"Starting http test\",\"run\":\"0\",\"url\":\"http://httpbin.default/get\",\"threads\":\"4\",\"qps\":\"8.0\",\"warmup\":\"parallel\",\"conn-reuse\":\"\"}\n  {\"ts\":1693596712.606946,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T001 ended after 12.534760214s : 25 calls. qps=1.9944537887591696\"}\n  {\"ts\":1693596712.616122,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T002 ended after 12.544591006s : 25 calls. qps=1.9928907995519867\"}\n  {\"ts\":1693596712.623089,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T003 ended after 12.551572714s : 25 calls. qps=1.9917822706086104\"}\n  {\"ts\":1693596712.628555,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":832,\"msg\":\"T000 ended after 12.557040548s : 25 calls. qps=1.9909149695293316\"}\n  {\"ts\":1693596712.629567,\"level\":\"info\",\"file\":\"periodic.go\",\"line\":564,\"msg\":\"Run ended\",\"run\":\"0\",\"elapsed\":\"12.557657464s\",\"calls\":\"100\",\"qps\":\"7.963268649959411\"}\n  time=2023-09-01 19:31:52 level=info msg=task 3: http: completed\n</code></pre>"},{"location":"getting-started/uninstall/","title":"Uninstall","text":"<pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/abn/","title":"A/B Testing with the Iter8 SDK","text":"<p>Your first A/B/n test describes how to perform an A/B test of an backend component using the with the Iter8 SDK.</p>"},{"location":"tutorials/blue-green/","title":"Blue-green release","text":"<p>Your first automated release describes how to implement a progressive blue-green release of a Kubernetes application.</p>"},{"location":"tutorials/canary/","title":"Canary release","text":"<p>This tutorial shows how Iter8 can be used to release a basic Kubernetes application using a canary release strategy.  In a canary release, requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary, or initial, version of the model. The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the canary release strategy.</p> <p></p> Before you begin <ol> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> and <code>helm</code> CLIs. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Install Istio. It suffices to install the demo profile, for example by using:  <pre><code>istioctl install --set profile=demo -y\n</code></pre></li> </ol>"},{"location":"tutorials/canary/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/canary/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the application using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  strategy: canary\nEOF\n</code></pre> What happens? <p>Because <code>environment</code> is set to <code>deployment-istio</code>, a <code>Deployment</code> and a <code>Service</code> object are created.     - The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.     - The name <code>httpbin-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.     - Alternatively, a <code>deploymentSpecification</code> and/or a <code>serviceSpecification</code> could have been specified.</p> <p>To support routing, a <code>Service</code> (of type <code>ExternalName</code>) named <code>default/httpbin</code> pointing at the Istio gateway, <code>istio-ingressgateway.istio-system</code>, is deployed. The name is the Helm release name since it not specified in <code>application.metadata</code>. Further, an Iter8 routemap is created.</p> <p>Once the application components are ready, the Iter8 controller automatically configures the routing by creating an Istio <code>VirtualService</code>. It is configured to route all traffic to the only deployed version, <code>httpbin-0</code>.</p>"},{"location":"tutorials/canary/#verify-routing","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code>:</p> <pre><code>kubectl get virtualservice httpbin -o yaml\n</code></pre> <p>You can also send requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.4/samples/kserve-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>Send requests: <pre><code>curl httpbin.default -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the success of the request (the HTTP return code) and the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre> To send requests from outside the cluster <p>To configure the release for traffic from outside the cluster, a suitable Istio <code>Gateway</code> is required (for example). When using the Iter8 <code>release</code> chart, set the <code>gateway</code> field to the name of your <code>Gateway</code>. Finally, to send traffic:</p> <p>(a) In a separate terminal, port-forward the ingress gateway: <pre><code>kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80\n</code></pre> (b) Send requests using the <code>Host</code> header: <pre><code>curl -H 'Host: httpbin.default' localhost:8080 -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p>"},{"location":"tutorials/canary/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate can deployed by simply adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: canary\nEOF\n</code></pre> About the candidate version <p>In this tutorial, the candidate image is the same as the one for the primary version. In a real world example, it would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the candidate version is ready, the Iter8 controller will Iter8 will automatically reconfigure the routing so that requests with the header <code>traffic</code> set to <code>test</code> will be sent to the candidate model:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-1\n</code></pre> <p>All other requests will be sent to the primary model (<code>httpbin-0</code>):</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/canary/#verify-routing_1","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. Those with header <code>traffic</code> set to <code>test</code> will be handled by the candidate model (<code>httpbin-1</code>) while all others will be handled by the primary version.</p>"},{"location":"tutorials/canary/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: canary\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, the image would also have been updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary version ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/canary/#verify-routing_2","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/canary/#cleanup","title":"Cleanup","text":"<p>Delete the application and its routing configuration:</p> <pre><code>helm delete httpbin\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/mirror/","title":"Mirrored release","text":"<p>This tutorial shows how Iter8 can be used to release a new version of an application using mirroring. When an application is mirrored, all requests are sent to the primary version of the application.  A percentage of requests is replicated and sent to the candidate version of the model.  This percentage can be changed over time. Only the responses from the primary version are returned to the user. The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the mirrored release strategy.</p> <p></p> Before you begin <ol> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> and <code>helm</code> CLIs. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Install Istio. It suffices to install the demo profile, for example by using:  <pre><code>istioctl install --set profile=demo -y\n</code></pre></li> </ol>"},{"location":"tutorials/mirror/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/mirror/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the model using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  strategy: mirror\nEOF\n</code></pre> What happens? <p>Because <code>environment</code> is set to <code>deployment-istio</code>, a <code>Deployment</code> and a <code>Service</code> object are created.     - The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.     - The name <code>httpbin-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.     - Alternatively, a <code>deploymentSpecification</code> and/or a <code>serviceSpecification</code> could have been specified.</p> <p>To support routing, a <code>Service</code> (of type <code>ExternalName</code>) named <code>default/httpbin</code> pointing at the Istio gateway, <code>istio-ingressgateway.istio-system</code>, is deployed. The name is the Helm release name since it not specified in <code>application.metadata</code>. Further, an Iter8 routemap is created.</p> <p>Once the application components are ready, the Iter8 controller automatically configures the routing by creating an Istio <code>VirtualService</code>. It is configured to route all traffic to the only deployed version, <code>httpbin-0</code>.</p>"},{"location":"tutorials/mirror/#verify-routing","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code>:</p> <pre><code>kubectl get virtualservice httpbin -o yaml\n</code></pre> <p>You can also send requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.4/samples/kserve-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>Send requests: <pre><code>curl httpbin.default -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the success of the request (the HTTP return code) and the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre> To send requests from outside the cluster <p>To configure the release for traffic from outside the cluster, a suitable Istio <code>Gateway</code> is required (for example). When using the Iter8 <code>release</code> chart, set the <code>gateway</code> field to the name of your <code>Gateway</code>. Finally, to send traffic:</p> <p>(a) In a separate terminal, port-forward the ingress gateway: <pre><code>kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80\n</code></pre> (b) Send requests using the <code>Host</code> header: <pre><code>curl -H 'Host: httpbin.default' localhost:8080 -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p>"},{"location":"tutorials/mirror/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate can deployed by simply adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n    weight: 100\n  strategy: mirror\nEOF\n</code></pre> About the candidate version <p>In this tutorial, the candidate image is the same as the one for the primary version. In a real world example, it would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>To support modifying the percentage of traffic sent to the candidate, a <code>ConfigMap</code> (<code>httpbin-1-weight-config</code>) is created to be used to manage the proportion of traffic sent to this version.</p> <p>When the second version is deployed and ready, the Iter8 controller automatically reconfigures the routing; the <code>VirtualService</code> is updated to mirror a portion of the traffic to the candidate version.</p>"},{"location":"tutorials/mirror/#verify-routing_1","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code>. With mirroring, only the response of the primary is returned to the user so output will continue to be:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/mirror/#modify-mirrored-weight-optional","title":"Modify mirrored weight (optional)","text":"<p>To modify the percentage of traffic mirrored, set the weight in the candidate version:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n    weight: 50\n  strategy: mirror\nEOF\n</code></pre> <p>Iter8 automatically reconfigures the routing to send the desired percentage of requests to the candidate version.</p>"},{"location":"tutorials/mirror/#verify-routing_2","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. Again, user requests will always be responded to by the primary version.</p>"},{"location":"tutorials/mirror/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-istio\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: mirror\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, the image would also have been updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary version ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/mirror/#verify-routing_3","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/mirror/#cleanup","title":"Cleanup","text":"<p>Delete the application and its routing configuration:</p> <pre><code>helm delete httpbin\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/integrations/ghactions/","title":"Trigger a GitHub Actions workflow during a performance test","text":"<p>Iter8 provides a <code>github</code> task that sends a <code>repository_dispatch</code> which can trigger the workflows in the default branch of a GitHub repository.</p>"},{"location":"tutorials/integrations/ghactions/#example","title":"Example","text":"<p>In this example, you will run the Your first performance test but at the end of the performance test, Iter8 will trigger a workflow on GitHub.</p> <p>In this simple example, the workflow will simply print out a test summary that it will receive with the <code>repository_dispatch</code>. In a more sophisticated scenario, the workflow could, for example, read from the test summary and determine what to do next.</p> <p>To summarize what will happen, you will create a new GitHub repository, add a workflow that will respond to the <code>github</code> task, set up and run a performance test, and check if the workflow was triggered.</p> <p>The <code>github</code> task requires the name of a repository, the name of the owner, as well as an authentication token in order to send the <code>repository_dispatch</code>. To see a full list of the <code>github</code> task parameters, see here.</p> <ol> <li>Create a new repository on GitHub.</li> <li>Add the following workflow.</li> </ol> <pre><code>name: iter8 `github` task test\non:\n  repository_dispatch:\n    types: iter8\njobs:\n  my-job:\n    runs-on: ubuntu-latest\n    steps:\n      - run: 'echo \"payload: ${{ toJson(github.event.client_payload) }}\"'\n</code></pre> <p>Note that this workflow has one job that will print out the <code>client_payload</code>. The default <code>github</code> task payload is configured with <code>client_payload</code> set to <code>.Report</code>, a summary of the performance test.</p> <p>Also note that the <code>on.repository_dispatch.types</code> is set to <code>iter8</code>. The default <code>github</code> task payload is configured with <code>event_type</code> set to <code>iter8</code>. This indicates that once the <code>repository_dispatch</code> has been sent, only workflows on the default branch with <code>on.repository_dispatch.types</code> set to <code>iter8</code> will be triggered.</p> <ol> <li>Create a GitHub personal access token for the <code>token</code> parameter.</li> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> CLI. You can create a local Kubernetes cluster using tools like Kind or Minikube.</li> <li> <p>Install the Iter8 controller</p> <p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p> </li> <li> <p>Deploy the sample HTTP service in the Kubernetes cluster. <pre><code>kubectl create deploy httpbin --image=kennethreitz/httpbin --port=80\nkubectl expose deploy httpbin --port=80\n</code></pre></p> </li> <li>Launch the performance test using the <code>github</code> task with the appropriate values. <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http,github}\" \\\n--set http.url=http://httpbin.default/get \\\n--set github.owner=&lt;GitHub owner&gt; \\\n--set github.repo=&lt;GitHub repository&gt; \\\n--set github.token=&lt;GitHub token&gt;\n</code></pre></li> <li>Verify that the workflow has been triggered after the performance test has completed.</li> </ol> Some variations and extensions of the <code>github</code> task <p>The default <code>github</code> task payload sends a summary of the performance test. </p> <p>In your workflow, you can read from the report and use that data for control flow or use snippets of that data in different actions. For example, you can check to see if there have been any task failures and take alternative actions.</p> <p>You do not need to use the default <code>github</code> task payload. You can provide your own payload by overriding the default of the <code>payloadTemplateURL</code>.</p>"},{"location":"tutorials/integrations/overview/","title":"Integrations","text":"<p>The tutorials under the <code>integrations</code> section are maintained by members of the Iter8 community. They may become outdated. </p> <p>If you find that something is not working, please lend a helping hand and fix it in a PR.</p> <p>More integrations and examples are always welcome.</p>"},{"location":"tutorials/integrations/slack/","title":"Use Iter8 to send a message to a Slack channel","text":"<p>Iter8 provides a <code>slack</code> task  that sends a message to a Slack channel using a webhook.</p>"},{"location":"tutorials/integrations/slack/#example","title":"Example","text":"<p>In this example, you will run the Your first performance test but at the end of the performance test, Iter8 will send a message on Slack. </p> <p>The message will simply contain a summary of the performance test in text form. However, you can easily construct a more sophisticated message by providing your own payload template.</p> <p>This task could provide important updates on a performance test over Slack, for example a summary at the end of the test.</p> <p>To summarize what will happen, you will create a new channel on Slack and configure a webhook, set up and run a performance test, and check if a message was sent to the channel.</p> <p>The <code>slack</code> task requires the URL of the Slack webhook. To see a full list of the <code>github</code> task parameters, see here.</p> <ol> <li>Create a new channel in your Slack organization.</li> <li>Create a Slack app, enable incoming webhooks, and create a new incoming webhook. See here.</li> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> CLI. You can create a local Kubernetes cluster using tools like Kind or Minikube.</li> <li> <p>Install the Iter8 controller</p> <p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p> </li> <li> <p>Deploy the sample HTTP service in the Kubernetes cluster. <pre><code>kubectl create deploy httpbin --image=kennethreitz/httpbin --port=80\nkubectl expose deploy httpbin --port=80\n</code></pre></p> </li> <li>Launch the performance test with the <code>slack</code> task with the appropriate values. <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http,slack}\" \\\n--set http.url=http://httpbin.default/get \\\n--set slack.url=&lt;Slack webhook&gt; \\\n--set slack.method=POST\n</code></pre></li> <li>Verify that the message has been sent after the performance test has completed.</li> </ol> Some variations and extensions of the <code>slack</code> task <p>The default <code>slack</code> task payload sends a summary of the performance test.</p> <p>However, you do not need to use the default payload. You can provide your own payload by overriding the default of the <code>payloadTemplateURL</code>.</p> <p>For example, you can also use Slack's Block Kit to create more sophisticated messages. You can use markdown, create different sections, or add interactivity, such as buttons.</p>"},{"location":"tutorials/integrations/kserve/abn-grpc/","title":"A/B Testing a backend ML model","text":"<p>This tutorial describes how to do A/B testing as part of the release of a backend ML model hosted on KServe using the Iter8 SDK. In this tutorial, communication with the model is via gRPC calls.</p> <p></p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe. You can create a KServe Quickstart environment as follows: <pre><code>curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash\n</code></pre> If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"tutorials/integrations/kserve/abn-grpc/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve/abn-grpc/#deploy-the-sample-application","title":"Deploy the sample application","text":"<p>A simple sample two-tier application using the Iter8 SDK is provided. Note that only the frontend component uses the Iter8 SDK. Deploy both the frontend and backend components:</p>"},{"location":"tutorials/integrations/kserve/abn-grpc/#frontend","title":"Frontend","text":"<p>The frontend component uses the Iter8 SDK method <code>Lookup()</code> before each call to the backend (ML model). The frontend uses the returned version number to route the request to the recommended version of backend.</p> <p>Deploy the frontend:</p> <pre><code>kubectl create deployment frontend --image=iter8/abn-sample-kserve-grpc-frontend-go:0.17.3\nkubectl expose deployment frontend --name=frontend --port=8090\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-grpc/#backend","title":"Backend","text":"<p>The backend application component is an ML model. Release it using the Iter8 <code>release</code> chart:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/version: backend\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  protocolVersion: v2\n  ports:\n  - containerPort: 9000\n    name: h2c\n    protocol: TCP\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n</code></pre> <p>Wait for the backend model to be ready:</p> <pre><code>kubectl wait --for condition=ready isvc/backend-0 --timeout=600s\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-grpc/#generate-load","title":"Generate load","text":"<p>In one shell, port-forward requests to the frontend component:     <pre><code>kubectl port-forward service/frontend 8090:8090\n</code></pre></p> <p>In another shell, run a script to generate load from multiple users:     <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.3/samples/abn-sample/generate_load.sh | sh -s --\n</code></pre></p> <p>The load generator and sample frontend application outputs the backend that handled each recommendation. With just one version is deployed, all requests are handled by <code>backend-0</code>. In the output you will see something like:</p> <pre><code>Recommendation: backend-0\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-grpc/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the model can be deployed simply by adding a second version to the list of versions:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/version: backend\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  protocolVersion: v2\n  ports:\n  - containerPort: 9000\n    name: h2c\n    protocol: TCP\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n</code></pre> About the candidate <p>In this tutorial, the model source (field <code>storageUri</code>) for the candidate version is the same as for the primary version of the model. In a real example, this would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>Until the candidate version is ready, calls to <code>Lookup()</code> will return only the version index number <code>0</code>; that is, the first, or primary, version of the model. Once the candidate version is ready, <code>Lookup()</code> will return both <code>0</code> and <code>1</code>, the indices of both versions, so that requests can be distributed across both versions.</p> <p>Once both backend versions are responding to requests, the output of the load generator will include recommendations from the candidate version. In this example, you should see something like:</p> <pre><code>Recommendation: backend-1\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-grpc/#compare-versions-using-grafana","title":"Compare versions using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>default/backend</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/abnDashboard</code></li> <li>Query string: <code>namespace=default&amp;application=backend</code></li> </ul> <p>Create a new dashboard by import. Copy and paste the contents of the <code>abn</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source above.</p> <p>The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following:</p> <p></p> <p>Once you identify a winner, it can be promoted, and the candidate version deleted.</p>"},{"location":"tutorials/integrations/kserve/abn-grpc/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/version: backend\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  protocolVersion: v2\n  ports:\n  - containerPort: 9000\n    name: h2c\n    protocol: TCP\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, <code>storageUri</code> would also be updated (with that from the candidate version).</p> <p>Calls to <code>Lookup()</code> will now recommend that all traffic be sent to the new primary version <code>backend-0</code> (currently serving the promoted version of the code).</p> <p>The output of the load generator will again show just <code>backend_0</code>:</p> <pre><code>Recommendation: backend-0\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-grpc/#cleanup","title":"Cleanup","text":"<p>Delete the backend:</p> <pre><code>helm delete backend\n</code></pre> <p>Delete the frontend:</p> <pre><code>kubectl delete deploy/frontend svc/frontend\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-http/","title":"A/B Testing a backend ML model","text":"<p>This tutorial describes how to do A/B testing as part of the release of a backend ML model hosted on KServe when using the Iter8 SDK. In this tutorial, communication from the frontend to the backend model is via HTTP calls.</p> <p></p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe. You can create a KServe Quickstart environment as follows: <pre><code>curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash\n</code></pre> If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"tutorials/integrations/kserve/abn-http/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve/abn-http/#deploy-the-sample-application","title":"Deploy the sample application","text":"<p>A simple sample two-tier application using the Iter8 SDK is provided. Note that only the frontend component uses the Iter8 SDK. Deploy both the frontend and backend components:</p>"},{"location":"tutorials/integrations/kserve/abn-http/#frontend","title":"Frontend","text":"<p>The frontend component uses the Iter8 SDK method <code>Lookup()</code> before each call to the backend (ML model). The frontend uses the returned version number to route the request to the recommended version of backend.</p> <p>Deploy the frontend:</p> <pre><code>kubectl create deployment frontend --image=iter8/abn-sample-kserve-http-frontend-go:0.17.3\nkubectl expose deployment frontend --name=frontend --port=8090\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-http/#backend","title":"Backend","text":"<p>The backend application component is an ML model. Release it using the Iter8 <code>release</code> chart:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/version: backend\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n</code></pre> <p>Wait for the backend model to be ready:</p> <pre><code>kubectl wait --for condition=ready isvc/backend-0 --timeout=600s\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-http/#generate-load","title":"Generate load","text":"<p>In one shell, port-forward requests to the frontend component:     <pre><code>kubectl port-forward service/frontend 8090:8090\n</code></pre></p> <p>In another shell, run a script to generate load from multiple users:     <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.3/samples/abn-sample/generate_load.sh | sh -s --\n</code></pre></p> <p>The load generator and sample frontend application outputs the backend that handled each recommendation. With just one version is deployed, all requests are handled by <code>backend-0</code>. In the output you will see something like:</p> <pre><code>Recommendation: backend-0\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-http/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the model can be deployed simply by adding a second version to the list of versions:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/version: backend\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n</code></pre> About the candidate <p>In this tutorial, the model source (field <code>storageUri</code>) for the candidate version is the same as for the primary version of the model. In a real example, this would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>Until the candidate version is ready, calls to <code>Lookup()</code> will return only the version index number <code>0</code>; that is, the first, or primary, version of the model. Once the candidate version is ready, <code>Lookup()</code> will return both <code>0</code> and <code>1</code>, the indices of both versions, so that requests can be distributed across both versions.</p> <p>Once both backend versions are responding to requests, the output of the load generator will include recommendations from the candidate version. In this example, you should see something like:</p> <pre><code>Recommendation: backend-1\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-http/#compare-versions-using-grafana","title":"Compare versions using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>default/backend</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/abnDashboard</code></li> <li>Query string: <code>namespace=default&amp;application=backend</code></li> </ul> <p>Create a new dashboard by import. Copy and paste the contents of the <code>abn</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source above.</p> <p>The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following:</p> <p></p> <p>Once you identify a winner, it can be promoted, and the candidate version deleted.</p>"},{"location":"tutorials/integrations/kserve/abn-http/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/version: backend\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, <code>storageUri</code> would also be updated (with that from the candidate version).</p> <p>Calls to <code>Lookup()</code> will now recommend that all traffic be sent to the new primary version <code>backend-0</code> (currently serving the promoted version of the code).</p> <p>The output of the load generator will again show just <code>backend_0</code>:</p> <pre><code>Recommendation: backend-0\n</code></pre>"},{"location":"tutorials/integrations/kserve/abn-http/#cleanup","title":"Cleanup","text":"<p>Delete the backend:</p> <pre><code>helm delete backend\n</code></pre> <p>Delete the frontend:</p> <pre><code>kubectl delete deploy/frontend svc/frontend\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre>"},{"location":"tutorials/integrations/kserve/blue-green/","title":"Blue-green release of a KServe ML model","text":"<p>This tutorial shows how Iter8 can be used to release ML models hosted in a KServe environment using a blue-green release strategy.  In a blue-green release, a percentage of requests are directed to a candidate version of the model.  This percentage can be changed over time.  The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the blue-green release strategy.</p> <p></p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe. You can create a KServe Quickstart environment as follows: <pre><code>curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash\n</code></pre> If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> </ol>"},{"location":"tutorials/integrations/kserve/blue-green/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve/blue-green/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the model using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  strategy: blue-green\nEOF\n</code></pre> <p>Wait for the backend model to be ready:</p> <pre><code>kubectl wait --for condition=ready isvc/wisdom-0 --timeout=600s\n</code></pre> What happens? <ul> <li>Because <code>environment</code> is set to <code>kserve</code>, an <code>InferenceService</code> object is created.</li> <li>The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.</li> <li>The name <code>wisdom-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.</li> <li>Alternatively, an <code>inferenceServiceSpecification</code> could have been provided.</li> </ul> <p>To support routing, a <code>Service</code> (of type <code>ExternalName</code>) named <code>default/wisdom</code> pointing at the KNative gateway, <code>knative-local-gateway.istio-system</code>, is deployed. The name is the Helm release name since it not specified in <code>application.metadata</code>. Further, an Iter8 routemap is created. Finally, to support the blue-green release, a <code>ConfigMap</code> (<code>wisdom-0-weight-config</code>) is created to be used to manage the proportion of traffic sent to this version.</p> <p>Once the <code>InferenceService</code> is ready, the Iter8 controller automatically configures the routing by creating an Istio <code>VirtualService</code>. It is configured to route all inference requests to the only deployed version, <code>wisdom-0</code>.</p>"},{"location":"tutorials/integrations/kserve/blue-green/#verify-routing","title":"Verify routing","text":"<p>You can send verify the routing configuration by inspecting the <code>VirtualService</code>:</p> <pre><code>kubectl get virtualservice wisdom -o yaml\n</code></pre> <p>You can also send inference requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.4/samples/kserve-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>Send requests: <pre><code>curl -H 'Content-Type: application/json' \\\nhttp://wisdom.default -d @input.json -s -D - \\\n| grep -e HTTP -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the success of the request (the HTTP return code) and the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>HTTP/1.1 200 OK\napp-version: wisdom-0\n</code></pre> To send requests from outside the cluster <p>To configure the release for traffic from outside the cluster:</p> <p>(a) In a separate terminal, port-forward the Istio ingress gateway: <pre><code>kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80\n</code></pre> (b) Download the sample input: <pre><code>curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/input.json\n</code></pre> (c) Send inference requests using the <code>Host</code> header: <pre><code>curl -H 'Content-Type: application/json' \\\n-H 'Host: wisdom.default' \\\nlocalhost:8080 -d @input.json -s -D - \\\n| grep -e '^HTTP' -e app-version\n</code></pre></p>"},{"location":"tutorials/integrations/kserve/blue-green/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the model can be deployed simply by adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  strategy: blue-green\nEOF\n</code></pre> About the candidate <p>In this tutorial, the model source (field <code>storageUri</code>) for the candidate version is the same as for the primary version of the model. In a real example, this would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the candidate model is ready, Iter8 will automatically reconfigure the routing so that inference requests are sent to both versions.</p>"},{"location":"tutorials/integrations/kserve/blue-green/#verify-routing_1","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. Requests will be handled equally by both versions. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: wisdom-0\n...\nHTTP/1.1 200 OK\napp-version: wisdom-1\n</code></pre>"},{"location":"tutorials/integrations/kserve/blue-green/#modify-weights-optional","title":"Modify weights (optional)","text":"<p>To modify the request distribution between versions, add a <code>weight</code> to each version. The weights are relative to each other.</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n    weight: 30\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n    weight: 70\n  strategy: blue-green\nEOF\n</code></pre> <p>Iter8 automatically reconfigures the routing to distribute traffic between the versions based on the new weights.</p>"},{"location":"tutorials/integrations/kserve/blue-green/#verify-routing_2","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. 70 percent of requests will now be handled by the candidate version; the remaining 30 percent by the primary version.</p>"},{"location":"tutorials/integrations/kserve/blue-green/#promote-candidate","title":"Promote candidate","text":"<p>The candidate model can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  strategy: blue-green\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, <code>storageUri</code> would also be updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary <code>InferenceService</code> ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/integrations/kserve/blue-green/#verify-routing_3","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: wisdom-0\n</code></pre>"},{"location":"tutorials/integrations/kserve/blue-green/#cleanup","title":"Cleanup","text":"<p>Delete the models are their routing:</p> <pre><code>helm delete wisdom\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/integrations/kserve/canary/","title":"Canary release of a KServe ML model","text":"<p>This tutorial shows how Iter8 can be used to release ML models hosted in a KServe environment using a canary release strategy.  In a canary release, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model.  The remaining requests go to the primary, or initial, version of the model. The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the canary release strategy.</p> <p></p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe. You can create a KServe Quickstart environment as follows: <pre><code>curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash\n</code></pre> If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> </ol>"},{"location":"tutorials/integrations/kserve/canary/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve/canary/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the model using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  strategy: canary\nEOF\n</code></pre> <p>Wait for the backend model to be ready:</p> <pre><code>kubectl wait --for condition=ready isvc/wisdom-0 --timeout=600s\n</code></pre> What happens? <ul> <li>Because <code>environment</code> is set to <code>kserve</code>, an <code>InferenceService</code> object is created.</li> <li>The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.</li> <li>The name <code>wisdom-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.</li> <li>Alternatively, an <code>inferenceServiceSpecification</code> could have been provided.</li> </ul> <p>To support routing, a <code>Service</code> (of type <code>ExternalName</code>) named <code>default/wisdom</code> pointing at the KNative gateway, <code>knative-local-gateway.istio-system</code>, is deployed. The name is the Helm release name since it not specified in <code>application.metadata</code>. Further, an Iter8 routemap is created.</p> <p>Once the <code>InferenceService</code> is ready, the Iter8 controller automatically configures the routing by creating an Istio <code>VirtualService</code>. It is configured to route all inference requests to the only deployed version, <code>wisdom-0</code>.</p>"},{"location":"tutorials/integrations/kserve/canary/#verify-routing","title":"Verify routing","text":"<p>You can send verify the routing configuration by inspecting the <code>VirtualService</code>:</p> <pre><code>kubectl get virtualservice wisdom -o yaml\n</code></pre> <p>You can also send inference requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.4/samples/kserve-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>To send requests without the header <code>traffic</code>: <pre><code>curl -H 'Content-Type: application/json' \\\nhttp://wisdom.default -d @input.json -s -D - \\\n| grep -e HTTP -e app-version\n</code></pre></p> </li> <li> <p>Requests can also be sent with the header <code>traffic: test</code>. When a candidate is deployed, requests with this header will be routed to the candidate. When no candidate is deployed, all requests will be routed to the primary version. <pre><code>curl -H 'Content-Type: application/json' \\\n-H 'traffic: test' \\\nhttp://wisdom.default -d @input.json -s -D - \\\n| grep -e HTTP -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the success of the request (the HTTP return code) and the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>HTTP/1.1 200 OK\napp-version: wisdom-0\n</code></pre> To send requests from outside the cluster <p>To configure the release for traffic from outside the cluster:</p> <p>(a) In a separate terminal, port-forward the Istio ingress gateway: <pre><code>kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80\n</code></pre> (b) Download the sample input: <pre><code>curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/kserve-serving/input.json\n</code></pre> (c) To send requests without the header <code>traffic</code>: <pre><code>curl -H 'Content-Type: application/json' \\\n-H 'Host: wisdom.default' \\\nlocalhost:8080 -d @input.json -s -D - \\\n| grep -e '^HTTP' -e app-version\n</code></pre>  (d) To send requests with the header <code>traffic: test</code>: <pre><code>curl -H 'Content-Type: application/json' \\\n-H 'Host: wisdom.default' \\\n-H 'traffic: test' \\\nlocalhost:8080 -d @input.json -s -D - \\\n| grep -e HTTP -e app-version\n</code></pre></p>"},{"location":"tutorials/integrations/kserve/canary/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the model can be deployed simply by adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  strategy: canary\nEOF\n</code></pre> About the candidate <p>In this tutorial, the model source (field <code>storageUri</code>) for the candidate version is the same as for the primary version of the model. In a real example, this would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the candidate version is ready, the Iter8 controller will Iter8 will automatically reconfigure the routing so that inference requests with the header <code>traffic</code> set to <code>test</code> will be sent to the candidate model:</p> <pre><code>HTTP/1.1 200 OK\napp-version: wisdom-1\n</code></pre> <p>All other requests will be sent to the primary model (<code>wisdom-0</code>):</p> <pre><code>HTTP/1.1 200 OK\napp-version: wisdom-0\n</code></pre>"},{"location":"tutorials/integrations/kserve/canary/#verify-routing_1","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. Those with header <code>traffic</code> set to <code>test</code> will be handled by the candidate model (<code>wisdom-1</code>) while all others will be handled by the primary version.</p>"},{"location":"tutorials/integrations/kserve/canary/#promote-candidate","title":"Promote candidate","text":"<p>The candidate model can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n  modelFormat: sklearn\n  runtime: kserve-mlserver\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n  strategy: canary\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, <code>storageUri</code> would also be updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary <code>InferenceService</code> ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/integrations/kserve/canary/#verify-routing_2","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. They will be handled by the primary version. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: wisdom-0\n</code></pre>"},{"location":"tutorials/integrations/kserve/canary/#cleanup","title":"Cleanup","text":"<p>Delete the models and their routing:</p> <pre><code>helm delete wisdom\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/integrations/kserve/grpc/","title":"Load test a KServe model (via gRPC)","text":"<p>This tutorial shows how easy it is to run a load test for KServe when using gRPC to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. </p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe. You can create a KServe Quickstart environment as follows: <pre><code>curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash\n</code></pre> If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"tutorials/integrations/kserve/grpc/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve/grpc/#deploy-an-inferenceservice","title":"Deploy an InferenceService","text":"<p>Create an InferenceService which exposes a gRPC port. The following serves the sklearn irisv2 model:</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-irisv2\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      runtime: kserve-mlserver\n      protocolVersion: v2\n      storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n      ports:\n      - containerPort: 9000\n        name: h2c\n        protocol: TCP\nEOF\n</code></pre>"},{"location":"tutorials/integrations/kserve/grpc/#launch-performance-test","title":"Launch performance test","text":"<pre><code>GRPC_HOST=$(kubectl get isvc sklearn-irisv2 -o jsonpath='{.status.components.predictor.address.url}' | sed 's#.*//##')\nGRPC_PORT=80\n</code></pre> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 model-test iter8 \\\n--set \"tasks={ready,grpc}\" \\\n--set ready.isvc=sklearn-irisv2 \\\n--set ready.timeout=180s \\\n--set grpc.protoURL=https://raw.githubusercontent.com/kserve/kserve/master/docs/predict-api/v2/grpc_predict_v2.proto \\\n--set grpc.host=${GRPC_HOST}:${GRPC_PORT} \\\n--set grpc.call=inference.GRPCInferenceService.ModelInfer \\\n--set grpc.dataURL=https://gist.githubusercontent.com/kalantar/6e9eaa03cad8f4e86b20eeb712efef45/raw/56496ed5fa9078b8c9cdad590d275ab93beaaee4/sklearn-irisv2-input-grpc.json\n</code></pre> About this performance test <p>This performance test consists of two tasks, namely, ready and grpc. </p> <p>The ready task checks if the <code>sklearn-irisv2</code> InferenceService exists and is <code>Ready</code>. </p> <p>The grpc task sends call requests to the <code>inference.GRPCInferenceService.ModelInfer</code> method of the cluster-local gRPC service with host address <code>${GRPC_HOST}:${GRPC_PORT}</code>, and collects Iter8's built-in gRPC load test metrics.</p>"},{"location":"tutorials/integrations/kserve/grpc/#view-results-using-grafana","title":"View results using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>. </p> <p>Add a JSON API data source <code>model-test</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/grpcDashboard</code> </li> <li>Query string: <code>namespace=default&amp;test=model-test</code></li> </ul> <p>Create a new dashboard by import. Paste the contents of the <code>grpc</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above.</p> <p>The Iter8 dashboard will look like the following:</p> <p></p>"},{"location":"tutorials/integrations/kserve/grpc/#cleanup","title":"Cleanup","text":"<pre><code>helm delete model-test\nkubectl delete inferenceservice sklearn-irisv2\n</code></pre>"},{"location":"tutorials/integrations/kserve/grpc/#uninstall-the-iter8-controller","title":"Uninstall the Iter8 controller","text":"<pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre> Some variations and extensions of this performance test <ol> <li>The grpc task can be configured with load related parameters such as the number of requests, requests per second, or number of concurrent connections.</li> </ol>"},{"location":"tutorials/integrations/kserve/http/","title":"Load test a KServe model (via HTTP)","text":"<p>This tutorial shows how easy it is to run a load test for KServe when using HTTP to make requests. We use a sklearn model to demonstrate. The same approach works for any model type. </p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe. You can create a KServe Quickstart environment as follows: <pre><code>curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.11/hack/quick_install.sh\" | bash\n</code></pre> If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"tutorials/integrations/kserve/http/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve/http/#deploy-an-inferenceservice","title":"Deploy an InferenceService","text":"<p>Create an InferenceService which exposes an HTTP port. The following serves the sklearn irisv2 model:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-irisv2\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      runtime: kserve-mlserver\n      storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n</code></pre>"},{"location":"tutorials/integrations/kserve/http/#launch-performance-test","title":"Launch performance test","text":"<pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 model-test iter8 \\\n--set \"tasks={ready,http}\" \\\n--set ready.isvc=sklearn-irisv2 \\\n--set ready.timeout=180s \\\n--set http.url=http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer \\\n--set http.payloadURL=https://gist.githubusercontent.com/kalantar/d2dd03e8ebff2c57c3cfa992b44a54ad/raw/97a0480d0dfb1deef56af73a0dd31c80dc9b71f4/sklearn-irisv2-input.json \\\n--set http.contentType=\"application/json\"\n</code></pre> About this performance test <p>This performance test consists of two tasks, namely, ready and http. </p> <p>The ready task checks if the <code>sklearn-irisv2</code> InferenceService exists and is <code>Ready</code>. </p> <p>The http task sends requests to the cluster-local HTTP service whose URL exposed by the InferenceService, <code>http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer</code>, and collects Iter8's built-in HTTP load test metrics.</p>"},{"location":"tutorials/integrations/kserve/http/#view-results-using-grafana","title":"View results using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>model-test</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/httpDashboard</code> </li> <li>Query string: <code>namespace=default&amp;test=model-test</code></li> </ul> <p>Create a new dashboard by import. Paste the contents of the <code>http</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above.</p> <p>The Iter8 dashboard will look like the following:</p> <p></p>"},{"location":"tutorials/integrations/kserve/http/#cleanup","title":"Cleanup","text":"<pre><code>helm delete model-test\nkubectl delete inferenceservice sklearn-irisv2\n</code></pre>"},{"location":"tutorials/integrations/kserve/http/#uninstall-the-iter8-controller","title":"Uninstall the Iter8 controller","text":"<pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre> Some variations and extensions of this performance test <ol> <li>The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections.</li> <li>The http task can be configured to send various types of content as payload.</li> </ol>"},{"location":"tutorials/integrations/kserve-mm/abn/","title":"A/B Testing a backend ML model","text":"<p>This tutorial describes how to do A/B testing as part of the release of a backend ML model hosted on KServe ModelMesh using the Iter8 SDK. </p> <p></p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe ModelMesh Serving. For example, you can create a modelmesh-serving Quickstart environment.  If using the Quickstart environment, your default namespace will be changed to <code>modelmesh-serving</code>. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"tutorials/integrations/kserve-mm/abn/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve-mm/abn/#deploy-the-sample-application","title":"Deploy the sample application","text":"<p>A simple sample two-tier application using the Iter8 SDK is provided. Note that only the frontend component uses the Iter8 SDK. Deploy both the frontend and backend components:</p>"},{"location":"tutorials/integrations/kserve-mm/abn/#frontend","title":"Frontend","text":"<p>The frontend component uses the Iter8 SDK method <code>Lookup()</code> before each call to the backend (ML model). The frontend uses the returned version number to route the request to the recommended version of backend.</p> <pre><code>kubectl create deployment frontend --image=iter8/abn-sample-mm-frontend-go:0.17.3\nkubectl expose deployment frontend --name=frontend --port=8090\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/abn/#backend","title":"Backend","text":"<p>The backend application component is an ML model. Release it using the Iter8 <code>release</code> chart:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: backend\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib\nEOF\n</code></pre> <p>Wait for the backend model to be ready:</p> <pre><code>kubectl wait --for condition=ready isvc/backend-0 --timeout=600s\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/abn/#generate-load","title":"Generate load","text":"<p>In one shell, port-forward requests to the frontend component:     <pre><code>kubectl port-forward service/frontend 8090:8090\n</code></pre></p> <p>In another shell, run a script to generate load from multiple users:     <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.3/samples/abn-sample/generate_load.sh | sh -s --\n</code></pre></p> <p>The load generator and sample frontend application outputs the backend that handled each recommendation. With just one version is deployed, all requests are handled by <code>backend-0</code>. In the output you will see something like:</p> <pre><code>Recommendation: backend-0__isvc-3642375d03\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/abn/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the model can be deployed simply by adding a second version to the list of versions:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: backend\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib\nEOF\n</code></pre> About the candidate <p>In this tutorial, the model source (field <code>storageUri</code>) for the candidate version is the same as for the primary version of the model. In a real example, this would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>Until the candidate version is ready, calls to <code>Lookup()</code> will return only the version index number <code>0</code>; that is, the first, or primary, version of the model. Once the candidate version is ready, <code>Lookup()</code> will return both <code>0</code> and <code>1</code>, the indices of both versions, so that requests can be distributed across both versions.</p> <p>Once both backend versions are responding to requests, the output of the load generator will include recommendations from the candidate version. In this example, you should see something like:</p> <pre><code>Recommendation: backend-1__isvc-3642375d03\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/abn/#compare-versions-using-grafana","title":"Compare versions using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>modelmesh-serving/backend</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.modelmesh-serving:8080/abnDashboard</code></li> <li>Query string: <code>namespace=modelmesh-serving&amp;application=backend</code></li> </ul> <p>Create a new dashboard by import. Copy and paste the contents of the <code>abn</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source above.</p> <p>The Iter8 dashboard allows you to compare the behavior of the two versions of the backend component against each other and select a winner. Since user requests are being sent by the load generation script, the values in the report may change over time. The Iter8 dashboard will look like the following:</p> <p></p> <p>Once you identify a winner, it can be promoted, and the candidate version deleted.</p>"},{"location":"tutorials/integrations/kserve-mm/abn/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install backend --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: backend\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, <code>storageUri</code> would also be updated (with that from the candidate version).</p> <p>Calls to <code>Lookup()</code> will now recommend that all traffic be sent to the new primary version <code>backend-0</code> (currently serving the promoted version of the code).</p> <p>The output of the load generator will again show just <code>backend_0</code>:</p> <pre><code>Recommendation: backend-0__isvc-3642375d03\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/abn/#cleanup","title":"Cleanup","text":"<p>Delete the backend:</p> <pre><code>helm delete backend\n</code></pre> <p>Delete the frontend:</p> <pre><code>kubectl delete deploy/frontend svc/frontend\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/blue-green/","title":"Blue-green release of a ML model","text":"<p>This tutorial shows how Iter8 can be used to release ML models hosted in a KServe ModelMesh environment using a blue-green release strategy.  In a blue-green release, a percentage of requests are directed to a candidate version of the model.  This percentage can be changed over time.  The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the blue-green release strategy.</p> <p></p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe ModelMesh Serving. For example, you can create a modelmesh-serving Quickstart environment.  If using the Quickstart environment, your default namespace will be changed to <code>modelmesh-serving</code>. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Install Istio. It suffices to install the demo profile, for example by using:  <pre><code>istioctl install --set profile=demo -y\n</code></pre></li> </ol>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the model using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib\n  strategy: blue-green\nEOF\n</code></pre> <p>Wait for the backend model to be ready:</p> <pre><code>kubectl wait --for condition=ready isvc/wisdom-0 --timeout=600s\n</code></pre> What happens? <ul> <li>Because <code>environment</code> is set to <code>kserve-modelmesh-istio</code>,  an <code>InferenceService</code> object is created.</li> <li>The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.</li> <li>The name <code>wisdom-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.</li> <li>Alternatively, an <code>inferenceServiceSpecification</code> could have been provided.</li> </ul> <p>To support routing, a <code>ServiceEntry</code> named <code>default/wisdom</code> is deployed. Further, an Iter8 routemap is created.</p> <p>Once the <code>InferenceService</code> is ready, the Iter8 controller automatically configures the routing by creating an Istio <code>VirtualService</code>. It is configured to route all inference requests to the only deployed version, <code>wisdom-0</code>.</p>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-routing","title":"Verify routing","text":"<p>You can send verify the routing configuration by inspecting the <code>VirtualService</code>:</p> <pre><code>kubectl get virtualservice wisdom -o yaml\n</code></pre> <p>You can also send inference requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>Send requests: <pre><code>cat grpc_input.json \\\n| grpcurl -vv -plaintext -proto kserve.proto -d @ \\\n  -authority wisdom.modelmesh-serving \\\n  modelmesh-serving.modelmesh-serving:8033 \\\n  inference.GRPCInferenceService.ModelInfer \\\n| grep -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>app-version: wisdom-0\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the model can be deployed simply by adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"s3://modelmesh-example-models/sklearn/mnist-svm.joblib\"\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"s3://modelmesh-example-models/sklearn/mnist-svm.joblib\"\n  strategy: blue-green\nEOF\n</code></pre> About the candidate <p>In this tutorial, the model source (field <code>storageUri</code>) for the candidate version is the same as for the primary version of the model. In a real example, this would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the candidate version is ready, the Iter8 controller will Iter8 will automatically reconfigure the routing so that requests are sent to both versions.</p>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-routing_1","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. Requests will be handled equally by both versions. Output will be something like:</p> <pre><code>app-version: wisdom-0\n...\napp-version: wisdom-1\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#modify-weights-optional","title":"Modify weights (optional)","text":"<p>To modify the request distribution between versions, add a <code>weight</code> to each version. The weights are relative to each other.</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"s3://modelmesh-example-models/sklearn/mnist-svm.joblib\"\n    weight: 30\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"s3://modelmesh-example-models/sklearn/mnist-svm.joblib\"\n    weight: 70\n  strategy: blue-green\nEOF\n</code></pre> <p>Iter8 automatically reconfigures the routing to distribute traffic between the versions based on the new weights.</p>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-routing_2","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. 70 percent of requests will now be handled by the candidate version (<code>wisdom-1</code>); the remaining 30 percent by the primary version (<code>wisdom-0</code>).</p>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#promote-candidate","title":"Promote candidate","text":"<pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib\n  strategy: blue-green\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, <code>storageUri</code> would also be updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary <code>InferenceService</code> ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#verify-routing_3","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>app-version: wisdom-0\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/blue-green/#cleanup","title":"Cleanup","text":"<p>Delete the models are their routing:</p> <pre><code>helm delete wisdom\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/integrations/kserve-mm/canary/","title":"Canary release of a ML model","text":"<p>This tutorial shows how Iter8 can be used to release ML models hosted in a KServe ModelMesh environment using a canary release strategy.  In a canary release, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model.  The remaining requests go to the primary, or initial, version of the model. The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the canary release strategy.</p> <p></p> Before you begin <ol> <li>Ensure that you have the <code>kubectl</code> and <code>helm</code> CLIs installed.</li> <li>Have access to a cluster running KServe ModelMesh Serving. For example, you can create a modelmesh-serving Quickstart environment.  If using the Quickstart environment, your default namespace will be changed to <code>modelmesh-serving</code>. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Install Istio. It suffices to install the demo profile, for example by using:  <pre><code>istioctl install --set profile=demo -y\n</code></pre></li> </ol>"},{"location":"tutorials/integrations/kserve-mm/canary/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kserve-mm/canary/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the model using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib\n  strategy: canary\nEOF\n</code></pre> <p>Wait for the backend model to be ready:</p> <pre><code>kubectl wait --for condition=ready isvc/wisdom-0 --timeout=600s\n</code></pre> What happens? <ul> <li>Because <code>environment</code> is set to <code>kserve-modelmesh-istio</code>,  an <code>InferenceService</code> object is created.</li> <li>The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.</li> <li>The name <code>wisdom-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.</li> <li>Alternatively, an <code>inferenceServiceSpecification</code> could have been provided.</li> </ul> <p>To support routing, a <code>ServiceEntry</code> named <code>default/wisdom</code> is deployed. Further, an Iter8 routemap is created.</p> <p>Once the <code>InferenceService</code> is ready, the Iter8 controller automatically configures the routing by creating an Istio <code>VirtualService</code>. It is configured to route all inference requests to the only deployed version, <code>wisdom-0</code>.</p>"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-routing","title":"Verify routing","text":"<p>You can send verify the routing configuration by inspecting the <code>VirtualService</code>:</p> <pre><code>kubectl get virtualservice wisdom -o yaml\n</code></pre> <p>You can also send inference requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>curl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>To send requests without the header <code>traffic</code>: <pre><code>cat grpc_input.json \\\n| grpcurl -vv -plaintext -proto kserve.proto -d @ \\\n  -authority wisdom.modelmesh-serving \\\n  modelmesh-serving.modelmesh-serving:8033 \\\n  inference.GRPCInferenceService.ModelInfer \\\n| grep -e app-version\n</code></pre></p> </li> <li>Requests can also be sent with the header <code>traffic: test</code>. When a candidate is deployed, requests with this header will be routed to the candidate. When no candidate is deployed, all requests will be routed to the primary version. <pre><code>cat grpc_input.json \\\n| grpcurl -vv -plaintext -proto kserve.proto -d @ \\\n  -H 'traffic: test' \\\n  -authority wisdom.modelmesh-serving \\\n  modelmesh-serving.modelmesh-serving:8033 \\\n  inference.GRPCInferenceService.ModelInfer \\\n| grep -e app-version\n</code></pre></li> </ol> <p>The output includes the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>app-version: wisdom-0\n</code></pre> To send requests from outside the cluster <p>To configure the release for traffic from outside the cluster, a suitable Istio <code>Gateway</code> is required (for example). When using the Iter8 <code>release</code> chart, set the <code>gateway</code> field to the name of your <code>Gateway</code>. Finally, to send traffic:</p> <p>(a) In a separate terminal, port-forward the Istio ingress gateway: <pre><code>kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80\n</code></pre> (b) Download the proto file and sample input: <pre><code>curl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/kserve.proto\ncurl -sO https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/modelmesh-serving/grpc_input.json\n</code></pre> (c) To send requests without the header <code>traffic</code>: <pre><code>cat grpc_input.json | \\\ngrpcurl -vv -plaintext -proto kserve.proto -d @ \\\n-authority wisdom.modelmesh-serving \\\nlocalhost:8080 inference.GRPCInferenceService.ModelInfer \\\n| grep -e app-version\n</code></pre> (d) To send requests with the header <code>traffic: test</code>: <pre><code>cat grpc_input.json | \\\ngrpcurl -vv -plaintext -proto kserve.proto -d @ \\\n-H 'traffic: test' \\\n-authority wisdom.modelmesh-serving \\\nlocalhost:8080 inference.GRPCInferenceService.ModelInfer \\\n| grep -e app-version\n</code></pre></p>"},{"location":"tutorials/integrations/kserve-mm/canary/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate version of the model can be deployed simply by adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    storageUri: \"s3://modelmesh-example-models/sklearn/mnist-svm.joblib\"\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"s3://modelmesh-example-models/sklearn/mnist-svm.joblib\"\n  strategy: canary\nEOF\n</code></pre> About the candidate <p>In this tutorial, the model source (field <code>storageUri</code>) for the candidate version is the same as for the primary version of the model. In a real example, this would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the candidate version is ready, the Iter8 controller will Iter8 will automatically reconfigure the routing so that inference requests with the header <code>traffic</code> set to <code>test</code> will be sent to the candidate model. All other requests will be sent to the primary model.</p>"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-routing_1","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. Those with header <code>traffic</code> set to <code>test</code> will be handled by the candidate model (<code>wisdom-1</code>):</p> <pre><code>app-version: wisdom-1\n</code></pre> <p>All others will be handled by the primary version (<code>wisdom-0</code>):</p> <pre><code>app-version: wisdom-0\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/canary/#promote-candidate","title":"Promote candidate","text":"<p>Redefine the primary to use the candidate model remove the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install wisdom --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: kserve-modelmesh-istio\napplication: \n  metadata:\n    labels:\n      app.kubernetes.io/name: wisdom\n    annotations:\n      serving.kserve.io/secretKey: localMinIO\n  modelFormat: sklearn\n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    storageUri: \"s3://modelmesh-example-models/sklearn/mnist-svm.joblib\"\n  strategy: canary\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, <code>storageUri</code> would also be updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary <code>InferenceService</code> ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/integrations/kserve-mm/canary/#verify-routing_2","title":"Verify Routing","text":"<p>You can verify the routing configuration by inspecting the <code>VirtualService</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>app-version: wisdom-0\n</code></pre>"},{"location":"tutorials/integrations/kserve-mm/canary/#cleanup","title":"Cleanup","text":"<p>Delete the models and their routing:</p> <pre><code>helm delete wisdom\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/integrations/kserve-mm/performance/","title":"Performance testing a ML model","text":"<p>You can load test an ML model deployed to KServe ModelMesh similar to performance testing other gRPC methods. See the Single gRPC method tutorial or the Load test a KServe model (via gRPC) tutorial for examples.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/","title":"Blue-green release","text":"<p>This tutorial shows how Iter8 can be used to release a basic Kubernetes application using a blue-green release strategy.  In a blue-green release, a percentage of requests are directed to a candidate version of the model.  This percentage can be changed over time.  The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the blue-green release strategy.</p> <p></p> <p>This tutorial uses the Kubernetes Gateway API to allow the use any service mesh that supports this API. In this case, we use demonstrate with Linkerd.</p> Before you begin <ol> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> and <code>helm</code> CLIs. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Install Linkerd.</li> </ol>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the application (httpbin) using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used. Note that we deploy the application to the namespace <code>test</code>. </p> About creating a namespace for Linkerd deployments <p>When creating a namespace, it should be annotated so that all created pods are injected with the Linkerd proxy. This can be done, for example, by using the Linkerd CLI: <pre><code>kubectl create ns test --dry-run=client -o yaml | linkerd inject - | kubectl apply -f -\n</code></pre></p> <pre><code>cat &lt;&lt;EOF | helm -n test upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-gtw\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  strategy: blue-green\nEOF\n</code></pre> What happens? <p>The environment defines what kind of application is deployed. In this case (<code>deployment-gtw</code>), a <code>Deployment</code> and a <code>Service</code> object are created.     - The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.     - The name <code>httpbin-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.     - Alternatively, a <code>deploymentSpecification</code> and/or a <code>serviceSpecification</code> could have been specified.</p> <p>To support routing, a <code>Service</code> (<code>httpbin</code>) is deployed. The name is the Helm release name since it not specified in <code>application.metadata</code>. Further, an Iter8 routemap is created. Finally, to support the blue-green release, a <code>ConfigMap</code> (<code>httpbin-0-weight-config</code>) is created to be used to manage the proportion of traffic sent to this version.</p> <p>Once the application components are ready, the Iter8 controller automatically configures the routing by creating an <code>HTTPRoute</code>. It is configured to route all traffic to the only deployed version, <code>httpbin-0</code>.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#verify-routing","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>HTTPRoute</code>:</p> <pre><code>kubectl -n test get httproute.gateway.networking.k8s.io httpbin -o yaml\n</code></pre> <p>You can also send requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>export SERVICE_MESH=linkerd\ncurl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.4/samples/kserve-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>Send requests: <pre><code>curl httpbin.test -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate can deployed by simply adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm -n test upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-gtw\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: blue-green\nEOF\n</code></pre> About the candidate version <p>In this tutorial, the candidate image is the same as the one for the primary version. In a real world example, it would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the second version is deployed and ready, the Iter8 controller automatically reconfigures the routing; the <code>HTTPRoute</code> is updated to distribute traffic between versions based on the weights.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#verify-routing_1","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>HTTPRoute</code> and/or by sending requests as described above. Requests will now be handled equally by both versions. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n...\nHTTP/1.1 200 OK\napp-version: httpbin-1\n</code></pre>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#modify-weights-optional","title":"Modify weights (optional)","text":"<p>To modify the request distribution between the versions, add a <code>weight</code> to each version. The weights are relative to each other.</p> <pre><code>cat &lt;&lt;EOF | helm -n test upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-gtw\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n    weight: 30\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n    weight: 70\n  strategy: blue-green\nEOF\n</code></pre> <p>Iter8 automatically reconfigures the routing to distribute traffic between the versions based on the new weights.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#verify-routing_2","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>HTTPRoute</code> and/or by sending requests as described above. 70 percent of requests will now be handled by the candidate version; the remaining 30 percent by the primary version.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm -n test upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-gtw\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: blue-green\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, the image would also have been updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary version ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#verify-routing_3","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>HTTPRoute</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/integrations/kubernetes-gateway-api/blue-green/#cleanup","title":"Cleanup","text":"<p>Delete the application and its routing configuration:</p> <pre><code>helm -n test delete httpbin\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/","title":"Canary release","text":"<p>This tutorial shows how Iter8 can be used to release a basic Kubernetes application using a canary release strategy.  In a canary release, inference requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model.  The remaining requests go to the primary, or initial, version of the model. The user declaratively describes the desired application state at any given moment.  An Iter8 <code>release</code> chart assists users who describe the application state at any given moment.  The chart provides the configuration needed for Iter8 to automatically deploy application versions and configure the routing to implement the canary release strategy.</p> <p></p> <p>This tutorial uses the Kubernetes Gateway API to allow the use any service mesh that supports this API. In this case, we use demonstrate with Linkerd.</p> Before you begin <ol> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> and <code>helm</code> CLIs. If using a local cluster (for example, Kind or Minikube), we recommend providing the cluster with at least 16GB of memory.</li> <li>Install Linkerd.</li> </ol>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#deploy-initial-version","title":"Deploy initial version","text":"<p>Deploy the initial version of the application (httpbin) using the Iter8 <code>release</code> chart by identifying the environment into which it should be deployed, a list of the versions to be deployed (only one here), and the release strategy to be used. Note that we deploy the application to the namespace <code>test</code>. </p> About creating a namespace for Linkerd deployments <p>When creating a namespace, it should be annotated so that all created pods are injected with the Linkerd proxy. This can be done, for example, by using the Linkerd CLI: <pre><code>kubectl create ns test --dry-run=client -o yaml | linkerd inject - | kubectl apply -f -\n</code></pre></p> <pre><code>cat &lt;&lt;EOF | helm -n test upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-gtw\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  strategy: canary\nEOF\n</code></pre> What happens? <p>The environment defines what kind of application is deployed. In this case (<code>deployment-gtw</code>), a <code>Deployment</code> and a <code>Service</code> object are created.     - The namespace <code>default</code> is inherited from the Helm release namespace since it is not specified in the version or in <code>application.metadata</code>.     - The name <code>httpbin-0</code> is derived from the Helm release name since it is not specified in the version or in <code>application.metadata</code>. The name is derived by appending the index of the version in the list of versions; <code>-0</code> in this case.     - Alternatively, a <code>deploymentSpecification</code> and/or a <code>serviceSpecification</code> could have been specified.</p> <p>To support routing, a <code>Service</code> (<code>httpbin</code>) is deployed. The name is the Helm release name since it not specified in <code>application.metadata</code>. Further, an Iter8 routemap is created.</p> <p>Once the application components are ready, the Iter8 controller automatically configures the routing by creating an <code>HTTPRoute</code>. It is configured to route all traffic to the only deployed version, <code>httpbin-0</code>.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#verify-routing","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>HTTPRoute</code>:</p> <pre><code>kubectl -n test get httproute.gateway.networking.k8s.io httpbin -o yaml\n</code></pre> <p>You can also send requests from a pod within the cluster:</p> <ol> <li> <p>Create a <code>sleep</code> pod in the cluster from which requests can be made: <pre><code>export SERVICE_MESH=linkerd\ncurl -s https://raw.githubusercontent.com/iter8-tools/docs/v0.18.4/samples/kserve-serving/sleep.sh | sh -\n</code></pre></p> </li> <li> <p>Exec into the sleep pod: <pre><code>kubectl exec --stdin --tty \"$(kubectl get pod --sort-by={metadata.creationTimestamp} -l app=sleep -o jsonpath={.items..metadata.name} | rev | cut -d' ' -f 1 | rev)\" -c sleep -- /bin/sh\n</code></pre></p> </li> <li> <p>To send requests without the header <code>traffic</code>: <pre><code>curl httpbin.test -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p> </li> <li> <p>Requests can also be sent with the header <code>traffic: test</code>. When a candidate is deployed, requests with this header will be routed to the candidate. When no candidate is deployed, all requests will be routed to the primary version. <pre><code>curl httpbin.test -H 'traffic: test' -s -D - | grep -e '^HTTP' -e app-version\n</code></pre></p> </li> </ol> <p>The output includes the version of the application that responded (in the <code>app-version</code> response header). In this example:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#deploy-candidate","title":"Deploy candidate","text":"<p>A candidate can deployed by simply adding a second version to the list of versions comprising the application:</p> <pre><code>cat &lt;&lt;EOF | helm -n test upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-gtw\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v0\n    image: kennethreitz/httpbin\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n    matches:\n    - headers:\n      - name: traffic\n        value: test\n  strategy: canary\nEOF\n</code></pre> About the candidate version <p>In this tutorial, the candidate image is the same as the one for the primary version. In a real world example, it would be different. The version label (<code>app.kubernetes.io/version</code>) can be used to distinguish between versions.</p> <p>When the second version is deployed and ready, the Iter8 controller automatically reconfigures the routing so that requests with the header <code>traffic</code> set to <code>test</code> will be sent to the candidate. All other requests will be sent to the primary version.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#verify-routing_1","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>HTTPRoute</code> and/or by sending requests as described above. Those with header <code>traffic</code> set to <code>test</code> will be handled by the candidate version:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-1\n</code></pre> <p>All others will be handled by the primary version:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#promote-candidate","title":"Promote candidate","text":"<p>The candidate can be promoted by redefining the primary version and removing the candidate:</p> <pre><code>cat &lt;&lt;EOF | helm -n test upgrade --install httpbin --repo https://iter8-tools.github.io/iter8 release --version 1.1 -f -\nenvironment: deployment-gtw\napplication: \n  versions:\n  - metadata:\n      labels:\n        app.kubernetes.io/version: v1\n    image: kennethreitz/httpbin\n  strategy: canary\nEOF\n</code></pre> What is different? <p>The version label (<code>app.kubernetes.io/version</code>) of the primary version was updated. In a real world example, the image would also have been updated (with that from the candidate version).</p> <p>Once the (reconfigured) primary version ready, the Iter8 controller will automatically reconfigure the routing to send all requests to it.</p>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#verify-routing_2","title":"Verify routing","text":"<p>You can verify the routing configuration by inspecting the <code>HTTPRoute</code> and/or by sending requests as described above. They will all be handled by the primary version. Output will be something like:</p> <pre><code>HTTP/1.1 200 OK\napp-version: httpbin-0\n</code></pre>"},{"location":"tutorials/integrations/kubernetes-gateway-api/canary/#cleanup","title":"Cleanup","text":"<p>Delete the application and its routing configuration:</p> <pre><code>helm -n test delete httpbin\n</code></pre> <p>If you used the <code>sleep</code> pod to generate load, remove it:</p> <pre><code>kubectl delete deploy sleep\n</code></pre> <p>Uninstall Iter8 controller:</p> <pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p>"},{"location":"tutorials/performance/load-test-grpc-multiple/","title":"Load test multiple gRPC methods","text":"<p>This tutorial expands on the tutorial Single gRPC method by describing how to load test multiple gRPC methods.</p> <p></p> Before you begin <ol> <li>Try Your first performance test. Understand the main concepts behind Iter8.</li> <li>Deploy the sample gRPC service in the Kubernetes cluster. <pre><code>kubectl create deployment routeguide --image=golang --port=50051 \\\n-- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i \"''\" \"'\"s/localhost//\"'\" server/server.go; go run server/server.go\"\nkubectl expose deployment routeguide --port=50051\n</code></pre></li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"tutorials/performance/load-test-grpc-multiple/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/performance/load-test-grpc-multiple/#launch-performance-test","title":"Launch performance test","text":"<pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={ready,grpc}\" \\\n--set ready.deploy=routeguide \\\n--set ready.service=routeguide \\\n--set ready.timeout=60s \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.endpoints.getFeature.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.endpoints.getFeature.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\\n--set grpc.endpoints.listFeatures.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.endpoints.listFeatures.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json\n</code></pre> About this performance test <p>This performance test consists of two tasks, namely, ready, and grpc.</p> <p>The ready task checks if the <code>routeguide</code> deployment exists and is available, and the <code>routeguide</code> service exists. </p> <p>The grpc task sends call requests to two methods of the cluster-local gRPC service, and collects Iter8's built-in gRPC load test metrics. The two methods are <code>routeguide.RouteGuide.GetFeature</code> and <code>routeguide.RouteGuide.ListFeatures</code>. Note that each method also has its own <code>dataURL</code> for the request payload.</p>"},{"location":"tutorials/performance/load-test-grpc-multiple/#view-results-using-grafana","title":"View results using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>routeguide-test</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/grpcDashboard</code> </li> <li>Query string: <code>namespace=default&amp;test=routeguide-test</code></li> </ul> <p>Create a new dashboard by import. Paste the contents of the <code>grpc</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above.</p> <p>The Iter8 dashboard will look like the following:</p> <p></p>"},{"location":"tutorials/performance/load-test-grpc-multiple/#view-logs","title":"View logs","text":"<p>Logs are useful for debugging.</p> <pre><code>kubectl logs -l iter8.tools/test=routeguide-test\n</code></pre>"},{"location":"tutorials/performance/load-test-grpc-multiple/#cleanup","title":"Cleanup","text":"<p>Remove the performance test and the sample app from the Kubernetes cluster.</p> <pre><code>helm delete routeguide-test\nkubectl delete svc/routeguide\nkubectl delete deploy/routeguide\n</code></pre>"},{"location":"tutorials/performance/load-test-grpc-multiple/#uninstall-the-iter8-controller","title":"Uninstall the Iter8 controller","text":"<pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre> Some variations and extensions of this performance test <ol> <li>The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections.</li> </ol>"},{"location":"tutorials/performance/load-test-grpc/","title":"Load test gRPC method","text":"<p>Load test a Kubernetes gRPC service and visualize the performance metrics with an Iter8 Grafana dashboard.</p> <p></p> Before you begin <ol> <li>Try Your first performance test. Understand the main concepts behind.</li> <li> <p>Deploy the sample gRPC service in the Kubernetes cluster. <pre><code>kubectl create deployment routeguide --image=golang --port=50051 \\\n-- bash -c \"git clone -b v1.52.0 --depth 1 https://github.com/grpc/grpc-go; cd grpc-go/examples/route_guide; sed -i \"''\" \"'\"s/localhost//\"'\" server/server.go; go run server/server.go\"\nkubectl expose deployment routeguide --port=50051\n</code></pre></p> </li> <li> <p>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></p> </li> </ol>"},{"location":"tutorials/performance/load-test-grpc/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/performance/load-test-grpc/#launch-performance-test","title":"Launch performance test","text":"Unary exampleServer streaming exampleClient streaming exampleBidirectional example <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={ready,grpc}\" \\\n--set ready.deploy=routeguide \\\n--set ready.service=routeguide \\\n--set ready.timeout=60s \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json\n</code></pre> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={ready,grpc}\" \\\n--set ready.deploy=routeguide \\\n--set ready.service=routeguide \\\n--set ready.timeout=60s \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json\n</code></pre> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={ready,grpc}\" \\\n--set ready.deploy=routeguide \\\n--set ready.service=routeguide \\\n--set ready.timeout=60s \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.call=routeguide.RouteGuide.RecordRoute \\\n--set grpc.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json\n</code></pre> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={ready,grpc}\" \\\n--set ready.deploy=routeguide \\\n--set ready.service=routeguide \\\n--set ready.timeout=60s \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.call=routeguide.RouteGuide.RouteChat \\\n--set grpc.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/bidirectional.json\n</code></pre> About this performance test <p>This performance test consists of two tasks, namely, ready, and grpc.</p> <p>The ready task checks if the <code>routeguide</code> deployment exists and is available, and the <code>routeguide</code> service exists. </p> <p>The grpc task sends call requests to the specified method of the cluster-local gRPC service with host address <code>routeguide.default:50051</code> and collects Iter8's built-in gRPC load test metrics. This task supports all four gRPC service methods: unary, server streaming, client streaming, and bidirectional streaming, and will provide payload in the appropriate manner using <code>dataURL</code>.</p>"},{"location":"tutorials/performance/load-test-grpc/#view-results-using-grafana","title":"View results using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>routeguide-test</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/grpcDashboard</code> </li> <li>Query string: <code>namespace=default&amp;test=routeguide-test</code></li> </ul> <p>Create a new dashboard by import. Paste the contents of the <code>grpc</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above.</p> <p>The Iter8 dashboard will look like the following:</p> <p></p>"},{"location":"tutorials/performance/load-test-grpc/#view-logs","title":"View logs","text":"<p>Logs are useful for debugging.</p> <pre><code>kubectl logs -l iter8.tools/test=routeguide-test\n</code></pre>"},{"location":"tutorials/performance/load-test-grpc/#cleanup","title":"Cleanup","text":"<p>Remove the performance test and the sample app from the Kubernetes cluster.</p> <pre><code>helm delete routeguide-test\nkubectl delete svc/routeguide\nkubectl delete deploy/routeguide\n</code></pre>"},{"location":"tutorials/performance/load-test-grpc/#uninstall-the-iter8-controller","title":"Uninstall the Iter8 controller","text":"<pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre> Some variations and extensions of this performance test <ol> <li>The grpc task can be configured with load related parameters such as the total number of requests, requests per second, or number of concurrent connections.</li> </ol>"},{"location":"tutorials/performance/load-test-http-multiple/","title":"Load test multiple HTTP endpoints","text":"<p>Your first performance test describes how to load test a single HTTP service. This tutorial expands on the previous tutorial and describes how to load test multiple HTTP endpoints.</p> <p></p> Before you begin <ol> <li>Try Your first performance test. Understand the main concepts behind Iter8.</li> <li>Ensure that you have a Kubernetes cluster and the <code>kubectl</code> CLI. You can create a local Kubernetes cluster using tools like Kind or Minikube.</li> <li>Deploy the sample HTTP service in the Kubernetes cluster. <pre><code>kubectl create deploy httpbin --image=kennethreitz/httpbin --port=80\nkubectl expose deploy httpbin --port=80\n</code></pre></li> <li>Have Grafana available. For example, Grafana can be installed on your cluster as follows: <pre><code>kubectl create deploy grafana --image=grafana/grafana\nkubectl expose deploy grafana --port=3000\n</code></pre></li> </ol>"},{"location":"tutorials/performance/load-test-http-multiple/#install-the-iter8-controller","title":"Install the Iter8 controller","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>For additional install options, see Iter8 Installation.</p>"},{"location":"tutorials/performance/load-test-http-multiple/#launch-performance-test","title":"Launch performance test","text":"<pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={ready,http}\" \\\n--set ready.deploy=httpbin \\\n--set ready.service=httpbin \\\n--set ready.timeout=60s \\\n--set http.endpoints.get.url=http://httpbin.default/get \\\n--set http.endpoints.getAnything.url=http://httpbin.default/anything \\\n--set http.endpoints.post.url=http://httpbin.default/post \\\n--set http.endpoints.post.payloadStr=hello\n</code></pre> About this performance test <p>This performance test consists of two tasks, namely, ready and http.</p> <p>The ready task checks if the <code>httpbin</code> deployment exists and is available, and the <code>httpbin</code> service exists. </p> <p>The http task sends requests to three endpoints from the cluster-local HTTP service, and collects Iter8's built-in HTTP load test metrics. The three endpoints are <code>http://httpbin.default/get</code>, <code>http://httpbin.default/anything</code>, and <code>http://httpbin.default/post</code>. The last endpoint also has a payload string <code>hello</code>.</p>"},{"location":"tutorials/performance/load-test-http-multiple/#view-results-using-grafana","title":"View results using Grafana","text":"<p>Inspect the metrics using Grafana. If Grafana is deployed to your cluster, port-forward requests as follows:</p> <pre><code>kubectl port-forward service/grafana 3000:3000\n</code></pre> <p>Open Grafana in a browser by going to http://localhost:3000 and login. The default username/password are <code>admin</code>/<code>admin</code>.</p> <p>Add a JSON API data source <code>httpbin-test</code> with the following parameters:</p> <ul> <li>URL: <code>http://iter8.default:8080/httpDashboard</code> </li> <li>Query string: <code>namespace=default&amp;test=httpbin-test</code></li> </ul> <p>Create a new dashboard by import. Paste the contents of the <code>http</code> Grafana dashboard into the text box and load it. Associate it with the JSON API data source defined above.</p> <p>The Iter8 dashboard will look like the following:</p> <p></p>"},{"location":"tutorials/performance/load-test-http-multiple/#view-logs","title":"View logs","text":"<p>Logs are useful for debugging.</p> <pre><code>kubectl logs -l iter8.tools/test=httpbin-test\n</code></pre>"},{"location":"tutorials/performance/load-test-http-multiple/#cleanup","title":"Cleanup","text":"<p>Remove the performance test and the sample app from the Kubernetes cluster.</p> <pre><code>helm delete httpbin-test\nkubectl delete svc/httpbin\nkubectl delete deploy/httpbin\n</code></pre>"},{"location":"tutorials/performance/load-test-http-multiple/#uninstall-the-iter8-controller","title":"Uninstall the Iter8 controller","text":"<pre><code>helm delete iter8\n</code></pre> <p>For additional uninstall options, see Iter8 Uninstall.</p> <p>If you installed Grafana, you can delete it as follows:</p> <pre><code>kubectl delete svc/grafana deploy/grafana\n</code></pre> Some variations and extensions of this performance test <ol> <li>The http task can be configured with load related parameters such as the number of requests, queries per second, or number of parallel connections.</li> <li>The http task can be configured to send various types of content as payload.</li> </ol>"},{"location":"tutorials/performance/load-test-http/","title":"Load test HTTP endpoint","text":"<p>Your first performance test describes how to load test a HTTP service.</p>"},{"location":"user-guide/install/","title":"Install options","text":"<p>By default, Iter8 uses BadgerDB to store metrics from A/B/n and performance tests. BadgerDB is not suitable for production use. To install for production, use Redis.</p>"},{"location":"user-guide/install/#install-with-helm","title":"Install with <code>helm</code>","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller\n</code></pre> <pre><code>helm install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller \\\n--set clusterScoped=true\n</code></pre> <p>To install Iter8 in a non-default namespace, use the <code>-n</code> option.</p>"},{"location":"user-guide/install/#install-with-kustomize","title":"Install with <code>kustomize</code>","text":"<p>Iter8 can be installed and configured to watch resources either in a single namespace (namespace-scoped) or in the whole cluster (cluster-scoped). </p> Namespace-scopedCluster-scoped <pre><code>kubectl apply -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/namespaceScoped?ref=v1.1.1'\n</code></pre> <pre><code>kubectl apply -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/clusterScoped?ref=v1.1.1'\n</code></pre> <p>To install Iter8 in a non-default namespace, download the <code>kustomize</code> folder and modify the <code>namespace</code> field in the <code>kustomization.yaml</code> file.</p>"},{"location":"user-guide/install/#install-using-rancher-desktop","title":"Install using Rancher Desktop","text":"<p>Rancher does not support a <code>standard</code> storage class by default. Install the local path provisioner:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml\n</code></pre> <p>And set <code>metrics.badgerdb.storageClassName</code> when starting the Iter8 controller:</p> <pre><code>--set metrics.badgerdb.storageClassName=local-path\n</code></pre>"},{"location":"user-guide/install/#install-on-opendatahub","title":"Install on OpenDataHub","text":"<p>See https://github.com/opendatahub-io-contrib/odh-contrib-manifests/tree/main/iter8</p>"},{"location":"user-guide/metrics_store/","title":"Metrics store","text":"<p>Iter8 provides a metrics store for A/B/n and performance metrics. This simplifies the set up and execution of test scenarios. Iter8 currently supports the following databases:</p> <ul> <li>BadgerDB</li> <li>Redis</li> </ul> <p>By default, Iter8 uses BadgerDB. Note, however, that BadgerDB is not suitable for production use and is only suitable for a single instance of Iter8. Support for other databases is in the works. See below for details on how you can contribute additional implementations.</p>"},{"location":"user-guide/metrics_store/#badgerdb-configuration-options","title":"BadgerDB configuration options","text":"<p>BadgerDB is a file system-based database. When installing Iter8, the following options can be configured:</p> <ul> <li><code>bagderdb.storage</code> - The amount of space allocated to BadgerDB (default 50Mi).</li> <li><code>badgerdb.storageClassName</code> - The Kubernetes storage class that should be used. The default (<code>standard</code>) may not work for all clusters.</li> </ul>"},{"location":"user-guide/metrics_store/#redis-configuration-options","title":"Redis configuration options","text":"<p>We assume that Redis is deployed. For example, for a basic deployment:</p> <pre><code>kubectl create deploy redis --image=redis/redis-stack:latest --port=6379\nkubectl expose deploy redis --port=6379\n</code></pre> <p>Run Iter8 with the metrics store implementation set to <code>redis</code> and specify its endpoint:</p> <pre><code>helm upgrade --install --repo https://iter8-tools.github.io/iter8 --version 1.1 iter8 controller  \\\n--set clusterScoped=true \\\n--set metrics.implementation=redis \\\n--set metrics.redis.addresss=redis:6379\n</code></pre>"},{"location":"user-guide/metrics_store/#contribute-a-new-metrics-store-implementation","title":"Contribute a new metrics store implementation","text":"<p>To contribute a new metrics store implementation:</p> <ol> <li> <p>Create an issue for discussion.</p> </li> <li> <p>Submit a pull request on the Iter8 project with the following updates:</p> <ul> <li> <p>Create sub-folder in storage and provide an implementation of this interface including test cases.</p> </li> <li> <p>Add a new case to metrics.GetClient()</p> </li> <li> <p>Update go.mod and go.sum if needed.</p> </li> <li> <p>Update the default Helm chart configuration values.yaml and bump the chart version in Chart.yaml.</p> </li> <li> <p>Please also consider including your information in our list of adopters.</p> </li> </ul> </li> <li> <p>Submit a second pull request on the Iter8 docs project updating the list of available implementations.</p> </li> <li> <p>Alert the project reviewers on Slack <code>#development</code> channel.</p> </li> </ol>"},{"location":"user-guide/routemap/","title":"Routemaps","text":"<p>A routemap contains a description of each version of an application and may contain one or more routing templates. The description of versions is used by Iter8 to identify which versions of the application are available at any moment. Whenever versions become available or disappear, any defined routing templates will be applied. This results in the automatic reconfiguration of the routing.</p>"},{"location":"user-guide/routemap/#version-list","title":"Version list","text":"<p>A version is a list of resources that, when available and ready, indicate that the version is available. For example, the following describes a application with two versions. In this case, each version has a <code>Service</code> and a <code>Deployment</code>. In one version, they are named <code>httpbin-0</code>, and in the other, <code>httpbin-1</code>:</p> <pre><code>versions:\n- resources:\n  - gvrShort: svc\n    name: httpbin-0\n    namespace: default\n  - gvrShort: deploy\n    name: httpbin-0\n    namespace: default\n- resources:\n  - gvrShort: svc\n    name: httpbin-1\n    namespace: default\n  - gvrShort: deploy\n    name: httpbin-1\n    namespace: default\n</code></pre> <p>Note that the resources types are specified using a short name. In this example, <code>svc</code> and <code>deploy</code>. A short name is used to simplify the specification of the resources in a version. A mapping of short name to Kubernetes group, version, resource is captured in the configuration of the Iter8 controller. This set can be extended to include any types including custom resources; that is, those defined by a CRD.</p> <p>A version may optionally specify an integer <code>weight</code> indicating the proportion of traffic that should be sent to it relative to other versions.</p>"},{"location":"user-guide/routemap/#routing-templates","title":"Routing templates","text":"<p>A routing template is a Go template that is applied each time a new version becomes available or an old one goes away. Multiple templates can be defined/applied.</p> <p>THe application of the templates allows Iter8 to automatically reconfigure the routing when versions come and go. For example, the template created in the blue-green release tutorial contains an Istio <code>VirtualService</code>. Applying the template to the available versions yields the necessary <code>VirtualSerivce</code> definition. In this tutorial, the template definition is:</p> <pre><code>blue-green:\n  gvrShort: vs\n  template: |\n    apiVersion: networking.istio.io/v1beta1\n    kind: VirtualService\n    metadata:\n      name: httpbin\n    spec:\n      gateways:\n      - mm-gateway\n      - mesh\n      hosts:\n      - httpbin.default\n      - httpbin.default.svc\n      - httpbin.default.svc.cluster.local\n      http:\n      - route:\n        # primary version\n        - destination:\n            host: httpbin-0.default.svc.cluster.local\n          {{- if gt (index .Weights 1) 0 }}\n          weight: {{ index .Weights 0 }}\n          {{- end }}\n          headers:\n            response:\n              add:\n                app-version: httpbin-0\n        # other versions\n        {{- if gt (index .Weights 1) 0 }}\n        - destination:\n            host: httpbin-1.default.svc.cluster.local\n          weight: {{ index .Weights 1 }}\n          headers:\n            response:\n              add:\n                app-version: httpbin-1\n        {{- end }}\n</code></pre>"},{"location":"user-guide/routemap/#implementation","title":"Implementation","text":"<p>A routemap is implemented as a <code>ConfigMap</code> with the following labels: - <code>iter8.tools/kind</code> with value <code>routemap</code> - <code>iter8.tools/version</code> with value corresponding the version of the controller being used</p> <p>The list of versions and routing templates are stored as stringified YAML.</p>"},{"location":"user-guide/uninstall/","title":"Uninstall","text":""},{"location":"user-guide/uninstall/#uninstall-with-helm","title":"Uninstall with <code>helm</code>","text":"<p>If installed with <code>helm</code>, uninstall with:</p> <pre><code>helm delete iter8\n</code></pre>"},{"location":"user-guide/uninstall/#uninstall-with-kustomize","title":"Uninstall with <code>kustomize</code>","text":"<p>If installed with <code>kustomize</code>, uninstall with one of the following:</p> Namespace-scopedCluster-scoped <pre><code>kubectl delete -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/namespaceScoped?ref=v1.1.1'\n</code></pre> <pre><code>kubectl delete -k 'https://github.com/iter8-tools/iter8.git/kustomize/controller/clusterScoped?ref=v1.1.1'\n</code></pre>"},{"location":"user-guide/abn/about/","title":"A/B/n Testing","text":"<p>A/B/n testing relies on business metrics typically computed by a frontend, user-facing, application component. </p> <p></p> <p>Metric values often depend on one or more interactions with backend (not user-facing) application components. To run an A/B/n test on a backend component, it is necessary to be able to associate a metric value (computed by the frontend component) to the version of the backend component that contributed to its computation.  The challenge is that the frontend component often does not know which version of the backend component processed a given request. To address this challenge, Iter8 introduces an A/B/n SDK. </p> <p>The Iter8 SDK uses a fixed set of versions numbers (<code>0</code>, <code>1</code>, etc.) as a way to refer to the current set of versions of a Kubernetes application or ML model. The version of the application associated with a given version number changes over time as new versions are developed, deployed for testing, and either promoted or deleted. Since the set of version numbers is fixed, they can be used to configure routing to the application.</p> <p>The Iter8 SDK provides two APIs to frontend application components:</p> <p>a. Lookup() - Given an application and user session, returns a version number to be used as an index to a table of routes. So long as there are no changes in configuration, the version number (and hence the route) will be same for the same user session, guaranteeing session stickiness.</p> <p>b. WriteMetric() -  Given an application, a user session, a metric name its value, WriteMetric() associates the metric value with the appropriate version of the application. </p>"},{"location":"user-guide/abn/about/#configuring-the-iter8-controller","title":"Configuring the Iter8 controller","text":"<p>The Iter8 controller is implemented using gRPC. The service is configured to watch a given set of Kubernetes resource types. The default set of types Iter8 can watch are identified in the default <code>values.yaml</code> file. Other configuration options are described in the same file.</p> <p>To configure the specific resources to watch for a given application, a Kubernetes <code>ConfigMap</code> is created. It identifies the specific resources that comprise each version. For example, consider the <code>ConfigMap</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: backend\n  labels:\n    app.kubernetes.io/managed-by: iter8\n    iter8.tools/kind: routemap\n    iter8.tools/version: \"v1.1\"\nimmutable: true\ndata:\n  strSpec: |\n    versions:\n    - resources:\n      - gvrShort: svc\n        name: backend\n        namespace: default\n      - gvrShort: deploy\n        name: backend\n        namespace: default\n    - resources:\n      - gvrShort: svc\n        name: backend-candidate-1\n        namespace: default\n      - gvrShort: deploy\n        name: backend-candidate-1\n        namespace: default\n</code></pre> <p>This <code>ConfigMap</code> describes an application <code>backend</code>. It identifies two versions of the application. The first is comprised of a Kubernetes <code>Deployment</code> and a <code>Service</code> object both named <code>backend</code> in the <code>default</code> namespace.  The second is comprised of the same resource types named <code>backend-candidate-1</code> in the same namespace.</p>"},{"location":"user-guide/abn/about/#deployment-time-configuration-of-backend-components","title":"Deployment time configuration of backend components","text":"<p>As versions of a watched application are deployed or deleted, the Iter8 controller keeps track of which versions are available enabling it to respond appropriately to <code>Lookup()</code> and <code>WriteMetric()</code> requests.</p>"},{"location":"user-guide/abn/about/#developing-frontend-components-using-the-sdk","title":"Developing frontend components: Using the SDK","text":"<p>The basic steps to author a frontend application component using the Iter8 SDK are outlined below for Node.js and Go. Similar steps would be required for any gRPC supported language.</p>"},{"location":"user-guide/abn/about/#useimport-language-specific-libraries","title":"Use/import language specific libraries","text":"<p>The gRPC protocol buffer definition is used to generate language specific implementation. These files can be used directly or packaged and imported as a library. As examples, the Node.js sample uses manually generated files directly. On the other hand, the Go sample imports the library provided by the core Iter8 service implementation. In addition to the API specific methods, some general gRPC libraries are required.</p> Node.jsGo <p>The manually generated node files <code>abn_pd.js</code> and <code>abn_grpc_pb.js</code> used in the sample application can be copied and used without modification.</p> <pre><code>var grpc = require('@grpc/grpc-js');\n\nvar messages = require('./abn_pb.js');\nvar services = require('./abn_grpc_pb.js');\n</code></pre> <pre><code>import (\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/credentials/insecure\"\n\n    pb \"github.com/iter8-tools/iter8/abn/grpc\"\n)\n</code></pre>"},{"location":"user-guide/abn/about/#instantiate-a-grpc-client","title":"Instantiate a gRPC client","text":"<p>Instantiate a client to the Iter8 controller:</p> Node.jsGo <pre><code>var client = new services.ABNClient(abnEndpoint, grpc.credentials.createInsecure());\n</code></pre> <pre><code>opts := []grpc.DialOption{grpc.WithTransportCredentials(insecure.NewCredentials())}\nconn, err := grpc.Dial(fmt.Sprintf(\"%s:%s\", getAbnService(), getAbnServicePort()), opts...)\nif err != nil {\n    panic(\"Cannot establish connection with abn service\")\n}\nc := pb.NewABNClient(conn)\nclient = &amp;c\n</code></pre>"},{"location":"user-guide/abn/about/#define-routing","title":"Define routing","text":"<p>Track identifiers are mapped to a static set of endpoints. One approach is to maintain a map from track identifier to endpoint:</p> Node.jsGo <pre><code>const versionNumberToRoute = [\n    \"http://backend.default.svc.cluster.local:8091\",\n    \"http://backend-candidate-1.default.svc.cluster.local:8091\",\n]\n</code></pre> <pre><code>versionNumberToRoute = []string{\n    \"http://backend.default.svc.cluster.local:8091\",\n    \"http://backend-candidate-1.default.svc.cluster.local:8091\",\n}\n</code></pre>"},{"location":"user-guide/abn/about/#using-lookup","title":"Using <code>Lookup()</code>","text":"<p>Given a user session identifier, <code>Lookup()</code> returns a version number that can be used to route requests. In code sample below, the user session identifier is assumed to be passed in the <code>X-User</code> header of user requests. The version number is used as an index to the <code>versionNumberToRoute</code> map defined above. A default is used if the call to <code>Lookup()</code> fails for any reason.</p> Node.jsGo <pre><code>var application = new messages.Application();\napplication.setName('default/backend');\napplication.setUser(req.header('X-User'));\nclient.lookup(application, function(err, versionRecommendation) {\n    if (err) {\n        // use default route (see above)\n        console.warn(\"error calling Lookup()\")\n    } else {\n        // use route determined by recommended track\n        console.info('lookup suggested track %d', versionRecommendation.getVersionnumber())\n        versionNumber = versionRecommendation.getVersionnumber()\n        if (versionNumber != NaN &amp;&amp; 0 &lt;= versionNumber &amp;&amp; versionNumber &lt; versionNumberToRoute.length) {\n            route = versionNumberToRoute[versionNumber]\n        }\n    }\n\n    // call backend service using route\n    ...\n});\n</code></pre> <pre><code>route := versionNumberToRoute[0]\nuser := req.Header[\"X-User\"][0]\ns, err := (*client).Lookup(\n    ctx,\n    &amp;pb.Application{\n        Name: \"default/backend\",\n        User: user,\n    },\n)\nif err == nil &amp;&amp; s != nil {\n    versionNumber := int(s.GetVersionNumber())\n    if err == nil &amp;&amp; 0 &lt;= versionNumber &amp;&amp; versionNumber &lt; len(versionNumberToRoute) {\n        route = versionNumberToRoute[versionNumber]\n    } // else use default value for route\n}\n\n// call backend service using route\n...\n</code></pre>"},{"location":"user-guide/abn/about/#using-writemetric","title":"Using WriteMetric()","text":"<p>As an example, a single metric named sample_metric is assigned a random value between 0 and 100 and written.</p> Node.jsGo <pre><code>var mv = new messages.MetricValue();\nmv.setName('sample_metric');\nmv.setValue(random({min: 0, max: 100, integer: true}).toString());\nmv.setApplication('default/backend');\nmv.setUser(user);\nclient.writeMetric(mv, function(err, session) {});\n</code></pre> <pre><code>_, _ = (*client).WriteMetric(\n    ctx,\n    &amp;pb.MetricValue{\n        Name:        \"sample_metric\",\n        Value:       fmt.Sprintf(\"%f\", rand.Float64()*100.0),\n        Application: \"default/backend\",\n        User:        user,\n    },\n)\n</code></pre>"},{"location":"user-guide/abn/extension/","title":"Using new resource types","text":"<p>Like progressive release, A/B/n tests use the Iter8 <code>release</code> chart. Extending this chart for a new resource type is similar to extending it for progressive release. The key difference is in the definition of the routemap. We briefly describe how to extend the chart for an Knative application.</p>"},{"location":"user-guide/abn/extension/#example-knative-service","title":"Example (Knative Service)","text":"<p>For example, to extend the chart to support A/B/n testing for Knative services, the following files could be added:</p> <ul> <li><code>_knative-istio.tpl</code> - wrapper for all objects that should be deployed</li> <li><code>_knative-istio.version.ksvc.tpl</code> - the Knative service object that should be deployed for a version</li> <li><code>_knative-istio.none.tpl</code> - wrapper for all objects that should be deployed to support the no automated traffic pattern</li> <li><code>_knative-istio.none.routemap.tpl</code> - the routemap definition</li> <li><code>_knative.helpers.tpl</code> - supporting functions</li> </ul> <p>An implementation of these is here.</p> <p>Note that many of these additions are the same as in the example for progressive release. Of the above files, only <code>_knative-istio.tpl</code> is different. </p> <p>Finally, update <code>release.yaml</code> to include <code>knative-istio</code> as a valid option:</p> <pre><code>{{- else if eq \"knative-istio\" .Values.environment }}\n  {{- include \"env.knative-istio\" . }}\n</code></pre>"},{"location":"user-guide/abn/extension/#extend-the-controller","title":"Extend the controller","text":"<p>The Iter8 controller will need to be restarted with permission to watch Knative service objects. Re-install the controller using the following additional options:</p> <pre><code>--set resourceTypes.ksvc.Group=serving.knative.dev \\\n--set resourceTypes.ksvc.Version=v1 \\\n--set resourceTypes.ksvc.Resource=services \\\n--set \"resourceTypes.ksvc.conditions[0]=Ready\"\n</code></pre>"},{"location":"user-guide/abn/extension/#using-the-modified-chart","title":"Using the modified chart","text":"<p>To use the modified chart to run A/B tests for Knative services, see the example for progressive release. Unset the <code>strategy</code> and <code>weight</code> fields.</p>"},{"location":"user-guide/abn/using-sdk/","title":"Using the SDK","text":"<p>The basic steps to author a frontend application component using the Iter8 SDK are outlined below for Node.js and Go. Similar steps would be required for any gRPC-supported language.</p>"},{"location":"user-guide/abn/using-sdk/#useimport-language-specific-libraries","title":"Use/import language specific libraries","text":"<p>The gRPC protocol buffer definition is used to generate a language-specific implementation. These files can be used directly or packaged and imported as a library. As examples, the Node.js sample uses manually generated files directly. On the other hand, the Go sample imports the library provided by the core Iter8 service implementation. In addition to the API specific methods, some general gRPC libraries are required.</p> Node.jsGo <p>The manually generated node files <code>abn_pd.js</code> and <code>abn_grpc_pb.js</code> used in the sample application can be copied and used without modification.</p> <pre><code>var grpc = require('@grpc/grpc-js');\n\nvar messages = require('./abn_pb.js');\nvar services = require('./abn_grpc_pb.js');\n</code></pre> <pre><code>import (\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/credentials/insecure\"\n\n    pb \"github.com/iter8-tools/iter8/abn/grpc\"\n)\n</code></pre>"},{"location":"user-guide/abn/using-sdk/#instantiate-a-grpc-client","title":"Instantiate a gRPC client","text":"<p>Instantiate a client to the Iter8 controller:</p> Node.jsGo <pre><code>var client = new services.ABNClient(abnEndpoint, grpc.credentials.createInsecure());\n</code></pre> <pre><code>opts := []grpc.DialOption{grpc.WithTransportCredentials(insecure.NewCredentials())}\nconn, err := grpc.Dial(fmt.Sprintf(\"%s:%s\", getAbnService(), getAbnServicePort()), opts...)\nif err != nil {\n    panic(\"Cannot establish connection with abn service\")\n}\nc := pb.NewABNClient(conn)\nclient = &amp;c\n</code></pre>"},{"location":"user-guide/abn/using-sdk/#define-routing","title":"Define routing","text":"<p>Track identifiers are mapped to a static set of endpoints. One approach is to maintain a map from track identifier to endpoint:</p> Node.jsGo <pre><code>const versionNumberToRoute = [\n    \"http://backend.default.svc.cluster.local:8091\",\n    \"http://backend-candidate-1.default.svc.cluster.local:8091\",\n]\n</code></pre> <pre><code>versionNumberToRoute = []string{\n    \"http://backend.default.svc.cluster.local:8091\",\n    \"http://backend-candidate-1.default.svc.cluster.local:8091\",\n}\n</code></pre>"},{"location":"user-guide/abn/using-sdk/#using-lookup","title":"Using <code>Lookup()</code>","text":"<p>Given a user session identifier, <code>Lookup()</code> returns a version number that can be used to route requests. In code sample below, the user session identifier is assumed to be passed in the <code>X-User</code> header of user requests. The version number is used as an index to the <code>versionNumberToRoute</code> map defined above. A default is used if the call to <code>Lookup()</code> fails for any reason.</p> Node.jsGo <pre><code>var application = new messages.Application();\napplication.setName('default/backend');\napplication.setUser(req.header('X-User'));\nclient.lookup(application, function(err, versionRecommendation) {\n    if (err) {\n        // use default route (see above)\n        console.warn(\"error calling Lookup()\")\n    } else {\n        // use route determined by recommended track\n        console.info('lookup suggested track %d', versionRecommendation.getVersionnumber())\n        versionNumber = versionRecommendation.getVersionnumber()\n        if (versionNumber != NaN &amp;&amp; 0 &lt;= versionNumber &amp;&amp; versionNumber &lt; versionNumberToRoute.length) {\n            route = versionNumberToRoute[versionNumber]\n        }\n    }\n\n    // call backend service using route\n    ...\n});\n</code></pre> <pre><code>route := versionNumberToRoute[0]\nuser := req.Header[\"X-User\"][0]\ns, err := (*client).Lookup(\n    ctx,\n    &amp;pb.Application{\n        Name: \"default/backend\",\n        User: user,\n    },\n)\nif err == nil &amp;&amp; s != nil {\n    versionNumber := int(s.GetVersionNumber())\n    if err == nil &amp;&amp; 0 &lt;= versionNumber &amp;&amp; versionNumber &lt; len(versionNumberToRoute) {\n        route = versionNumberToRoute[versionNumber]\n    } // else use default value for route\n}\n\n// call backend service using route\n...\n</code></pre>"},{"location":"user-guide/abn/using-sdk/#using-writemetric","title":"Using WriteMetric()","text":"<p>As an example, a single metric named sample_metric is assigned a random value between 0 and 100 and written.</p> Node.jsGo <pre><code>var mv = new messages.MetricValue();\nmv.setName('sample_metric');\nmv.setValue(random({min: 0, max: 100, integer: true}).toString());\nmv.setApplication('default/backend');\nmv.setUser(user);\nclient.writeMetric(mv, function(err, session) {});\n</code></pre> <pre><code>_, _ = (*client).WriteMetric(\n    ctx,\n    &amp;pb.MetricValue{\n        Name:        \"sample_metric\",\n        Value:       fmt.Sprintf(\"%f\", rand.Float64()*100.0),\n        Application: \"default/backend\",\n        User:        user,\n    },\n)\n</code></pre>"},{"location":"user-guide/performance/extension/","title":"Using new resource types","text":"<p>The Iter8 performance test task <code>ready</code> ensures that an object exists and is ready. To use this task with new resource types, including CRDs, add the new resource type to the list of known types defined in the default <code>values.yaml</code> file for the chart.  Alternatively, the new type can be specified at run time with the <code>--set</code> option.</p>"},{"location":"user-guide/performance/extension/#example","title":"Example","text":"<p>To include a Knative service as part of a version definition, add the following to the map of <code>resourceTypes</code> in the <code>values.yaml</code> file used to configure the controller. The addition identifies the Kubernetes group, version, and resource (GVR) and the status condition that should be checked for readiness.</p> <pre><code>ksvc:\n    Group: serving.knative.dev\n    Version: v1\n    Resource: services\n    conditions:\n    - Ready\n</code></pre> <p>Alternatively, to set the values at run time:</p> <pre><code>--set resourceTypes.ksvc.Group=serving.knative.dev \\\n--set resourceTypes.ksvc.Version=v1 \\\n--set resourceTypes.Resource=services \\\n--set \"resourceTypes.conditions[0]=Ready\"\n</code></pre>"},{"location":"user-guide/performance/parameters/","title":"Performance test parameters","text":"<p>Iter8 performance tests are run using Helm. Tests can be configured with parameters in the same manner as any other Helm command -- using chart values and the <code>--set</code> option.</p> <p>The <code>tasks</code> parameter is used to identify the sequence of tasks that should be executed. Each task has its own parameters. Iter8 uses the convention that the parameters of a task are nested under the name of that task. In the following example, the <code>url</code> parameter of the <code>http</code> task is nested under the <code>http</code> object.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.url=https://httpbin.org/get\n</code></pre> <p>All the parameters of the performance test (including of all included tasks) are optional unless otherwise documented. The task-specific parameters are documented in the task documentation.</p> <p>Currently available tasks are:</p> <ul> <li><code>http</code> - generate synthetic load and capture performance metrics for HTTP endpoints</li> <li><code>grpc</code> - generate synthetic load and capture performance metrics for gRPC methods</li> <li><code>ready</code> - check readiness of an object</li> <li><code>github</code> - send a GitHub notification</li> <li><code>slack</code> - send a Slack notification</li> </ul>"},{"location":"user-guide/performance/parameters/#global-parameters","title":"Global parameters","text":"<p>In addition to task-specific parameters, the following global parameters are available:</p> Name Type Description serviceAccountName string Optional name of a service account to use. If specified, it is assumed the service account has the necessary permissions to run a performance test. If not specified, Iter8 will create a service account. logLevel string Log level. Must be one of <code>trace</code>, <code>debug</code>, <code>info</code> (default), <code>warning</code>, or <code>error</code>."},{"location":"user-guide/performance/tasks/github/","title":"github","text":"<p>Trigger GitHub workflows via a repository_dispatch.</p> <p>A <code>repository_dispatch</code> will trigger workflows in the default branch of the GitHub repository. By default, an summary of the performance test will also be sent.</p>"},{"location":"user-guide/performance/tasks/github/#usage-example","title":"Usage Example","text":"<pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http,github}\" \\\n--set http.url=http://httpbin.default/get \\\n--set github.owner=&lt;GitHub owner&gt; \\\n--set github.repo=&lt;GitHub repository&gt; \\\n--set github.token=&lt;GitHub token&gt;\n</code></pre> <p>See here for a more in-depth tutorial.</p>"},{"location":"user-guide/performance/tasks/github/#parameters","title":"Parameters","text":"Name Type Required Default value Description owner string Yes N/A Owner of the GitHub repository repo string Yes N/A GitHub repository token string Yes N/A Authorization token payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/iter8/v1.1.1/templates/notify/_payload-github.tpl URL to a payload template softFailure bool No true Indicates the performance test should not fail if the task cannot successfully send the request"},{"location":"user-guide/performance/tasks/github/#default-payload","title":"Default payload","text":"<p>A <code>repository_dispatch</code> requires a payload that contains the type of the event. </p> <p>The default payload template will set the <code>event_type</code> to <code>iter8</code>. In addition, it will also provide a performance test summary in the <code>client_payload</code>, which means that this data will be accessible in the GitHub workflow via <code>${{ toJson(github.event.client_payload) }}</code>.</p> <p>However, if you would like to use a different payload template, simply set a <code>payloadTemplateURL</code> and Iter8 will not use the default.</p>"},{"location":"user-guide/performance/tasks/grpc/","title":"grpc","text":"<p>Generate requests for a gRPC service and and collect latency and error-related metrics.</p>"},{"location":"user-guide/performance/tasks/grpc/#usage-example","title":"Usage example","text":"<p>In this performance test, the <code>grpc</code> task generates call requests for a gRPC service hosted at <code>hello.default:50051</code>, defined in the protobuf file located at <code>grpc.protoURL</code>, with a gRPC method named <code>helloworld.Greeter.SayHello</code>. Metrics collected by this task are viewable with a Grafana dashboard.</p> <p>Single method: <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={grpc}\" \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json\n</code></pre></p> <p>Multiple methods: <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={grpc}\" \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.endpoints.getFeature.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.endpoints.getFeature.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\\n--set grpc.endpoints.listFeatures.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.endpoints.listFeatures.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\\n--set grpc.endpoints.recordRoute.call=routeguide.RouteGuide.RecordRoute \\\n--set grpc.endpoints.recordRoute.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json \\\n--set grpc.endpoints.routeChat.call=routeguide.RouteGuide.RouteChat \\\n--set grpc.endpoints.routeChat.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/bidirectional.json\n</code></pre></p>"},{"location":"user-guide/performance/tasks/grpc/#parameters","title":"Parameters","text":"<p>Any field in the <code>Config</code> struct of the <code>ghz</code> runner package can be used as a parameter in this task. The JSON tags of the struct fields directly correspond to the names of the parameters of this task. In the usage example, the parameters <code>host</code> and <code>call</code> correspond to the <code>Host</code> and <code>Call</code> fields respectively in the <code>Config</code> struct.</p> <p>In addition, the following fields are defined by this task. </p> Name Type Description protoURL string (URL) URL where the protobuf file that defines the gRPC service is located. dataURL string (URL) URL where JSON data to be used in call requests is located. binaryDataURL string (URL) URL where binary data to be used in call requests is located. metadataURL string (URL) URL where the JSON metadata data to be used in call requests is located. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The <code>string</code> is the name of the endpoint and the <code>EndPoint</code> struct includes all the parameters described above as well as those from the <code>Config</code> struct. Load testing and metric collection will be conducted separately for each endpoint."},{"location":"user-guide/performance/tasks/grpc/#precedence","title":"Precedence","text":"<p>Some parameters have a default value, which can be overwritten. In addition, with the <code>endpoints</code> parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value.</p> <p>In the following example, all three endpoints will use the default <code>timeout</code> of <code>20s</code> (from <code>Config</code> struct).</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={grpc}\" \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.endpoints.getFeature.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.endpoints.getFeature.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\\n--set grpc.endpoints.listFeatures.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.endpoints.listFeatures.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\\n--set grpc.endpoints.recordRoute.call=routeguide.RouteGuide.RecordRoute \\\n--set grpc.endpoints.recordRoute.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json\n</code></pre> <p>In the following example, the <code>getFeature</code> and <code>listFeatures</code> endpoints will use the default <code>timeout</code> of <code>20s</code> and the <code>recordRoute</code> endpoint will use a <code>timeout</code> of <code>30s</code>.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={grpc}\" \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.endpoints.getFeature.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.endpoints.getFeature.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\\n--set grpc.endpoints.listFeatures.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.endpoints.listFeatures.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\\n--set grpc.endpoints.recordRoute.call=routeguide.RouteGuide.RecordRoute \\\n--set grpc.endpoints.recordRoute.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json\n</code></pre> <p>In the following example, all three endpoints will use a <code>qps</code> of <code>40s</code>.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={grpc}\" \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.timeout=40s \\\n--set grpc.endpoints.getFeature.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.endpoints.getFeature.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\\n--set grpc.endpoints.listFeatures.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.endpoints.listFeatures.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\\n--set grpc.endpoints.recordRoute.call=routeguide.RouteGuide.RecordRoute \\\n--set grpc.endpoints.recordRoute.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json\n</code></pre> <p>In the following example, the <code>getFeature</code> and <code>listFeatures</code> endpoints will use a <code>timeout</code> of <code>40s</code> and the <code>listFeatures</code> endpoint will use a <code>timeout</code> of <code>30s</code>.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={grpc}\" \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.timeout=40s \\\n--set grpc.endpoints.getFeature.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.endpoints.getFeature.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\\n--set grpc.endpoints.listFeatures.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.endpoints.listFeatures.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\\n--set grpc.endpoints.listFeatures.timeout=30s \\\n--set grpc.endpoints.recordRoute.call=routeguide.RouteGuide.RecordRoute \\\n--set grpc.endpoints.recordRoute.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json \\\n--set grpc.endpoints.recordRoute.timeout=30s\n</code></pre> <p>Further more, set parameters will trickle down to the endpoints.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 routeguide-test iter8 \\\n--set \"tasks={grpc}\" \\\n--set grpc.host=routeguide.default:50051 \\\n--set grpc.protoURL=https://raw.githubusercontent.com/grpc/grpc-go/v1.52.0/examples/route_guide/routeguide/route_guide.proto \\\n--set grpc.skipFirst=5 \\\n--set grpc.endpoints.getFeature.call=routeguide.RouteGuide.GetFeature \\\n--set grpc.endpoints.getFeature.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/unary.json \\\n--set grpc.endpoints.listFeatures.call=routeguide.RouteGuide.ListFeatures \\\n--set grpc.endpoints.listFeatures.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/server.json \\\n--set grpc.endpoints.listFeatures.timeout=30s \\\n--set grpc.endpoints.recordRoute.call=routeguide.RouteGuide.RecordRoute \\\n--set grpc.endpoints.recordRoute.dataURL=https://raw.githubusercontent.com/iter8-tools/docs/v0.17.3/samples/grpc-payload/client.json\n</code></pre> <p>In this example, all three endpoints will have a <code>skipFirst</code> of 5.</p>"},{"location":"user-guide/performance/tasks/grpc/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>The results of the <code>grpc</code> task is visualized using the <code>grpc</code> Iter8 Grafana dashboard. The dashboard can be found here.</p> <p>Assuming the URL to the Grafana service is <code>$GRAFANA_URL</code>, you can install the dashboard as follows:</p> <ol> <li>Open Grafana in a browser. </li> <li>Add a new data JSON API data source with the following parameters<ul> <li>URL: <code>$GRAFANA_URL/grpcDashboard</code></li> <li>Query string: <code>namespace=&lt;namespace&gt;&amp;test=&lt;test name&gt;</code></li> </ul> </li> <li>Import the <code>grpc</code> Iter8 Grafana dashboard<ul> <li>Copy and paste the contents of this link into the text box</li> </ul> </li> </ol> <p>You will see a visualization of the performance test like the following:</p> <p></p> <p>For multiple endpoints, the visualization will look like the following:</p> <p></p>"},{"location":"user-guide/performance/tasks/http/","title":"http","text":"<p>Generate requests for an HTTP service and and collect latency and error-related metrics.</p>"},{"location":"user-guide/performance/tasks/http/#usage-example","title":"Usage example","text":"<p>In this performance test, the <code>http</code> task generates requests for <code>https://httpbin.org/get</code>, and collects latency and error-related metrics. Metrics collected by this task are viewable with a Grafana dashboard.</p> <p>Single endpoint: <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.url=https://httpbin.org/get\n</code></pre></p> <p>Multiple endpoints: <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.endpoints.get.url=http://httpbin.default/get \\\n--set http.endpoints.getAnything.url=http://httpbin.default/anything \\\n--set http.endpoints.post.url=http://httpbin.default/post \\\n--set http.endpoints.post.payloadStr=hello\n</code></pre></p>"},{"location":"user-guide/performance/tasks/http/#parameters","title":"Parameters","text":"Name Type Description url string (URL) URL where requests are sent. headers map[string]string HTTP headers to use in the requests. numRequests int Number of requests to be sent to the app. Default value is 100. duration string Duration of this task. Specified in the Go duration string format (example, <code>5s</code>). If both duration and numRequests are specified, then duration is ignored. qps float qps stands for queries-per-second. Number of requests per second sent to the app. Default value is 8.0. connections int Number of parallel connections used to send requests. Default value is 4. payloadURL string (URL) URL from which to download the content that will be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this content as the payload. payloadStr string String data to be used as the request payload. If this field is specified, Iter8 will send HTTP POST requests to the app using this string as the payload. contentType string Content type of the payload. This is intended to be used in conjunction with one of the <code>payload*</code> fields. If this field is specified, Iter8 will send HTTP POST requests to the app using this as the Content-Type header value. warmupNumRequests int Number of requests to be sent in a warmup task (results are ignored). warmupDuration string Duration of warmup task (results are ignored). Specified in the Go duration string format (example, 5s). If both warmupDuration and warmupNumRequests are specified, then warmupDuration is ignored. endpoints map[string]EndPoint Used to specify multiple endpoints and their configuration. The <code>string</code> is the name of the endpoint and the <code>EndPoint</code> struct includes all the parameters described above. Load testing and metric collection will be conducted separately for each endpoint."},{"location":"user-guide/performance/tasks/http/#precedence","title":"Precedence","text":"<p>Some parameters have a default value, which can be overwritten. In addition, with the <code>endpoints</code> parameter, you can test multiple endpoints and configure parameters for each of those endpoint. In these cases, the priority order is the default value, the value set at the base level, and the value set at the endpoint value.</p> <p>In the following example, all three endpoints will use the default <code>qps</code> (queries-per-second) of 8.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.endpoints.get.url=http://httpbin.default/get \\\n--set http.endpoints.getAnything.url=http://httpbin.default/anything \\\n--set http.endpoints.post.url=http://httpbin.default/post \\\n--set http.endpoints.post.payloadStr=hello\n</code></pre> <p>In the following example, the <code>get</code> and <code>getAnything</code> endpoints will use the default <code>qps</code> of 8 and the <code>post</code> endpoint will use a <code>qps</code> of 15.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.endpoints.get.url=http://httpbin.default/get \\\n--set http.endpoints.getAnything.url=http://httpbin.default/anything \\\n--set http.endpoints.post.url=http://httpbin.default/post \\\n--set http.endpoints.post.payloadStr=hello \\\n--set http.endpoints.post.qps=15\n</code></pre> <p>In the following example, all three endpoints will use a <code>qps</code> (queries-per-second) of 10.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.qps=10 \\\n--set http.endpoints.get.url=http://httpbin.default/get \\\n--set http.endpoints.getAnything.url=http://httpbin.default/anything \\\n--set http.endpoints.post.url=http://httpbin.default/post \\\n--set http.endpoints.post.payloadStr=hello\n</code></pre> <p>In the following example, the <code>get</code> and <code>getAnything</code> endpoints will use a <code>qps</code> of 10 and the <code>post</code> endpoint will use a <code>qps</code> of 15.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.qps=10 \\\n--set http.endpoints.get.url=http://httpbin.default/get \\\n--set http.endpoints.getAnything.url=http://httpbin.default/anything \\\n--set http.endpoints.post.url=http://httpbin.default/post \\\n--set http.endpoints.post.payloadStr=hello \\\n--set http.endpoints.post.qps=15\n</code></pre> <p>Further more, set parameters will trickle down to the endpoints.</p> <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http}\" \\\n--set http.numRequests=50 \\\n--set http.endpoints.get.url=http://httpbin.default/get \\\n--set http.endpoints.getAnything.url=http://httpbin.default/anything \\\n--set http.endpoints.post.url=http://httpbin.default/post \\\n--set http.endpoints.post.payloadStr=hello\n</code></pre> <p>In this example, all three endpoints will have a <code>numRequests</code> of 50.</p>"},{"location":"user-guide/performance/tasks/http/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>The results of the <code>http</code> task is visualized using the <code>http</code> Iter8 Grafana dashboard. The dashboard can be found here.</p> <p>Assuming the URL to the Grafana service is <code>$GRAFANA_URL</code>, you can install the dashboard as follows:</p> <ol> <li>Open Grafana in a browser. </li> <li>Add a new data JSON API data source with the following parameters<ul> <li>URL: <code>$GRAFANA_URL/httpDashboard</code></li> <li>Query string: <code>namespace=&lt;namespace&gt;&amp;test=&lt;test name&gt;</code></li> </ul> </li> <li>Import the <code>http</code> Iter8 Grafana dashboard<ul> <li>Copy and paste the contents of this link into the text box</li> </ul> </li> </ol> <p>You will see a visualization of the performance test like the following:</p> <p></p> <p>For multiple endpoints, the visualization will look like the following:</p> <p></p>"},{"location":"user-guide/performance/tasks/ready/","title":"ready","text":"<p>Check if a Kubernetes object exists and is ready.</p>"},{"location":"user-guide/performance/tasks/ready/#usage-example","title":"Usage example","text":"<p>In the following example, the <code>ready</code> task checks if a deployment named <code>httpbin-prod</code> exists and its availability condition is set to true, and a service named <code>httpbin</code> exists. <pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={ready,http}\" \\\n--set ready.deploy=httpbin-prod \\\n--set ready.service=httpbin \\\n--set http.url=http://httpbin.default/get\n</code></pre></p>"},{"location":"user-guide/performance/tasks/ready/#parameters","title":"Parameters","text":"Name Type Description deploy string Name of a Kubernetes deployment. The task checks if the deployment exists and its <code>Available</code> condition is set to true. service string Name of a Kubernetes service. The task checks if the service exists. ksvc string Name of a Knative service. The task checks if the service exists and its <code>Ready</code> condition is set to true. timeout string Timeout for readiness check to succeed. Default value is <code>60s</code>. namespace string The namespace under which to look for the Kubernetes objects."},{"location":"user-guide/performance/tasks/ready/#extensions","title":"Extensions","text":"<p>Iter8 can be easily extended to support readiness checks for any type of Kubernetes object (including objects with custom resource types). To do so, add the new resource type to the list of known types defined in the default <code>values.yaml</code> file for the chart.</p>"},{"location":"user-guide/performance/tasks/ready/#example","title":"Example","text":"<p>To include a Knative service as part of a version definition, add the following to the map of <code>resourceTypes</code> in the <code>values.yaml</code> file used to configure the controller. The addition identifies the Kubernetes group, version, and resource (GVR) and the status condition that should be checked for readiness.</p> <pre><code>ksvc:\n    Group: serving.knative.dev\n    Version: v1\n    Resource: services\n    conditions:\n    - Ready\n</code></pre>"},{"location":"user-guide/performance/tasks/slack/","title":"slack","text":"<p>Send an performance test summary in a message to a Slack channel using a incoming webhook. </p>"},{"location":"user-guide/performance/tasks/slack/#usage-example","title":"Usage Example","text":"<pre><code>helm upgrade --install \\\n--repo https://iter8-tools.github.io/iter8 --version 1.1 httpbin-test iter8 \\\n--set \"tasks={http,slack}\" \\\n--set http.url=http://httpbin.default/get \\\n--set slack.url=&lt;Slack webhook&gt; \\\n--set slack.method=POST\n</code></pre> <p>See here for a more in-depth tutorial.</p>"},{"location":"user-guide/performance/tasks/slack/#parameters","title":"Parameters","text":"Name Type Required Default value Description url string Yes N/A URL to the Slack webhook payloadTemplateURL string No https://raw.githubusercontent.com/iter8-tools/iter8/v1.1.1/templates/notify/_payload-slack.tpl URL to a payload template softFailure bool No true Indicates the performance test should not fail if the task cannot successfully send the request"},{"location":"user-guide/performance/tasks/slack/#default-payload","title":"Default payload","text":"<p>The payload will determine what will be contained in the Slack message. The default payload template of the <code>slack</code> task is to send a performance test summary in text form.</p> <p>However, if you would like to use a different payload template, simply set a <code>payloadTemplateURL</code> and Iter8 will not use the default.</p>"},{"location":"user-guide/progressive-release/about/","title":"About progressive release","text":"<p>Progressive release is an approach to application/ML model release that involves deploying a candidate version at the same time as the current, or primary, version. A portion of user traffic is directed to the candidate version allowing it to be evaluated for behavior and performance. Once approved, the candidate can be promoted -- becoming the new primary version.</p> <p>There are different strategies for distributing traffic between the primary and candidate version:</p> <ul> <li>blue-green - A percentage of requests are directed to a candidate version of the model. This percentage can be changed over time. The remaining requests go to the primary version.</li> <li>canary - Requests that match a particular pattern, for example those that have a particular header, are directed to the candidate version of the model. The remaining requests go to the primary version.</li> <li>mirrored - All requests are sent to the primary version. A percentage of requests is replicated and sent to a candidate version of the model. This percentage can be changed over time. Only responses from the primary version are returned to the user.</li> </ul> <p>Progressive release uses the Iter8 <code>release</code> Helm chart. Options are described in the default values.yaml file. The progressive release tutorials show how it can be used to progressively release versions of an application or ML model.</p> <p>The chart provided by Iter8 supports many common deployment scenarios including:</p> <ul> <li>Applications composed of a <code>Deployment</code> and a <code>Service</code> object using Istio as a service mesh</li> <li>Applications composed of a <code>Deployment</code> and a <code>Service</code> object using the Kubernetes Gateway API</li> <li>ML models deployed to KServe (using KNative)</li> <li>ML models deploy to KServe ModelMesh using Istio as a service mesh</li> </ul>"},{"location":"user-guide/progressive-release/about/#other-deployment-environments","title":"Other deployment environments","text":"<p>The <code>release</code> chart can be easily extended to include other deployment environments. Please consider contributing any extensions to Iter8.</p>"},{"location":"user-guide/progressive-release/extension/","title":"Extending progressive release to other deployment environments","text":"<p>The Iter8 <code>release</code> chart can be easily extended for applications/ML models composed of new types. We briefly describe how to extend the chart for an Knative application. </p>"},{"location":"user-guide/progressive-release/extension/#approach","title":"Approach","text":"<p>The <code>release</code> chart can be found in the <code>charts/release</code> sub-folder of the Iter8 GitHub repository. The file <code>release.yaml</code> is the starting point. For each valid environment, the chart contains a set of files defining the resources that should be created.  These may include:</p> <ul> <li>application object(s)</li> <li>routemaps for different traffic patterns</li> <li>configmaps used to specify request percentages</li> <li>a service defining a common entry for requests (if needed)</li> </ul> <p>Note that the file naming helps identify related template files.</p>"},{"location":"user-guide/progressive-release/extension/#example-knative-service","title":"Example (Knative Service)","text":"<p>For example, to implement a blue-green release for Knative services, the following files could be added.</p> <ul> <li><code>_knative-istio.tpl</code> - wrapper for all objects that should be deployed</li> <li><code>_knative-istio.version.ksvc.tpl</code> - the Knative service object that should be deployed for a version</li> <li><code>_knative-istio.blue-green.tpl</code> - wrapper for all objects that should be deployed to support the blue-green traffic pattern</li> <li><code>_knative-istio.blue-green.routemap.tpl</code> - the routemap definition</li> <li><code>_knative-istio.service.tpl</code> - a supporting external service</li> <li><code>_knative.helpers.tpl</code> - supporting functions</li> </ul> <p>An implementation of these is here.</p> <p>Note that this sample only implements the blue-green traffic pattern. A more complete implementation would include canary and mirroring traffic patterns.</p> <p>Finally, update <code>release.yaml</code> to include <code>knative-istio</code> as a valid option:</p> <pre><code>{{- else if eq \"knative-istio\" .Values.environment }}\n  {{- include \"env.knative-istio\" . }}\n</code></pre>"},{"location":"user-guide/progressive-release/extension/#extend-the-controller","title":"Extend the controller","text":"<p>The Iter8 controller will need to be restarted with permission to watch Knative service objects. Re-install the controller using the following additional options:</p> <pre><code>--set resourceTypes.ksvc.Group=serving.knative.dev \\\n--set resourceTypes.ksvc.Version=v1 \\\n--set resourceTypes.ksvc.Resource=services \\\n--set \"resourceTypes.ksvc.conditions[0]=Ready\"\n</code></pre>"},{"location":"user-guide/progressive-release/extension/#using-the-modified-chart","title":"Using the modified chart","text":"<p>Reference the location of the local copy of the chart instead of using the <code>--repo</code> and <code>--version</code> options. For example assuming the location is <code>$CHART</code>, a deployment of two versions of the Knative <code>hello</code> service with a 30-70 traffic split would be:</p> <pre><code>cat &lt;&lt;EOF | helm upgrade --install hello $CHART -f -\nenvironment: knative-istio\napplication:\n  versions:\n  - ksvcSpecification:\n      spec:\n        template:\n          spec:\n            containers:\n            - image: ghcr.io/knative/helloworld-go:latest\n              ports:\n              - containerPort: 80\n              env:\n              - name: TARGET\n                value: \"v1\"\n    weight: 30\n  - ksvcSpecification:\n      spec:\n        template:\n          spec:\n            containers:\n            - image: ghcr.io/knative/helloworld-go:latest\n              ports:\n              - containerPort: 80\n              env:\n              - name: TARGET\n                value: \"v2\"\n    weight: 70\n  strategy: blue-green\nEOF\n</code></pre>"},{"location":"user-guide/progressive-release/extension/#contribute-your-extensions","title":"Contribute your extensions","text":"<p>Please consider contributing any extensions to Iter8 by submitting a pull request.</p>"}]}